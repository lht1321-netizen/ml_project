{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "pip install 'git+https://github.com/facebookresearch/fvcore'"
      ],
      "metadata": {
        "id": "QsgJHZioIBC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "import inspect\n",
        "import functools\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
        "import fvcore.nn.weight_init as weight_init\n",
        "from fvcore.common.config import CfgNode as _CfgNode\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n",
        "from torch.cuda.amp import autocast"
      ],
      "metadata": {
        "id": "PN6p8ijWGYn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from typing import Optional\n",
        "class ShapeSpec:\n",
        "  def __init__(self, channels=None, height=None, width=None, stride=None):\n",
        "      self.channels = channels\n",
        "      self.height = height\n",
        "      self.width = width\n",
        "      self.stride = stride"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ForM2I9KGTUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n"
      ],
      "metadata": {
        "id": "zwiRCU5bXRqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def get_norm(norm, out_channels):\n",
        "    if norm is None:\n",
        "        return None\n",
        "    if isinstance(norm, str):\n",
        "        if norm == \"\" or norm.lower() == \"none\":\n",
        "            return None\n",
        "        if norm == \"BN\":\n",
        "            return nn.BatchNorm2d(out_channels)\n",
        "        if norm == \"SyncBN\":\n",
        "            return nn.SyncBatchNorm(out_channels)\n",
        "        if norm == \"GN\":\n",
        "            groups = 32 if out_channels % 32 == 0 else max(1, min(32, out_channels))\n",
        "            return nn.GroupNorm(groups, out_channels)\n",
        "        if norm == \"LN\":\n",
        "            class _ChannelsFirstLayerNorm(nn.Module):\n",
        "                def __init__(self, num_channels, eps=1e-6):\n",
        "                    super().__init__()\n",
        "                    self.weight = nn.Parameter(torch.ones(num_channels))\n",
        "                    self.bias = nn.Parameter(torch.zeros(num_channels))\n",
        "                    self.eps = eps\n",
        "                def forward(self, x):\n",
        "                    mean = x.mean(dim=1, keepdim=True)\n",
        "                    var = (x - mean).pow(2).mean(dim=1, keepdim=True)\n",
        "                    x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "                    return x * self.weight[:, None, None] + self.bias[:, None, None]\n",
        "            return _ChannelsFirstLayerNorm(out_channels)\n",
        "        raise ValueError(...)\n",
        "    if callable(norm):\n",
        "        return norm(out_channels)\n",
        "    if isinstance(norm, nn.Module):\n",
        "        return norm\n",
        "    raise TypeError(...)"
      ],
      "metadata": {
        "id": "k9jIqHDRZZnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        dilation=1,\n",
        "        groups=1,\n",
        "        bias=True,\n",
        "        norm=None,\n",
        "        activation=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "            groups=groups,\n",
        "            bias=bias if norm is None else False,\n",
        "        )\n",
        "        self.norm = norm\n",
        "        self.activation = activation\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self.conv.weight\n",
        "\n",
        "    @property\n",
        "    def bias(self):\n",
        "        return self.conv.bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) if callable(self.activation) else self.activation(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ayc5yv_bbpXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def configurable(init_func=None, *, from_config=None):\n",
        "\n",
        "    if init_func is not None:\n",
        "        assert (\n",
        "            inspect.isfunction(init_func)\n",
        "            and from_config is None\n",
        "            and init_func.__name__ == \"__init__\"\n",
        "        ), \"Incorrect use of @configurable. Check API documentation for examples.\"\n",
        "\n",
        "        @functools.wraps(init_func)\n",
        "        def wrapped(self, *args, **kwargs):\n",
        "            try:\n",
        "                from_config_func = type(self).from_config\n",
        "            except AttributeError as e:\n",
        "                raise AttributeError(\n",
        "                    \"Class with @configurable must have a 'from_config' classmethod.\"\n",
        "                ) from e\n",
        "            if not inspect.ismethod(from_config_func):\n",
        "                raise TypeError(\"Class with @configurable must have a 'from_config' classmethod.\")\n",
        "\n",
        "            if _called_with_cfg(*args, **kwargs):\n",
        "                explicit_args = _get_args_from_config(from_config_func, *args, **kwargs)\n",
        "                init_func(self, **explicit_args)\n",
        "            else:\n",
        "                init_func(self, *args, **kwargs)\n",
        "\n",
        "        return wrapped\n",
        "\n",
        "    else:\n",
        "        if from_config is None:\n",
        "            return configurable  # @configurable() is made equivalent to @configurable\n",
        "        assert inspect.isfunction(\n",
        "            from_config\n",
        "        ), \"from_config argument of configurable must be a function!\"\n",
        "\n",
        "        def wrapper(orig_func):\n",
        "            @functools.wraps(orig_func)\n",
        "            def wrapped(*args, **kwargs):\n",
        "                if _called_with_cfg(*args, **kwargs):\n",
        "                    explicit_args = _get_args_from_config(from_config, *args, **kwargs)\n",
        "                    return orig_func(**explicit_args)\n",
        "                else:\n",
        "                    return orig_func(*args, **kwargs)\n",
        "\n",
        "            wrapped.from_config = from_config\n",
        "            return wrapped\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "def _get_args_from_config(from_config_func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Use `from_config` to obtain explicit arguments.\n",
        "\n",
        "    Returns:\n",
        "        dict: arguments to be used for cls.__init__\n",
        "    \"\"\"\n",
        "    signature = inspect.signature(from_config_func)\n",
        "    if list(signature.parameters.keys())[0] != \"cfg\":\n",
        "        if inspect.isfunction(from_config_func):\n",
        "            name = from_config_func.__name__\n",
        "        else:\n",
        "            name = f\"{from_config_func.__self__}.from_config\"\n",
        "        raise TypeError(f\"{name} must take 'cfg' as the first argument!\")\n",
        "    support_var_arg = any(\n",
        "        param.kind in [param.VAR_POSITIONAL, param.VAR_KEYWORD]\n",
        "        for param in signature.parameters.values()\n",
        "    )\n",
        "    if support_var_arg:  # forward all arguments to from_config, if from_config accepts them\n",
        "        ret = from_config_func(*args, **kwargs)\n",
        "    else:\n",
        "        # forward supported arguments to from_config\n",
        "        supported_arg_names = set(signature.parameters.keys())\n",
        "        extra_kwargs = {}\n",
        "        for name in list(kwargs.keys()):\n",
        "            if name not in supported_arg_names:\n",
        "                extra_kwargs[name] = kwargs.pop(name)\n",
        "        ret = from_config_func(*args, **kwargs)\n",
        "        # forward the other arguments to __init__\n",
        "        ret.update(extra_kwargs)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def _called_with_cfg(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        bool: whether the arguments contain CfgNode and should be considered\n",
        "            forwarded to from_config.\n",
        "    \"\"\"\n",
        "    from omegaconf import DictConfig\n",
        "\n",
        "    if len(args) and isinstance(args[0], (_CfgNode, DictConfig)):\n",
        "        return True\n",
        "    if isinstance(kwargs.pop(\"cfg\", None), (_CfgNode, DictConfig)):\n",
        "        return True\n",
        "    # `from_config`'s first argument is forced to be \"cfg\".\n",
        "    # So the above check covers all cases.\n",
        "    return False\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DNAh5l4ZUMev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def build_pixel_decoder(cfg, input_shape):\n",
        "\n",
        "    model = TransformerEncoderPixelDecoder(cfg, input_shape)\n",
        "    forward_features = getattr(model, \"forward_features\", None)\n",
        "    if not callable(forward_features):\n",
        "        raise ValueError(\n",
        "            \"Only SEM_SEG_HEADS with forward_features method can be used as pixel decoder. \"\n",
        "            f\"Please implement forward_features for ㄹㅇㄴㄿㅁㅈㅍㅊ to only return mask features.\"\n",
        "        )\n",
        "    return model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "84lyoHFJSK8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VxxiPccdBqcw"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class BasePixelDecoder(nn.Module):\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Dict[str, ShapeSpec],\n",
        "        *,\n",
        "        conv_dim: int,\n",
        "        mask_dim: int,\n",
        "        norm: Optional[Union[str, Callable]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            input_shape: shapes (channels and stride) of the input features\n",
        "            conv_dims: number of output channels for the intermediate conv layers.\n",
        "            mask_dim: number of output channels for the final conv layer.\n",
        "            norm (str or callable): normalization for all conv layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n",
        "        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n",
        "        feature_channels = [v.channels for k, v in input_shape]\n",
        "\n",
        "        lateral_convs = []\n",
        "        output_convs = []\n",
        "\n",
        "        use_bias = norm == \"\"\n",
        "        for idx, in_channels in enumerate(feature_channels):\n",
        "            if idx == len(self.in_features) - 1:\n",
        "                output_norm = get_norm(norm, conv_dim)\n",
        "                output_conv = Conv2d(\n",
        "                    in_channels,\n",
        "                    conv_dim,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    bias=use_bias,\n",
        "                    norm=output_norm,\n",
        "                    activation=F.relu,\n",
        "                )\n",
        "                weight_init.c2_xavier_fill(output_conv)\n",
        "                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n",
        "\n",
        "                lateral_convs.append(None)\n",
        "                output_convs.append(output_conv)\n",
        "            else:\n",
        "                lateral_norm = get_norm(norm, conv_dim)\n",
        "                output_norm = get_norm(norm, conv_dim)\n",
        "\n",
        "                lateral_conv = Conv2d(\n",
        "                    in_channels, conv_dim, kernel_size=1, bias=use_bias, norm=lateral_norm\n",
        "                )\n",
        "                output_conv = Conv2d(\n",
        "                    conv_dim,\n",
        "                    conv_dim,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    bias=use_bias,\n",
        "                    norm=output_norm,\n",
        "                    activation=F.relu,\n",
        "                )\n",
        "                weight_init.c2_xavier_fill(lateral_conv)\n",
        "                weight_init.c2_xavier_fill(output_conv)\n",
        "                self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n",
        "                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n",
        "\n",
        "                lateral_convs.append(lateral_conv)\n",
        "                output_convs.append(output_conv)\n",
        "        # Place convs into top-down order (from low to high resolution)\n",
        "        # to make the top-down computation in forward clearer.\n",
        "        self.lateral_convs = lateral_convs[::-1]\n",
        "        self.output_convs = output_convs[::-1]\n",
        "\n",
        "        self.mask_dim = mask_dim\n",
        "        self.mask_features = nn.Conv2d(\n",
        "            conv_dim,\n",
        "            mask_dim,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )\n",
        "        weight_init.c2_xavier_fill(self.mask_features)\n",
        "\n",
        "        self.maskformer_num_feature_levels = 3  # always use 3 scales\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n",
        "        ret = {}\n",
        "        ret[\"input_shape\"] = {\n",
        "            k: v for k, v in input_shape.items() if k in cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES\n",
        "        }\n",
        "        ret[\"conv_dim\"] = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n",
        "        ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n",
        "        ret[\"norm\"] = cfg.MODEL.SEM_SEG_HEAD.NORM\n",
        "        return ret\n",
        "\n",
        "    def forward_features(self, features):\n",
        "        multi_scale_features = []\n",
        "        num_cur_levels = 0\n",
        "        # Reverse feature maps into top-down order (from low to high resolution)\n",
        "        for idx, f in enumerate(self.in_features[::-1]):\n",
        "            x = features[f]\n",
        "            lateral_conv = self.lateral_convs[idx]\n",
        "            output_conv = self.output_convs[idx]\n",
        "            if lateral_conv is None:\n",
        "                y = output_conv(x)\n",
        "            else:\n",
        "                cur_fpn = lateral_conv(x)\n",
        "                # Following FPN implementation, we use nearest upsampling here\n",
        "                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "                y = output_conv(y)\n",
        "            if num_cur_levels < self.maskformer_num_feature_levels:\n",
        "                multi_scale_features.append(y)\n",
        "                num_cur_levels += 1\n",
        "        return self.mask_features(y), None, multi_scale_features\n",
        "\n",
        "    def forward(self, features, targets=None):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n",
        "        return self.forward_features(features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(\n",
        "                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos\n",
        "            )\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4e1QpiSSWoTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jDFmiJ_CYF5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        nhead,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"relu\",\n",
        "        normalize_before=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(\n",
        "            q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n",
        "        )[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(\n",
        "            q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n",
        "        )[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2qNVdNjIXaE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class TransformerEncoderOnly(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model=512,\n",
        "        nhead=8,\n",
        "        num_encoder_layers=6,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"relu\",\n",
        "        normalize_before=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(\n",
        "            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n",
        "        )\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, pos_embed):\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        return memory.permute(1, 2, 0).view(bs, c, h, w)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COqCEaEUMLE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if mask is None:\n",
        "            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack(\n",
        "            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n",
        "        ).flatten(3)\n",
        "        pos_y = torch.stack(\n",
        "            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n",
        "        ).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "    def __repr__(self, _repr_indent=4):\n",
        "        head = \"Positional encoding \" + self.__class__.__name__\n",
        "        body = [\n",
        "            \"num_pos_feats: {}\".format(self.num_pos_feats),\n",
        "            \"temperature: {}\".format(self.temperature),\n",
        "            \"normalize: {}\".format(self.normalize),\n",
        "            \"scale: {}\".format(self.scale),\n",
        "        ]\n",
        "        # _repr_indent = 4\n",
        "        lines = [head] + [\" \" * _repr_indent + line for line in body]\n",
        "        return \"\\n\".join(lines)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jdeUEyl8Xtjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "class TransformerEncoderPixelDecoder(BasePixelDecoder):\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Dict[str, ShapeSpec],\n",
        "        *,\n",
        "        transformer_dropout: float,\n",
        "        transformer_nheads: int,\n",
        "        transformer_dim_feedforward: int,\n",
        "        transformer_enc_layers: int,\n",
        "        transformer_pre_norm: bool,\n",
        "        conv_dim: int,\n",
        "        mask_dim: int,\n",
        "        norm: Optional[Union[str, Callable]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            input_shape: shapes (channels and stride) of the input features\n",
        "            transformer_dropout: dropout probability in transformer\n",
        "            transformer_nheads: number of heads in transformer\n",
        "            transformer_dim_feedforward: dimension of feedforward network\n",
        "            transformer_enc_layers: number of transformer encoder layers\n",
        "            transformer_pre_norm: whether to use pre-layernorm or not\n",
        "            conv_dims: number of output channels for the intermediate conv layers.\n",
        "            mask_dim: number of output channels for the final conv layer.\n",
        "            norm (str or callable): normalization for all conv layers\n",
        "        \"\"\"\n",
        "        super().__init__(input_shape, conv_dim=conv_dim, mask_dim=mask_dim, norm=norm)\n",
        "\n",
        "        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n",
        "        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n",
        "        feature_strides = [v.stride for k, v in input_shape]\n",
        "        feature_channels = [v.channels for k, v in input_shape]\n",
        "\n",
        "        in_channels = feature_channels[len(self.in_features) - 1]\n",
        "        self.input_proj = Conv2d(in_channels, conv_dim, kernel_size=1)\n",
        "        weight_init.c2_xavier_fill(self.input_proj)\n",
        "        self.transformer = TransformerEncoderOnly(\n",
        "            d_model=conv_dim,\n",
        "            dropout=transformer_dropout,\n",
        "            nhead=transformer_nheads,\n",
        "            dim_feedforward=transformer_dim_feedforward,\n",
        "            num_encoder_layers=transformer_enc_layers,\n",
        "            normalize_before=transformer_pre_norm,\n",
        "        )\n",
        "        N_steps = conv_dim // 2\n",
        "        self.pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "        # update layer\n",
        "        use_bias = norm == \"\"\n",
        "        output_norm = get_norm(norm, conv_dim)\n",
        "        output_conv = Conv2d(\n",
        "            conv_dim,\n",
        "            conv_dim,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=use_bias,\n",
        "            norm=output_norm,\n",
        "            activation=F.relu,\n",
        "        )\n",
        "        weight_init.c2_xavier_fill(output_conv)\n",
        "        delattr(self, \"layer_{}\".format(len(self.in_features)))\n",
        "        self.add_module(\"layer_{}\".format(len(self.in_features)), output_conv)\n",
        "        self.output_convs[0] = output_conv\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n",
        "        ret = super().from_config(cfg, input_shape)\n",
        "        ret[\"transformer_dropout\"] = cfg.MODEL.MASK_FORMER.DROPOUT\n",
        "        ret[\"transformer_nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n",
        "        ret[\"transformer_dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n",
        "        ret[\n",
        "            \"transformer_enc_layers\"\n",
        "        ] = cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS  # a separate config\n",
        "        ret[\"transformer_pre_norm\"] = cfg.MODEL.MASK_FORMER.PRE_NORM\n",
        "        return ret\n",
        "\n",
        "    def forward_features(self, features):\n",
        "        multi_scale_features = []\n",
        "        num_cur_levels = 0\n",
        "        # Reverse feature maps into top-down order (from low to high resolution)\n",
        "        for idx, f in enumerate(self.in_features[::-1]):\n",
        "            x = features[f]\n",
        "            lateral_conv = self.lateral_convs[idx]\n",
        "            output_conv = self.output_convs[idx]\n",
        "            if lateral_conv is None:\n",
        "                transformer = self.input_proj(x)\n",
        "                pos = self.pe_layer(x)\n",
        "                transformer = self.transformer(transformer, None, pos)\n",
        "                y = output_conv(transformer)\n",
        "                # save intermediate feature as input to Transformer decoder\n",
        "                transformer_encoder_features = transformer\n",
        "            else:\n",
        "                cur_fpn = lateral_conv(x)\n",
        "                # Following FPN implementation, we use nearest upsampling here\n",
        "                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "                y = output_conv(y)\n",
        "            if num_cur_levels < self.maskformer_num_feature_levels:\n",
        "                multi_scale_features.append(y)\n",
        "                num_cur_levels += 1\n",
        "        return self.mask_features(y), transformer_encoder_features, multi_scale_features\n",
        "\n",
        "    def forward(self, features, targets=None):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n",
        "        return self.forward_features(features)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PayhMleEMP0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SMOKE TEST for TransformerEncoderPixelDecoder =====\n",
        "\n",
        "\n",
        "# 더미 입력 feature dict 생성 (res2~res5)\n",
        "features = {\n",
        "    \"res2\": torch.randn(2, 256, 128, 128),\n",
        "    \"res3\": torch.randn(2, 512, 64, 64),\n",
        "    \"res4\": torch.randn(2, 1024, 32, 32),\n",
        "    \"res5\": torch.randn(2, 2048, 16, 16),\n",
        "}\n",
        "\n",
        "# input_shape 사전 정의\n",
        "input_shape = {\n",
        "    \"res2\": ShapeSpec(channels=256, stride=4),\n",
        "    \"res3\": ShapeSpec(channels=512, stride=8),\n",
        "    \"res4\": ShapeSpec(channels=1024, stride=16),\n",
        "    \"res5\": ShapeSpec(channels=2048, stride=32),\n",
        "}\n",
        "\n",
        "# 간이 config 흉내\n",
        "class DummyCfg:\n",
        "    class MODEL:\n",
        "        class SEM_SEG_HEAD:\n",
        "            IN_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
        "            CONVS_DIM = 256\n",
        "            MASK_DIM = 128\n",
        "            NORM = \"BN\"\n",
        "            TRANSFORMER_ENC_LAYERS = 3\n",
        "        class MASK_FORMER:\n",
        "            DROPOUT = 0.1\n",
        "            NHEADS = 8\n",
        "            DIM_FEEDFORWARD = 1024\n",
        "            PRE_NORM = False\n",
        "\n",
        "cfg = DummyCfg()\n",
        "\n",
        "# 모델 초기화\n",
        "model = TransformerEncoderPixelDecoder.from_config(cfg, input_shape)\n",
        "model = TransformerEncoderPixelDecoder(**model)\n",
        "\n",
        "# forward 테스트\n",
        "mask_features, transformer_features, multi_scale_features = model.forward_features(features)\n",
        "\n",
        "print(\"✅ forward() 성공!\")\n",
        "print(f\"mask_features: {mask_features.shape}\")\n",
        "print(f\"transformer_features: {transformer_features.shape if transformer_features is not None else None}\")\n",
        "print(f\"multi_scale_features ({len(multi_scale_features)} levels):\",\n",
        "      [f.shape for f in multi_scale_features])\n",
        "\n",
        "# backward 테스트\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "opt.zero_grad()\n",
        "loss = mask_features.mean()\n",
        "loss.backward()\n",
        "opt.step()\n",
        "print(\"✅ backward() & step() 성공, loss =\", float(loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX-GB5GyceUo",
        "outputId": "ae6bfc8f-cce3-493c-a9a5-f3e6e48f8809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ forward() 성공!\n",
            "mask_features: torch.Size([2, 128, 128, 128])\n",
            "transformer_features: torch.Size([2, 256, 16, 16])\n",
            "multi_scale_features (3 levels): [torch.Size([2, 256, 16, 16]), torch.Size([2, 256, 32, 32]), torch.Size([2, 256, 64, 64])]\n",
            "✅ backward() & step() 성공, loss = -0.00120543094817549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2423830191.py:56: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  print(\"✅ backward() & step() 성공, loss =\", float(loss))\n"
          ]
        }
      ]
    }
  ]
}