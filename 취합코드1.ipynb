{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLnCDxXi3g/36dOeav78WQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##ì¼ë‹¨ ë‹¤ ëª¨ì•„ë†“ê¸´ í–ˆê³ , ì˜¤ë¥˜ìˆ˜ì •ì´ë‘ ê²€í† ì¢€ í•˜ê³  ìˆì„ê²Œìš”\n","\n","# 1.1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n","#!pip install torch torchvision\n","#!pip install timm\n","#!pip install 'git+https://github.com/facebookresearch/fvcore'\n","#!pip install geopandas rasterio\n","\n","# 1.2: ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import glob\n","import numpy as np\n","import math\n","import copy\n","import inspect\n","import functools\n","import logging\n","from typing import Callable, Dict, List, Optional, Tuple, Union\n","\n","# 1.3: ëª¨ë¸ ì½”ë“œì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import fvcore.nn.weight_init as weight_init\n","from fvcore.common.config import CfgNode as _CfgNode\n","from timm.layers import trunc_normal_, to_2tuple, DropPath\n","from torch.cuda.amp import autocast\n","from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n","import torch.utils.checkpoint as checkpoint\n","\n","# 1.4: (ì°¸ê³ ) ë°ì´í„° ì „ì²˜ë¦¬ì— ì‚¬ìš©í–ˆë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import geopandas as gpd\n","import rasterio\n","from rasterio.windows import Window\n","from rasterio.features import rasterize\n","from shapely.geometry import box"],"metadata":{"id":"EDuNFUyHp-zT","executionInfo":{"status":"ok","timestamp":1762799217342,"user_tz":-540,"elapsed":26,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# 2. ë°ì´í„° ì „ì²˜ë¦¬ (ë³„ë„ ì‹¤í–‰ ê°€ì •)\n","# ------------------------------------\n","\n","def convert_to_target_format(label_mask):\n","    \"\"\"\n","    [í—¬í¼ í•¨ìˆ˜]\n","    'í†µí•© ë¼ë²¨ ë§ˆìŠ¤í¬'([H, W])ë¥¼ ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ 'targets' ë”•ì…”ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n","    ì´ í•¨ìˆ˜ëŠ” GIS ë°ì´í„°(ì •ìˆ˜ ID)ë¥¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹(ì´ì§„ ë§ˆìŠ¤í¬)ìœ¼ë¡œ ë²ˆì—­í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    # í†µí•© ë§ˆìŠ¤í¬ì—ì„œ ê³ ìœ í•œ ê±´ë¬¼ ID(1, 2, 3...)ë¥¼ ëª¨ë‘ ì°¾ì•„ëƒ…ë‹ˆë‹¤. ë°°ê²½ ê°’ì¸ 0ì€ ì œì™¸í•©ë‹ˆë‹¤.\n","    instance_ids = torch.unique(label_mask)\n","    instance_ids = instance_ids[instance_ids != 0]\n","\n","    # ë§Œì•½ í˜„ì¬ íƒ€ì¼ì— ê±´ë¬¼ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´(IDê°€ ì—†ë‹¤ë©´), í•™ìŠµ ì‹œ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•Šë„ë¡\n","    # ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” í…ì„œë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n","    if len(instance_ids) == 0:\n","        return {'instance_class': torch.tensor([], dtype=torch.long), 'mask': torch.tensor([], dtype=torch.uint8)}\n","\n","    #'ë¸Œë¡œë“œìºìŠ¤íŒ…'ì´ë¼ëŠ” í…ì„œ ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬, ê° ê±´ë¬¼ IDì— í•´ë‹¹í•˜ëŠ” í”½ì…€ë§Œ 1(True)ì´ê³ \n","    # ë‚˜ë¨¸ì§€ëŠ” 0(False)ì¸ ê°œë³„ ë§ˆìŠ¤í¬ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n","    # ê²°ê³¼ì ìœ¼ë¡œ [N, H, W] í¬ê¸°ì˜ 3ì°¨ì› í…ì„œê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. (N: ê±´ë¬¼ ê°œìˆ˜)\n","    masks = (label_mask == instance_ids[:, None, None])\n","\n","    # í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œëŠ” 'ê±´ë¬¼'ì´ë¼ëŠ” ë‹¨ì¼ í´ë˜ìŠ¤ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ,\n","    # ê±´ë¬¼ ê°œìˆ˜(N)ë§Œí¼ 0ìœ¼ë¡œ ì±„ì›Œì§„ í´ë˜ìŠ¤ ë¼ë²¨ í…ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n","    instance_classes = torch.zeros(len(instance_ids), dtype=torch.long)\n","\n","    # íŒ€ì›ì´ ìš”ì²­í•œ ìµœì¢… í˜•ì‹ì— ë§ì¶° 'instance_class'ì™€ 'mask'ë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ êµ¬ì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n","    return {'instance_class': instance_classes, 'masks': masks.to(torch.uint8)}\n","\n","\n","def create_final_dataset_per_file(\n","    ortho_image_path,\n","    building_shp_path,\n","    output_dir,\n","    tile_size=1024\n","):\n","    \"\"\"\n","    [RAM/ìŠ¤í† ë¦¬ì§€ ë¬¸ì œ í•´ê²° ìµœì¢… ë²„ì „]\n","    GIS ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬, ì´ë¯¸ì§€ íƒ€ì¼ê³¼ ê·¸ì— 1:1ë¡œ ëŒ€ì‘í•˜ëŠ” 'targets' ë”•ì…”ë„ˆë¦¬ë¥¼\n","    ê°ê° ê°œë³„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ RAM ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ìµœì¢… ì „ì²˜ë¦¬ í•¨ìˆ˜ì„\n","\n","    [ì¤‘ìš”] ì™œ 'ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸' ([{...}, {...}, ...])ë¥¼ ë§Œë“¤ì§€ ì•ŠëŠ”ê°€?\n","    ----------------------------------------------------------------\n","    ì´ 'ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸' í˜•ì‹ì€ ëª¨ë“  ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ì˜ ê±°ëŒ€í•œ ë¦¬ìŠ¤íŠ¸ ë³€ìˆ˜ì—\n","    ë‹´ì•„ë‘ì—ˆë‹¤ê°€ ë§ˆì§€ë§‰ì— í•œ ë²ˆì— íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë°©ì‹ì„\n","\n","    ë°œìƒí–ˆë˜ ë¬¸ì œ:\n","    1. ë©”ëª¨ë¦¬ ì´ˆê³¼ (RAM OOM): 'targets' ë”•ì…”ë„ˆë¦¬ëŠ” ë‚´ë¶€ì— [N, H, W] í¬ê¸°ì˜ í° ë§ˆìŠ¤í¬ í…ì„œë¥¼\n","       í¬í•¨í•˜ì—¬ ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤. ìˆ˜ì²œ ê°œì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‘ ë‹´ìœ¼ë©´\n","       Colabì˜ ì œí•œëœ RAM(12GB)ì„ ìˆœì‹ê°„ì— ì´ˆê³¼í•˜ì—¬ ì»¤ë„ ì¬ì‹œì‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŒ\n","    2. ì €ì¥ ë¹„íš¨ìœ¨: ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì˜ í¬ê¸°ê°€ ë§¤ìš° ì»¤ì ¸\n","       Google Driveì˜ ì €ì¥ ê³µê°„ë„ ë¹ ë¥´ê²Œ ì†Œì§„ë¨\n","\n","    í˜„ì¬ ì½”ë“œì˜ í•´ê²°ì±…:\n","    ì´ í•¨ìˆ˜ëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ íƒ€ì¼ë§Œ ì²˜ë¦¬í•˜ê³ , ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¦‰ì‹œ\n","    ê°œë³„ íŒŒì¼ë¡œ ì €ì¥í•¨. ì´ 'íŒŒì¼ ë‹¨ìœ„ ì €ì¥' ë°©ì‹ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•˜ì—¬\n","    Colab í™˜ê²½ì—ì„œë„ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•¨\n","    ----------------------------------------------------------------\n","    \"\"\"\n","    # ê²°ê³¼ë¬¼ì´ ì €ì¥ë  'images'ì™€ 'labels_dict' í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n","    img_tile_dir = os.path.join(output_dir, \"images\")\n","    lbl_dict_dir = os.path.join(output_dir, \"labels_dict\")\n","    os.makedirs(img_tile_dir, exist_ok=True)\n","    os.makedirs(lbl_dict_dir, exist_ok=True)\n","\n","    # GeoPandasë¡œ ê±´ë¬¼ shapefileì„, Rasterioë¡œ ì •ì‚¬ì˜ìƒ(.tif)ì„ ì—½ë‹ˆë‹¤.\n","    gdf_buildings = gpd.read_file(building_shp_path)\n","\n","    with rasterio.open(ortho_image_path) as src:\n","        # ì˜ìƒ ì „ì²´ë¥¼ 1024x1024 íƒ€ì¼ë¡œ ê¹”ë”í•˜ê²Œ ë‚˜ëˆ„ê¸° ìœ„í•´ ê°€ë¡œ/ì„¸ë¡œ í¬ê¸°ë¥¼ 1024ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\n","        width = (src.width // tile_size) * tile_size\n","        height = (src.height // tile_size) * tile_size\n","        print(f\"ì „ì²´ ì˜ìƒ í¬ê¸°: ({src.width}, {src.height}), ì¡°ì •ëœ í¬ê¸°: ({width}, {height})\")\n","\n","        # ë§Œì•½ ë‘ ë°ì´í„°ì˜ ì¢Œí‘œê³„(CRS)ê°€ ë‹¤ë¥´ë‹¤ë©´, ê±´ë¬¼ ë°ì´í„°ë¥¼ ì •ì‚¬ì˜ìƒ ê¸°ì¤€ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.\n","        if gdf_buildings.crs != src.crs:\n","            gdf_buildings = gdf_buildings.to_crs(src.crs)\n","\n","        # ìƒì„±ëœ íŒŒì¼ ê°œìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•œ ì¹´ìš´í„°ì…ë‹ˆë‹¤.\n","        tile_count = 0\n","        # ì´ì¤‘ forë¬¸ì„ ì‚¬ìš©í•´ ì „ì²´ ì˜ìƒì„ 1024x1024 í¬ê¸°ì˜ íƒ€ì¼(window) ë‹¨ìœ„ë¡œ ìˆœíšŒí•©ë‹ˆë‹¤.\n","        for j in range(0, height, tile_size):\n","            for i in range(0, width, tile_size):\n","                window = Window(i, j, tile_size, tile_size)\n","                tile_transform = src.window_transform(window)\n","\n","                # í˜„ì¬ íƒ€ì¼ì˜ ì§€ë¦¬ì  ì¢Œí‘œ ê²½ê³„ë¥¼ ê³„ì‚°í•˜ê³ , ì´ ê²½ê³„ì™€ ê²¹ì¹˜ëŠ” ê±´ë¬¼ë§Œ ë¹ ë¥´ê²Œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\n","                tile_bounds = rasterio.windows.bounds(window, src.transform)\n","                tile_bbox = box(*tile_bounds)\n","                intersecting_buildings = gdf_buildings[gdf_buildings.intersects(tile_bbox)]\n","\n","                # í˜„ì¬ íƒ€ì¼ ì•ˆì— ê±´ë¬¼ì´ í•˜ë‚˜ë¼ë„ ìˆì„ ê²½ìš°ì—ë§Œ ë°ì´í„° ìƒì„± ë° ì €ì¥ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n","                if not intersecting_buildings.empty:\n","                    # 1. 'í†µí•© ë¼ë²¨ ë§ˆìŠ¤í¬'ë¥¼ ë©”ëª¨ë¦¬ ìƒì—ì„œ ìƒì„±í•©ë‹ˆë‹¤. (ë°°ê²½=0, ê±´ë¬¼=1, 2, 3...)\n","                    simple_ids = range(1, len(intersecting_buildings) + 1)\n","                    shapes = [(geom, id) for geom, id in zip(intersecting_buildings.geometry, simple_ids)]\n","\n","                    label_mask = rasterize(\n","                        shapes=shapes,\n","                        out_shape=(tile_size, tile_size),\n","                        transform=tile_transform,\n","                        fill=0,\n","                        all_touched=True,\n","                        dtype=rasterio.int32\n","                    )\n","\n","                    # 2. ìœ„ì—ì„œ ìƒì„±ëœ 'í†µí•© ë¼ë²¨ ë§ˆìŠ¤í¬'ë¥¼ ì¦‰ì‹œ 'targets' ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n","                    target_dict = convert_to_target_format(torch.from_numpy(label_mask).long())\n","\n","                    # 3. ìµœì¢… ê²°ê³¼ë¬¼ì¸ ì´ë¯¸ì§€ì™€ 'targets' ë”•ì…”ë„ˆë¦¬ë¥¼ 1:1ë¡œ ë§¤ì¹­ë˜ëŠ” ê°œë³„ íŒŒì¼ë¡œ ì¦‰ì‹œ ì €ì¥í•©ë‹ˆë‹¤.\n","                    image_tile = src.read(window=window)\n","                    if image_tile.shape[0] > 3:\n","                        image_tile = image_tile[:3, :, :]\\\n","\n","                    image_tensor = torch.from_numpy(image_tile).float()\n","\n","                    torch.save(image_tensor, os.path.join(img_tile_dir, f\"tile_{tile_count}.pt\"))\n","                    torch.save(target_dict, os.path.join(lbl_dict_dir, f\"tile_{tile_count}.pt\"))\n","\n","                    # íŒŒì¼ ì €ì¥ í›„ ì¹´ìš´í„°ë¥¼ 1 ì¦ê°€ì‹œí‚µë‹ˆë‹¤.\n","                    tile_count += 1\n","\n","        print(f\"\\n--- ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ ---\")\n","        print(f\"ì´ {tile_count}ê°œì˜ [ì´ë¯¸ì§€ íŒŒì¼]ê³¼ [ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ íŒŒì¼] ìŒì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n","        print(f\"ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜: {img_tile_dir}\")\n","        print(f\"ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ì €ì¥ ìœ„ì¹˜: {lbl_dict_dir}\")\n","\n","\n","\n","# 3ë‹¨ê³„: í•¨ìˆ˜ ì‹¤í–‰\n","\n","\n","# --- Google Drive ê²½ë¡œ ì„¤ì • ---\n","# QGISì—ì„œ ì „ì²˜ë¦¬í•œ ì›ë³¸ ë°ì´í„° íŒŒì¼ë“¤ì˜ ê²½ë¡œì…ë‹ˆë‹¤.\n","ORTHO_TIF_PATH = \"/content/drive/MyDrive/gis_project/clipped_ì •ì‚¬ì˜ìƒ.tif\"\n","BUILDING_SHP_PATH = \"/content/drive/MyDrive/gis_project/clipped_buildings.shp\"\n","# ìµœì¢… ê²°ê³¼ë¬¼ì´ ì €ì¥ë  í´ë”ì˜ ê²½ë¡œì…ë‹ˆë‹¤.\n","OUTPUT_DIR = \"/content/drive/MyDrive/gis_project/final_dataset_per_file\"\n","# --------------------------------\n","\n","# ìœ„ì—ì„œ ì •ì˜í•œ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n","# (ì°¸ê³ : ì´ ì½”ë“œëŠ” í•™ìŠµ ë…¸íŠ¸ë¶ê³¼ëŠ” ë³„ë„ë¡œ *ë¨¼ì €* ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.)\n","# create_final_dataset_per_file(ORTHO_TIF_PATH, BUILDING_SHP_PATH, OUTPUT_DIR)\n","\n","# preprocess_gis_data_ipynb.ipynb íŒŒì¼ì€\n","# ì´ í•™ìŠµ ì½”ë“œì™€ ë³„ë„ë¡œ *ë¯¸ë¦¬ ì‹¤í–‰*ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n","#\n","# ì•„ë˜ ì½”ë“œëŠ” Google Driveì˜ '/content/drive/MyDrive/gis_project/final_dataset_per_file' ê²½ë¡œì—\n","# ì´ë¯¸ì§€ íƒ€ì¼(images/tile_X.pt)ê³¼ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬(labels_dict/tile_X.pt)ê°€\n","# ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.\n","# ------------------------------------"],"metadata":{"id":"wc-FhJj-m8gk","executionInfo":{"status":"ok","timestamp":1762799217414,"user_tz":-540,"elapsed":50,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# --- 3. ğŸ§  ëª¨ë¸ í—¬í¼ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ ---\n","\n","from typing import Optional\n","\n","# @title ShapeSpec (pixeldecoder_tem_fpn.ipynb)\n","class ShapeSpec:\n","    \"\"\"\n","    (backboneê³¼ pixeldecoderì—ì„œ ëª¨ë‘ ì‚¬ìš©)\n","    \"\"\"\n","    def __init__(self, channels=None, height=None, width=None, stride=None):\n","        self.channels = channels\n","        self.height = height\n","        self.width = width\n","        self.stride = stride\n","\n","    def __str__(self) -> str:\n","        return f\"ShapeSpec(C={self.channels}, H={self.height}, W={self.width}, S={self.stride})\"\n","\n","    __repr__ = __str__\n","\n","# @title _get_clones (pixeldecoder_tem_fpn.ipynb)\n","def _get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","# @title _get_activation_fn (pixeldecoder_tem_fpn.ipynb)\n","def _get_activation_fn(activation):\n","    \"\"\"Return an activation function given a string\"\"\"\n","    if activation == \"relu\":\n","        return F.relu\n","    if activation == \"gelu\":\n","        return F.gelu\n","    if activation == \"glu\":\n","        return F.glu\n","    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n","\n","# @title get_norm (pixeldecoder_tem_fpn.ipynb)\n","def get_norm(norm, out_channels):\n","    if norm is None:\n","        return None\n","    if isinstance(norm, str):\n","        if norm == \"\" or norm.lower() == \"none\":\n","            return None\n","        if norm == \"BN\":\n","            return nn.BatchNorm2d(out_channels)\n","        if norm == \"SyncBN\":\n","            return nn.SyncBatchNorm(out_channels)\n","        if norm == \"GN\":\n","            groups = 32 if out_channels % 32 == 0 else max(1, min(32, out_channels))\n","            return nn.GroupNorm(groups, out_channels)\n","        if norm == \"LN\":\n","            class _ChannelsFirstLayerNorm(nn.Module):\n","                def __init__(self, num_channels, eps=1e-6):\n","                    super().__init__()\n","                    self.weight = nn.Parameter(torch.ones(num_channels))\n","                    self.bias = nn.Parameter(torch.zeros(num_channels))\n","                    self.eps = eps\n","                def forward(self, x):\n","                    mean = x.mean(dim=1, keepdim=True)\n","                    var = (x - mean).pow(2).mean(dim=1, keepdim=True)\n","                    x = (x - mean) / torch.sqrt(var + self.eps)\n","                    return x * self.weight[:, None, None] + self.bias[:, None, None]\n","            return _ChannelsFirstLayerNorm(out_channels)\n","        raise ValueError(...)\n","    if callable(norm):\n","        return norm(out_channels)\n","    if isinstance(norm, nn.Module):\n","        return norm\n","    raise TypeError(...)\n","\n","# @title Conv2d (pixeldecoder_tem_fpn.ipynb)\n","class Conv2d(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size,\n","        stride=1,\n","        padding=0,\n","        dilation=1,\n","        groups=1,\n","        bias=True,\n","        norm=None,\n","        activation=None,\n","    ):\n","        super().__init__()\n","        self.conv = nn.Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            dilation=dilation,\n","            groups=groups,\n","            bias=bias if norm is None else False,\n","        )\n","        self.norm = norm\n","        self.activation = activation\n","\n","    @property\n","    def weight(self):\n","        return self.conv.weight\n","\n","    @property\n","    def bias(self):\n","        return self.conv.bias\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        if self.activation is not None:\n","            x = self.activation(x) if callable(self.activation) else self.activation(x)\n","        return x\n","\n","# @title configurable (pixeldecoder_tem_fpn.ipynb)\n","# fvcoreì˜ configurable ë°ì½”ë ˆì´í„°ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤\n","def configurable(init_func=None, *, from_config=None):\n","    if init_func is not None:\n","        assert (\n","            inspect.isfunction(init_func)\n","            and from_config is None\n","            and init_func.__name__ == \"__init__\"\n","        ), \"Incorrect use of @configurable. Check API documentation for examples.\"\n","\n","        @functools.wraps(init_func)\n","        def wrapped(self, *args, **kwargs):\n","            try:\n","                from_config_func = type(self).from_config\n","            except AttributeError as e:\n","                raise AttributeError(\n","                    \"Class with @configurable must have a 'from_config' classmethod.\"\n","                ) from e\n","            if not inspect.ismethod(from_config_func):\n","                raise TypeError(\"Class with @configurable must have a 'from_config' classmethod.\")\n","\n","            if _called_with_cfg(*args, **kwargs):\n","                explicit_args = _get_args_from_config(from_config_func, *args, **kwargs)\n","                init_func(self, **explicit_args)\n","            else:\n","                init_func(self, *args, **kwargs)\n","\n","        return wrapped\n","    else:\n","        if from_config is None:\n","            return configurable\n","        assert inspect.isfunction(\n","            from_config\n","        ), \"from_config argument of configurable must be a function!\"\n","\n","        def wrapper(orig_func):\n","            @functools.wraps(orig_func)\n","            def wrapped(*args, **kwargs):\n","                if _called_with_cfg(*args, **kwargs):\n","                    explicit_args = _get_args_from_config(from_config, *args, **kwargs)\n","                    return orig_func(**explicit_args)\n","                else:\n","                    return orig_func(*args, **kwargs)\n","\n","            wrapped.from_config = from_config\n","            return wrapped\n","\n","        return wrapper\n","\n","def _get_args_from_config(from_config_func, *args, **kwargs):\n","    signature = inspect.signature(from_config_func)\n","    if list(signature.parameters.keys())[0] != \"cfg\":\n","        if inspect.isfunction(from_config_func):\n","            name = from_config_func.__name__\n","        else:\n","            name = f\"{from_config_func.__self__}.from_config\"\n","        raise TypeError(f\"{name} must take 'cfg' as the first argument!\")\n","    support_var_arg = any(\n","        param.kind in [param.VAR_POSITIONAL, param.VAR_KEYWORD]\n","        for param in signature.parameters.values()\n","    )\n","    if support_var_arg:\n","        ret = from_config_func(*args, **kwargs)\n","    else:\n","        supported_arg_names = set(signature.parameters.keys())\n","        extra_kwargs = {}\n","        for name in list(kwargs.keys()):\n","            if name not in supported_arg_names:\n","                extra_kwargs[name] = kwargs.pop(name)\n","        ret = from_config_func(*args, **kwargs)\n","        ret.update(extra_kwargs)\n","    return ret\n","\n","\n","def _called_with_cfg(*args, **kwargs):\n","    # omegaconfëŠ” ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","    try:\n","        from omegaconf import DictConfig\n","    except ImportError:\n","        # ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì„ì‹œ í´ë˜ìŠ¤\n","        class DictConfig:\n","            pass\n","\n","    if len(args) and isinstance(args[0], (_CfgNode, DictConfig)):\n","        return True\n","    if isinstance(kwargs.pop(\"cfg\", None), (_CfgNode, DictConfig)):\n","        return True\n","    return False"],"metadata":{"id":"2yudHzGcm8jD","executionInfo":{"status":"ok","timestamp":1762799217425,"user_tz":-540,"elapsed":6,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# 4. ëª¨ë¸ ì •ì˜: Backbone (Swin Transformer)\n","# (backboneì½”ë“œ.ipynb íŒŒì¼ì—ì„œ ë³µì‚¬)\n","\n","# ì´ë¯¸ì§€ ìª¼ê°œê¸°\n","def window_partition(x,window_size):\n","  B,H,W,C = x.shape\n","  x=x.view(B,H//window_size,window_size,W//window_size,window_size,C)\n","  windows=x.permute(0,1,3,2,4,5).contiguous().view(-1,window_size,window_size,C)\n","  return windows\n","\n","# ì´ë¯¸ì§€ í•©ì¹˜ê¸°\n","def window_reverse(windows,window_size,H,W):\n","  B = int(windows.shape[0] / (H * W / window_size / window_size))\n","  x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","  x = x.permute(0,1,3,2,4,5).contiguous().view(B, H, W, -1)\n","  return x\n","\n","# ì´ë¯¸ì§€ -> í† í°ìœ¼ë¡œ ë³€í˜•\n","class PatchEmbed(nn.Module):\n","    def __init__(self, patch_size=4, in_chans=3, embed_dim=96,norm_layer=None):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) #ì´ë¯¸ì§€ ë¶„í• \n","        self.norm = norm_layer\n","\n","    def forward(self, x):\n","        _, _, H, W = x.size()\n","        if W % self.patch_size != 0:\n","            x = F.pad(x, (0, self.patch_size - W % self.patch_size))\n","        if H % self.patch_size != 0:\n","            x = F.pad(x, (0, 0, 0, self.patch_size - H % self.patch_size))\n","        x = self.proj(x)\n","        return x\n","\n","# í•´ìƒë„ ì ˆë°˜, í”¼ì²˜ 2ë°°\n","class PatchMerging(nn.Module):\n","  def __init__(self,dim,norm_layer=nn.LayerNorm):\n","    super().__init__()\n","    self.dim = dim\n","    self.reduction = nn.Linear(4*dim,2*dim,bias=False)\n","    self.norm = norm_layer(4*dim)\n","\n","  def forward(self,x,H,W):\n","    B,L,C=x.shape\n","    assert L==H*W, \"Swin PatchMergingí´ë˜ìŠ¤ HWì˜¤ë¥˜\"\n","    x=x.view(B,H,W,C)\n","    pad_input=(H%2==1)or(W%2==1)\n","    if pad_input:\n","      x=F.pad(x,(0,0,0,W%2,0,H%2))\n","\n","    x0 = x[:, 0::2, 0::2, :]\n","    x1 = x[:, 1::2, 0::2, :]\n","    x2 = x[:, 0::2, 1::2, :]\n","    x3 = x[:, 1::2, 1::2, :]\n","    x = torch.cat([x0, x1, x2, x3], -1)\n","    x = x.view(B, -1, 4 * C)\n","\n","    x = self.norm(x)\n","    x = self.reduction(x)\n","    return x\n","\n","# Multilayer perceptron (FFN)\n","class Mlp(nn.Module):\n","    def __init__(\n","        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","# ìœˆë„ìš° ë©€í‹°í—¤ë“œ ì…€í”„ì–´í…ì…˜\n","class WindowAttention(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        window_size,\n","        num_heads,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        attn_drop=0.0,\n","        proj_drop=0.0,\n","    ):\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )\n","\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n","        coords_flatten = torch.flatten(coords, 1)\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","        relative_coords[:, :, 0] += self.window_size[0] - 1\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        B_, N, C = x.shape\n","        qkv = (\n","            self.qkv(x)\n","            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.view(-1)\n","        ].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )\n","        relative_position_bias = relative_position_bias.permute(\n","            2, 0, 1\n","        ).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","# Swin Transformer ë¸”ë¡\n","class SwinTransformerBlock(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        num_heads,\n","        window_size=7,\n","        shift_size=0,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        drop=0.0,\n","        attn_drop=0.0,\n","        drop_path=0.0,\n","        act_layer=nn.GELU,\n","        norm_layer=nn.LayerNorm,\n","    ):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=to_2tuple(self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            qk_scale=qk_scale,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(\n","            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop\n","        )\n","\n","        self.H = None\n","        self.W = None\n","\n","    def forward(self, x, mask_matrix):\n","        B, L, C = x.shape\n","        H, W = self.H, self.W\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        pad_l = pad_t = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n","\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if pad_r > 0 or pad_b > 0:\n","            x = x[:, :H, :W, :].contiguous()\n","\n","        x = x.view(B, H * W, C)\n","\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# Stage: BasicLayer\n","class BasicLayer(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        depth,\n","        num_heads,\n","        window_size=7,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        drop=0.0,\n","        attn_drop=0.0,\n","        drop_path=0.0,\n","        norm_layer=nn.LayerNorm,\n","        downsample=None,\n","        use_checkpoint=False,\n","    ):\n","        super().__init__()\n","        self.window_size = window_size\n","        self.shift_size = window_size // 2\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                SwinTransformerBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    window_size=window_size,\n","                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    qk_scale=qk_scale,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                    norm_layer=norm_layer,\n","                )\n","                for i in range(depth)\n","            ]\n","        )\n","\n","        if downsample is not None:\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def forward(self, x, H, W):\n","        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n","        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n","        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)\n","        h_slices = (\n","            slice(0, -self.window_size),\n","            slice(-self.window_size, -self.shift_size),\n","            slice(-self.shift_size, None),\n","        )\n","        w_slices = (\n","            slice(0, -self.window_size),\n","            slice(-self.window_size, -self.shift_size),\n","            slice(-self.shift_size, None),\n","        )\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","        for blk in self.blocks:\n","            blk.H, blk.W = H, W\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x, attn_mask)\n","            else:\n","                x = blk(x, attn_mask)\n","        if self.downsample is not None:\n","            x_down = self.downsample(x, H, W)\n","            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n","            return x, H, W, x_down, Wh, Ww\n","        else:\n","            return x, H, W, x, H, W\n","\n","# SwinTransformer\n","class SwinTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        pretrain_img_size=224,\n","        patch_size=4,\n","        in_chans=3,\n","        embed_dim=96,\n","        depths=[2, 2, 6, 2],\n","        num_heads=[3, 6, 12, 24],\n","        window_size=7,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        drop_rate=0.0,\n","        attn_drop_rate=0.0,\n","        drop_path_rate=0.2,\n","        norm_layer=nn.LayerNorm,\n","        ape=False,\n","        patch_norm=True,\n","        out_indices=(0, 1, 2, 3),\n","        frozen_stages=-1,\n","        use_checkpoint=False,\n","    ):\n","        super().__init__()\n","\n","        self.pretrain_img_size = pretrain_img_size\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.ape = ape\n","        self.patch_norm = patch_norm\n","        self.out_indices = out_indices\n","        self.frozen_stages = frozen_stages\n","\n","        self.patch_embed = PatchEmbed(\n","            patch_size=patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None,\n","        )\n","\n","        if self.ape:\n","            pretrain_img_size = to_2tuple(pretrain_img_size)\n","            patch_size = to_2tuple(patch_size)\n","            patches_resolution = [\n","                pretrain_img_size[0] // patch_size[0],\n","                pretrain_img_size[1] // patch_size[1],\n","            ]\n","            self.absolute_pos_embed = nn.Parameter(\n","                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1])\n","            )\n","            trunc_normal_(self.absolute_pos_embed, std=0.02)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2 ** i_layer),\n","                depth=depths[i_layer],\n","                num_heads=num_heads[i_layer],\n","                window_size=window_size,\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                qk_scale=qk_scale,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n","                norm_layer=norm_layer,\n","                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n","                use_checkpoint=use_checkpoint,\n","            )\n","            self.layers.append(layer)\n","\n","        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n","        self.num_features = num_features\n","\n","        for i_layer in out_indices:\n","            layer = norm_layer(num_features[i_layer])\n","            layer_name = f\"norm{i_layer}\"\n","            self.add_module(layer_name, layer)\n","\n","        self._freeze_stages()\n","\n","    def _freeze_stages(self):\n","        if self.frozen_stages >= 0:\n","            self.patch_embed.eval()\n","            for param in self.patch_embed.parameters():\n","                param.requires_grad = False\n","        if self.frozen_stages >= 1 and self.ape:\n","            self.absolute_pos_embed.requires_grad = False\n","        if self.frozen_stages >= 2:\n","            self.pos_drop.eval()\n","            for i in range(0, self.frozen_stages - 1):\n","                m = self.layers[i]\n","                m.eval()\n","                for param in m.parameters():\n","                    param.requires_grad = False\n","\n","    def init_weights(self, pretrained=None):\n","        def _init_weights(m):\n","            if isinstance(m, nn.Linear):\n","                trunc_normal_(m.weight, std=0.02)\n","                if isinstance(m, nn.Linear) and m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.LayerNorm):\n","                nn.init.constant_(m.bias, 0)\n","                nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","        Wh, Ww = x.size(2), x.size(3)\n","        if self.ape:\n","            absolute_pos_embed = F.interpolate(\n","                self.absolute_pos_embed, size=(Wh, Ww), mode=\"bicubic\"\n","            )\n","            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)\n","        else:\n","            x = x.flatten(2).transpose(1, 2)\n","        x = self.pos_drop(x)\n","\n","        outs = {}\n","        for i in range(self.num_layers):\n","            layer = self.layers[i]\n","            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n","\n","            if i in self.out_indices:\n","                norm_layer = getattr(self, f\"norm{i}\")\n","                x_out = norm_layer(x_out)\n","                out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n","                outs[\"res{}\".format(i + 2)] = out\n","\n","        return outs\n","\n","    def train(self, mode=True):\n","        super(SwinTransformer, self).train(mode)\n","        self._freeze_stages()\n","\n","# ë¸Œë¦¿ì§€ í´ë˜ìŠ¤ (Backboneìš©)\n","class D2SwinTransformer(SwinTransformer):\n","    @configurable\n","    def __init__(self, *, input_shape, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        # cfg ëŒ€ì‹  input_shapeì„ ì§ì ‘ ë°›ì•„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •\n","        # self._out_features = cfg.MODEL.SWIN.OUT_FEATURES # ì›ë³¸\n","        self._out_features = [\"res2\", \"res3\", \"res4\", \"res5\"] # ì„ì‹œ í•˜ë“œì½”ë”© (cfgì—ì„œ ë°›ì•„ì˜¤ë„ë¡ ìˆ˜ì • í•„ìš”)\n","\n","        self._out_feature_strides = {\n","            \"res2\": 4,\n","            \"res3\": 8,\n","            \"res4\": 16,\n","            \"res5\": 32,\n","        }\n","        self._out_feature_channels = {\n","            \"res2\": self.num_features[0],\n","            \"res3\": self.num_features[1],\n","            \"res4\": self.num_features[2],\n","            \"res5\": self.num_features[3],\n","        }\n","\n","    @classmethod\n","    def from_config(cls, cfg, input_shape):\n","        # ì´ ë¶€ë¶„ì€ cfg ê°ì²´ë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•Œ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤.\n","        # backboneì½”ë“œ.ipynbì˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ë„ë¡ kwargsë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n","        kwargs = {\n","            \"pretrain_img_size\": cfg.MODEL.SWIN.PRETRAIN_IMG_SIZE,\n","            \"patch_size\": cfg.MODEL.SWIN.PATCH_SIZE,\n","            \"in_chans\": input_shape.channels, # cfg.MODEL.SWIN.IN_CHANS ëŒ€ì‹  input_shape ì‚¬ìš©\n","            \"embed_dim\": cfg.MODEL.SWIN.EMBED_DIM,\n","            \"depths\": cfg.MODEL.SWIN.DEPTHS,\n","            \"num_heads\": cfg.MODEL.SWIN.NUM_HEADS,\n","            \"window_size\": cfg.MODEL.SWIN.WINDOW_SIZE,\n","            \"mlp_ratio\": cfg.MODEL.SWIN.MLP_RATIO,\n","            \"qkv_bias\": cfg.MODEL.SWIN.QKV_BIAS,\n","            \"qk_scale\": cfg.MODEL.SWIN.QK_SCALE,\n","            \"drop_rate\": cfg.MODEL.SWIN.DROP_RATE,\n","            \"attn_drop_rate\": cfg.MODEL.SWIN.ATTN_DROP_RATE,\n","            \"drop_path_rate\": cfg.MODEL.SWIN.DROP_PATH_RATE,\n","            \"norm_layer\": nn.LayerNorm, # cfg.MODEL.SWIN.NORM_LAYER ê°’ì— ë”°ë¼ ìˆ˜ì •\n","            \"ape\": cfg.MODEL.SWIN.APE,\n","            \"patch_norm\": cfg.MODEL.SWIN.PATCH_NORM,\n","            \"use_checkpoint\": cfg.MODEL.SWIN.USE_CHECKPOINT,\n","            \"out_indices\": cfg.MODEL.SWIN.OUT_INDICES\n","        }\n","        return {\"input_shape\": input_shape, **kwargs}\n","\n","\n","    def forward(self, x):\n","        assert (\n","            x.dim() == 4\n","        ), f\"ì…ë ¥í…ì„œê°€ (N, C, H, W).ê°€ ì•„ë‹Œ {x.shape}\"\n","        outputs = {}\n","        y = super().forward(x)\n","        for k in y.keys():\n","            if k in self._out_features:\n","                outputs[k] = y[k]\n","        return outputs\n","\n","    def output_shape(self):\n","        return {\n","            name: ShapeSpec(\n","                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n","            )\n","            for name in self._out_features\n","        }\n","\n","    @property\n","    def size_divisibility(self):\n","        return 32"],"metadata":{"id":"Qaa1edXZm8lq","executionInfo":{"status":"ok","timestamp":1762799217549,"user_tz":-540,"elapsed":119,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# --- 5. ğŸ§  ëª¨ë¸: 2ë¶€ (Pixel Decoder) ---\n","# (pixeldecoder_tem_fpn...ipynb íŒŒì¼ì—ì„œ ë³µì‚¬)\n","\n","# @title BasePixelDecoder (pixeldecoder_tem_fpn.ipynb)\n","# BasePixelDecoderëŠ” FPN ìŠ¤íƒ€ì¼ì´ë©°, TransformerEncoderPixelDecoderì˜ ë¶€ëª¨ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n","class BasePixelDecoder(nn.Module):\n","    @configurable\n","    def __init__(\n","        self,\n","        input_shape: Dict[str, ShapeSpec],\n","        *,\n","        conv_dim: int,\n","        mask_dim: int,\n","        norm: Optional[Union[str, Callable]] = None,\n","    ):\n","        \"\"\"\n","        NOTE: this interface is experimental.\n","        Args:\n","            input_shape: shapes (channels and stride) of the input features\n","            conv_dims: number of output channels for the intermediate conv layers.\n","            mask_dim: number of output channels for the final conv layer.\n","            norm (str or callable): normalization for all conv layers\n","        \"\"\"\n","        super().__init__()\n","\n","        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n","        self.in_features = [k for k, v in input_shape]  # res2 ~ res5\n","        feature_channels = [v.channels for k, v in input_shape]\n","\n","        lateral_convs = []\n","        output_convs = []\n","\n","        use_bias = norm == \"\"\n","        for idx, in_channels in enumerate(feature_channels):\n","            if idx == len(self.in_features) - 1: # ë§ˆì§€ë§‰ ë ˆì´ì–´ (res5)\n","                output_norm = get_norm(norm, conv_dim)\n","                output_conv = Conv2d(\n","                    in_channels,\n","                    conv_dim,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=1,\n","                    bias=use_bias,\n","                    norm=output_norm,\n","                    activation=F.relu,\n","                )\n","                weight_init.c2_xavier_fill(output_conv)\n","                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n","\n","                lateral_convs.append(None)\n","                output_convs.append(output_conv)\n","            else: # res2, res3, res4\n","                lateral_norm = get_norm(norm, conv_dim)\n","                output_norm = get_norm(norm, conv_dim)\n","\n","                lateral_conv = Conv2d(\n","                    in_channels, conv_dim, kernel_size=1, bias=use_bias, norm=lateral_norm\n","                )\n","                output_conv = Conv2d(\n","                    conv_dim,\n","                    conv_dim,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=1,\n","                    bias=use_bias,\n","                    norm=output_norm,\n","                    activation=F.relu,\n","                )\n","                weight_init.c2_xavier_fill(lateral_conv)\n","                weight_init.c2_xavier_fill(output_conv)\n","                self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n","                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n","\n","                lateral_convs.append(lateral_conv)\n","                output_convs.append(output_conv)\n","\n","        self.lateral_convs = lateral_convs[::-1]\n","        self.output_convs = output_convs[::-1]\n","\n","        self.mask_dim = mask_dim\n","        self.mask_features = nn.Conv2d(\n","            conv_dim,\n","            mask_dim,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","        )\n","        weight_init.c2_xavier_fill(self.mask_features)\n","\n","        self.maskformer_num_feature_levels = 3  # 3ê°œì˜ ìŠ¤ì¼€ì¼ ì‚¬ìš©\n","\n","    @classmethod\n","    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n","        ret = {}\n","        # cfgê°€ ì—†ìœ¼ë©´ AttributeErrorê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ try-except ì¶”ê°€ (ë¼ˆëŒ€ìš©)\n","        try:\n","            ret[\"input_shape\"] = {\n","                k: v for k, v in input_shape.items() if k in cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES\n","            }\n","            ret[\"conv_dim\"] = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","            ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","            ret[\"norm\"] = cfg.MODEL.SEM_SEG_HEAD.NORM\n","        except AttributeError:\n","             print(\"ê²½ê³ : cfg ê°ì²´ê°€ ì—†ê±°ë‚˜ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. BasePixelDecoder ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n","             ret[\"input_shape\"] = {k: v for k, v in input_shape.items() if k in [\"res2\", \"res3\", \"res4\", \"res5\"]}\n","             ret[\"conv_dim\"] = 256\n","             ret[\"mask_dim\"] = 256\n","             ret[\"norm\"] = \"GN\"\n","        return ret\n","\n","    def forward_features(self, features):\n","        multi_scale_features = []\n","        num_cur_levels = 0\n","        y = None # Top-down ì‹œì‘ì„ ìœ„í•´ ì´ˆê¸°í™”\n","\n","        # Reverse feature maps into top-down order (from low to high resolution)\n","        for idx, f in enumerate(self.in_features[::-1]):\n","            x = features[f]\n","            lateral_conv = self.lateral_convs[idx]\n","            output_conv = self.output_convs[idx]\n","            if lateral_conv is None:\n","                y = output_conv(x)\n","            else:\n","                if y is None:\n","                    raise ValueError(\"y (top-down feature) is not initialized in BasePixelDecoder.\")\n","\n","                cur_fpn = lateral_conv(x)\n","                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n","                y = output_conv(y)\n","\n","            if num_cur_levels < self.maskformer_num_feature_levels:\n","                multi_scale_features.append(y)\n","                num_cur_levels += 1\n","\n","        return self.mask_features(y), None, multi_scale_features\n","\n","    def forward(self, features, targets=None):\n","        # logger = logging.getLogger(__name__)\n","        # logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n","        return self.forward_features(features)\n","\n","\n","# @title TransformerEncoderLayer (pixeldecoder_tem_fpn.ipynb)\n","# Transformer Encoder Layer\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(\n","        self,\n","        d_model,\n","        nhead,\n","        dim_feedforward=2048,\n","        dropout=0.1,\n","        activation=\"relu\",\n","        normalize_before=False,\n","    ):\n","        super().__init__()\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.activation = _get_activation_fn(activation)\n","        self.normalize_before = normalize_before\n","\n","    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n","        return tensor if pos is None else tensor + pos\n","\n","    def forward_post(\n","        self,\n","        src,\n","        src_mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        q = k = self.with_pos_embed(src, pos)\n","        src2 = self.self_attn(\n","            q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n","        )[0]\n","        src = src + self.dropout1(src2)\n","        src = self.norm1(src)\n","        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n","        src = src + self.dropout2(src2)\n","        src = self.norm2(src)\n","        return src\n","\n","    def forward_pre(\n","        self,\n","        src,\n","        src_mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        src2 = self.norm1(src)\n","        q = k = self.with_pos_embed(src2, pos)\n","        src2 = self.self_attn(\n","            q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n","        )[0]\n","        src = src + self.dropout1(src2)\n","        src2 = self.norm2(src)\n","        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n","        src = src + self.dropout2(src2)\n","        return src\n","\n","    def forward(\n","        self,\n","        src,\n","        src_mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        if self.normalize_before:\n","            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n","        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n","\n","# @title TransformerEncoder (pixeldecoder_tem_fpn.ipynb)\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers, norm=None):\n","        super().__init__()\n","        self.layers = _get_clones(encoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","\n","    def forward(\n","        self,\n","        src,\n","        mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        output = src\n","\n","        for layer in self.layers:\n","            output = layer(\n","                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos\n","            )\n","\n","        if self.norm is not None:\n","            output = self.norm(output)\n","\n","        return output\n","\n","# @title TransformerEncoderOnly (pixeldecoder_tem_fpn.ipynb)\n","class TransformerEncoderOnly(nn.Module):\n","    def __init__(\n","        self,\n","        d_model=512,\n","        nhead=8,\n","        num_encoder_layers=6,\n","        dim_feedforward=2048,\n","        dropout=0.1,\n","        activation=\"relu\",\n","        normalize_before=False,\n","    ):\n","        super().__init__()\n","\n","        encoder_layer = TransformerEncoderLayer(\n","            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n","        )\n","        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n","        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n","\n","        self._reset_parameters()\n","\n","        self.d_model = d_model\n","        self.nhead = nhead\n","\n","    def _reset_parameters(self):\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","    def forward(self, src, mask, pos_embed):\n","        # flatten NxCxHxW to HWxNxC\n","        bs, c, h, w = src.shape\n","        src = src.flatten(2).permute(2, 0, 1)\n","        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n","        if mask is not None:\n","            mask = mask.flatten(1)\n","\n","        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n","        return memory.permute(1, 2, 0).view(bs, c, h, w)\n","\n","# @title TransformerEncoderPixelDecoder (pixeldecoder_tem_fpn.ipynb)\n","class TransformerEncoderPixelDecoder(BasePixelDecoder):\n","    @configurable\n","    def __init__(\n","        self,\n","        input_shape: Dict[str, ShapeSpec],\n","        *,\n","        transformer_dropout: float,\n","        transformer_nheads: int,\n","        transformer_dim_feedforward: int,\n","        transformer_enc_layers: int,\n","        transformer_pre_norm: bool,\n","        conv_dim: int,\n","        mask_dim: int,\n","        norm: Optional[Union[str, Callable]] = None,\n","    ):\n","        \"\"\"\n","        NOTE: this interface is experimental.\n","        Args:\n","            input_shape: shapes (channels and stride) of the input features\n","            transformer_dropout: dropout probability in transformer\n","            transformer_nheads: number of heads in transformer\n","            transformer_dim_feedforward: dimension of feedforward network\n","            transformer_enc_layers: number of transformer encoder layers\n","            transformer_pre_norm: whether to use pre-layernorm or not\n","            conv_dims: number of output channels for the intermediate conv layers.\n","            mask_dim: number of output channels for the final conv layer.\n","            norm (str or callable): normalization for all conv layers\n","        \"\"\"\n","        super().__init__(input_shape, conv_dim=conv_dim, mask_dim=mask_dim, norm=norm)\n","\n","        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n","        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n","        feature_strides = [v.stride for k, v in input_shape]\n","        feature_channels = [v.channels for k, v in input_shape]\n","\n","        in_channels = feature_channels[len(self.in_features) - 1]\n","        self.input_proj = Conv2d(in_channels, conv_dim, kernel_size=1)\n","        weight_init.c2_xavier_fill(self.input_proj)\n","        self.transformer = TransformerEncoderOnly(\n","            d_model=conv_dim,\n","            dropout=transformer_dropout,\n","            nhead=transformer_nheads,\n","            dim_feedforward=transformer_dim_feedforward,\n","            num_encoder_layers=transformer_enc_layers,\n","            normalize_before=transformer_pre_norm,\n","        )\n","        # N_steps = conv_dim // 2 # Original incorrect line\n","        # --- Fix: Initialize pixel_position_embedding with conv_dim ---\n","        self.pe_layer = pixel_position_embedding(conv_dim)\n","        # --- End Fix ---\n","\n","        # update layer\n","        use_bias = norm == \"\"\n","        output_norm = get_norm(norm, conv_dim)\n","        output_conv = Conv2d(\n","            conv_dim,\n","            conv_dim,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=use_bias,\n","            norm=output_norm,\n","            activation=F.relu,\n","        )\n","        weight_init.c2_xavier_fill(output_conv)\n","        delattr(self, \"layer_{}\".format(len(self.in_features)))\n","        self.add_module(\"layer_{}\".format(len(self.in_features)), output_conv)\n","        self.output_convs[0] = output_conv\n","\n","    @classmethod\n","    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n","        ret = super().from_config(cfg, input_shape)\n","\n","        # (cfgê°€ ì—†ìœ¼ë©´ AttributeError ë°œìƒ)\n","        try:\n","            ret[\"transformer_dropout\"] = cfg.MODEL.MASK_FORMER.DROPOUT\n","            ret[\"transformer_nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n","            ret[\"transformer_dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n","            ret[\n","                \"transformer_enc_layers\"\n","            ] = cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS  # a separate config\n","            ret[\"transformer_pre_norm\"] = cfg.MODEL.MASK_FORMER.PRE_NORM\n","        except AttributeError:\n","             print(\"ê²½ê³ : cfg ê°ì²´ê°€ ì—†ê±°ë‚˜ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. TransformerEncoderPixelDecoder ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n","             ret[\"transformer_dropout\"] = 0.1\n","             ret[\"transformer_nheads\"] = 8\n","             ret[\"transformer_dim_feedforward\"] = 1024 # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ê°’\n","             ret[\"transformer_enc_layers\"] = 3 # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ê°’\n","             ret[\"transformer_pre_norm\"] = False\n","\n","        return ret\n","\n","    def forward_features(self, features):\n","        multi_scale_features = []\n","        num_cur_levels = 0\n","        y = None # yë¥¼ Noneìœ¼ë¡œ ì´ˆê¸°í™” (ì›ë³¸ ì½”ë“œ)\n","\n","        # Reverse feature maps into top-down order (from low to high resolution)\n","        for idx, f in enumerate(self.in_features[::-1]):\n","            x = features[f]\n","            lateral_conv = self.lateral_convs[idx]\n","            output_conv = self.output_convs[idx]\n","            if lateral_conv is None:\n","                transformer = self.input_proj(x)\n","                pos = self.pe_layer(x)\n","                transformer = self.transformer(transformer, None, pos)\n","                y = output_conv(transformer)\n","                # save intermediate feature as input to Transformer decoder\n","                transformer_encoder_features = transformer\n","            else:\n","                cur_fpn = lateral_conv(x)\n","                # Following FPN implementation, we use nearest upsampling here\n","                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n","                y = output_conv(y)\n","            if num_cur_levels < self.maskformer_num_feature_levels:\n","                multi_scale_features.append(y)\n","                num_cur_levels += 1\n","        return self.mask_features(y), transformer_encoder_features, multi_scale_features\n","\n","    def forward(self, features, targets=None):\n","        # logger = logging.getLogger(__name__)\n","        # logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n","        return self.forward_features(features)"],"metadata":{"id":"Fv7psmhZ6Dpd","executionInfo":{"status":"ok","timestamp":1762799217668,"user_tz":-540,"elapsed":118,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# --- 6. ğŸ§  ëª¨ë¸: 3ë¶€ (Transformer Decoder) ---\n","# (TransformDecoder.py íŒŒì¼ì—ì„œ ë³µì‚¬)\n","\n","# 1ë‹¨ê³„ : masked attention + add & norm\n","class masked_attention(nn.Module):\n","\n","    #c_dim: feature dimension = query dimension\n","    #n_head: ë©€í‹°í•´ë” ìˆ˜\n","    #ë…¼ë¬¸ì—ì„œ dropoutì´ 0ì¼ ë•Œê°€ í¼í¬ë¨¼ìŠ¤ê°€ ê°€ì¥ ì¢‹ì•˜ë‹¤ê³  í•´ì„œ defaultë¥¼ 0ìœ¼ë¡œ ì„¤ì •\n","    def __init__(self, C_dim, n_head, dropout=0.0):\n","\n","        super(masked_attention, self).__init__()\n","        self.multihead_attn = nn.MultiheadAttention(C_dim, n_head, dropout=dropout, batch_first=True)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.norm = nn.LayerNorm(C_dim)\n","\n","        #parameter ì´ˆê¸°í™”, xavier_uniform_ ì‚¬ìš©\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for par in self.parameters():\n","            if par.dim() > 1:\n","                nn.init.xavier_uniform_(par)\n","\n","    # queryì™€ keyì— ìœ„ì¹˜ ì„ë² ë”© ë”í•˜ê¸° ìœ„í•´ êµ¬í˜„\n","    def add_pos_embedding(self, x: Tensor, pos: Tensor) -> Tensor:\n","        return x if pos is None else x + pos\n","\n","    # x_last: ê·¸ì „ ë‹¨ê³„ì˜ ì¶œë ¥ê°’\n","    # img_feat: ì´ë¯¸ì§€ feature map(C_dimìœ¼ë¡œ ì°¨ì›ë³€í™˜ ëœ)\n","    # mask: ë§ˆìŠ¤í¬ í…ì„œ / í‚¤ ë§ˆìŠ¤í¬ëŠ” ì—†ëŠ” ê²ƒìœ¼ë¡œ ì„¤ì • -> inputìœ¼ë¡œ ë„£ê¸° ì „ì— ì „ì²˜ë¦¬ ë”°ë¡œ í•  ì˜ˆì •\n","    # query_pos: ì¿¼ë¦¬ í¬ì§€ì…˜ ì„ë² ë”© (ì¿¼ë¦¬ê°€ ì–´ë–¤ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ì„ë² ë”©)\n","    # pixel_pos: í”½ì…€ í¬ì§€ì…˜ ì„ë² ë”© (ì´ë¯¸ì§€ í”½ì…€ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë‹´ì€ ì„ë² ë”©)\n","    def forward(self, x_last, img_feat: Tensor,\n","                mask: Tensor,\n","                query_pos: Tensor, pixel_pos: Tensor\n","                ):\n","        # masked attention\n","        mask_attened = self.multihead_attn(query = self.add_pos_embedding(x_last, query_pos),\n","                                        key = self.add_pos_embedding(img_feat, pixel_pos),\n","                                        value = img_feat,\n","                                        attn_mask = mask,\n","                                        key_padding_mask = None\n","                                        )[0]\n","        # add & norm\n","        # ë…¼ë¬¸ì— ë‚˜ì˜¤ëŠ” add&norm ë¶€ë¶„ì„ masked attention ëª¨ë“ˆ ì•ˆì— êµ¬í˜„\n","        # add: x + f(x) / norm : ì •ê·œí™”(LayerNorm)\n","        x_next = x_last + self.dropout(mask_attened)\n","        x_next = self.norm(x_next)\n","\n","        return x_next\n","\n","# 2ë‹¨ê³„ : self attention + add & norm\n","class self_attention(nn.Module):\n","    #c_dim: feature dimension = query dimension\n","    #n_head: ë©€í‹°í•´ë” ìˆ˜\n","    #ë…¼ë¬¸ì—ì„œ dropoutì´ 0ì¼ ë•Œê°€ í¼í¬ë¨¼ìŠ¤ê°€ ê°€ì¥ ì¢‹ì•˜ë‹¤ê³  í•´ì„œ defaultë¥¼ 0ìœ¼ë¡œ ì„¤ì •\n","    def __init__(self, C_dim, n_head, dropout=0.0):\n","        super(self_attention, self).__init__()\n","        self.self_attn = nn.MultiheadAttention(C_dim, n_head, dropout=dropout, batch_first=True)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.norm = nn.LayerNorm(C_dim)\n","\n","        self.reset_parameters()\n","\n","    #parameter ì´ˆê¸°í™”, xavier_uniform_ ì‚¬ìš©\n","    def reset_parameters(self):\n","        for par in self.parameters():\n","            if par.dim() > 1:\n","                nn.init.xavier_uniform_(par)\n","\n","    # queryì™€ keyì— ìœ„ì¹˜ ì„ë² ë”© ë”í•˜ê¸° ìœ„í•´ êµ¬í˜„\n","    def add_pos_embedding(self, x: Tensor, pos: Tensor) -> Tensor:\n","        return x if pos is None else x + pos\n","\n","    # x_last: ê·¸ì „ ë‹¨ê³„ì˜ ì¶œë ¥ê°’\n","    # mask: ë§ˆìŠ¤í¬ í…ì„œ / í‚¤ ë§ˆìŠ¤í¬ëŠ” ì—†ëŠ” ê²ƒìœ¼ë¡œ ì„¤ì •(inputìœ¼ë¡œ ë„£ê¸° ì „ì— ì „ì²˜ë¦¬ ë”°ë¡œ í•  ì˜ˆì •)\n","    # query_pos: ì¿¼ë¦¬ í¬ì§€ì…˜ ì„ë² ë”© (ì¿¼ë¦¬ê°€ ì–´ë–¤ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ì„ë² ë”©)\n","    # self attentionì´ê¸° ë•Œë¬¸ì— key = value\n","    def forward(self, x_last, mask = None, query_pos =None):\n","        # self attention\n","        Query_and_Key = self.add_pos_embedding(x_last, query_pos)\n","\n","        self_attened = self.self_attn(\n","            query = Query_and_Key,\n","            key = Query_and_Key,\n","            value = x_last,\n","            attn_mask = mask,\n","            key_padding_mask = None\n","            )[0]\n","\n","        # add & norm\n","        # ë…¼ë¬¸ì— ë‚˜ì˜¤ëŠ” add&norm ë¶€ë¶„ì„ self attention ëª¨ë“ˆ ì•ˆì— êµ¬í˜„\n","        # add: x + f(x) / norm : ì •ê·œí™”(LayerNorm)\n","        x_next = x_last + self.dropout(self_attened)\n","        x_next = self.norm(x_next)\n","\n","        return x_next\n","\n","# 3ë‹¨ê³„ : FFN + add & norm\n","class FFN(nn.Module):\n","    #C_dim: feature dimension\n","    #dim_feedforward: FFN ë‚´ë¶€ì˜ hidden layer ì°¨ì›\n","    #ë…¼ë¬¸ì—ì„œ dropoutì´ 0ì¼ ë•Œê°€ í¼í¬ë¨¼ìŠ¤ê°€ ê°€ì¥ ì¢‹ì•˜ë‹¤ê³  í•´ì„œ defaultë¥¼ 0ìœ¼ë¡œ ì„¤ì •\n","    def __init__(self, C_dim, dim_feedforward=2048, dropout=0.0):\n","        super(FFN, self).__init__()\n","\n","        self.first_linear = nn.Linear(C_dim, dim_feedforward)\n","        self.second_linear = nn.Linear(dim_feedforward, C_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.activation = F.relu\n","\n","        self.norm = nn.LayerNorm(C_dim)\n","        #parameter ì´ˆê¸°í™”, xavier_uniform_ ì‚¬ìš©\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for par in self.parameters():\n","            if par.dim() > 1:\n","                nn.init.xavier_uniform_(par)\n","\n","    def forward(self, x_last):\n","        # FFN\n","        # FFN = relu(w1x + b1)W2 + b2\n","        ffn = self.second_linear(self.dropout(self.activation(self.first_linear(x_last))))\n","\n","        # add & norm\n","        # ë…¼ë¬¸ì— ë‚˜ì˜¤ëŠ” add&norm ë¶€ë¶„ì„ FFN ëª¨ë“ˆ ì•ˆì— êµ¬í˜„\n","        # add: x + f(x) / norm : ì •ê·œí™”(LayerNorm)\n","        x_next = x_last + self.dropout(ffn)\n","        x_next = self.norm(x_next)\n","\n","        return x_next\n","\n","# ë§ˆìŠ¤í‚¹ ìœ„í•´ Multi-Layer Perceptron(MLP) êµ¬í˜„\n","class mlp(nn.Module):\n","    def __init__(self, n_layers, input_dim, hidden_dim, output_dim):\n","        super(mlp, self).__init__()\n","\n","        self.n_layers = n_layers\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.layers = self.make_layers()\n","        self.last_layer = nn.Linear(hidden_dim, output_dim)\n","\n","    def make_layers(self):\n","        mlp_layers = [nn.Linear(self.input_dim, self.hidden_dim)]\n","\n","        for _ in range(self.n_layers-2):\n","            mlp_layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n","\n","        return nn.ModuleList(mlp_layers)\n","\n","    def forward(self, input):\n","        for layer in self.layers:\n","            input = F.relu(layer(input))\n","\n","        output = self.last_layer(input)\n","        return output\n","\n","# pixel position embeddingì„ í•˜ê¸° ìœ„í•œ ëª¨ë¸ êµ¬í˜„\n","# sinusoidal position embedding ì‚¬ìš©\n","class pixel_position_embedding(nn.Module):\n","    def __init__(self, d_model):\n","        super(pixel_position_embedding, self).__init__()\n","        self.d_model = d_model\n","\n","    #feature_map : (B, C, H, W)\n","    def forward(self, feature_map: Tensor):\n","        #sinusoidalì„ ì‚¬ìš©í•˜ê³  [H | W] ê°™ì´ Hì™€ Wë¥¼ ë‚˜ëˆ„ì–´ì„œ ì…ë ¥í•´ì•¼ í•´ì„œ d_modelì´ 4ì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨\n","        assert self.d_model % 4 == 0, \"ì°¨ì›ì´ 4ì˜ ë°°ìˆ˜ê°€ ì•„ë‹™ë‹ˆë‹¤.\"\n","\n","        #ì¢Œí‘œ ê²©ì ë§Œë“¤ê¸°\n","        B, H, W = feature_map.size(0), feature_map.size(2), feature_map.size(3)\n","        H_coord, W_coord = torch.meshgrid(torch.arange(1, H+1, dtype=torch.float32, device=feature_map.device),\n","                                          torch.arange(1, W+1, dtype=torch.float32, device=feature_map.device),\n","                                          indexing='ij')\n","\n","        #ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ 0~2ã… \n","        H_coord = H_coord.unsqueeze(0).expand(B, -1, -1) / (H + 1e-6) * (2 * math.pi)\n","        W_coord = W_coord.unsqueeze(0).expand(B, -1, -1) / (W + 1e-6) * (2 * math.pi)\n","\n","        #pos ë‚˜ëˆŒ ë¶„ëª¨ ë§Œë“¤ê¸°\n","        half_d = self.d_model // 2\n","        denom = torch.arange(0, half_d, dtype=torch.float32, device=feature_map.device) // 2\n","        denom = 10000 ** (2*denom / half_d)\n","\n","        #pos/10000^(2i/d_model)\n","        pos_H = H_coord.unsqueeze(-1) / denom\n","        pos_W = W_coord.unsqueeze(-1) / denom\n","\n","        #sin, cos ì ìš©(ì§ìˆ˜ëŠ” sin, í™€ìˆ˜ëŠ” cos)\n","        pos_H[:, :, :, 0::2].sin_()\n","        pos_H[:, :, :, 1::2].cos_()\n","        pos_W[:, :, :, 0::2].sin_()\n","        pos_W[:, :, :, 1::2].cos_()\n","\n","        #Hì™€ W í•©ì¹˜ê¸° ë° (B, d_model, H, W)ë¡œ ì°¨ì› ë³€ê²½\n","        pos_embedding = torch.cat([pos_H, pos_W], dim=-1).permute(0, 3, 1, 2)\n","\n","        # output shape : (B, d_model, H, W)\n","        return pos_embedding\n","\n","class Transformer_Decoder(nn.Module):\n","    \"\"\"\n","    íŒŒë¼ë¯¸í„° ì„¤ëª…ë€\n","    C_dim: feature dimension = query dimension\n","    n_head: ë©€í‹°í—¤ë” ì–´í…ì…˜ì˜ í—¤ë” ìˆ˜\n","    dim_feedforward: FFN ë‚´ë¶€ì˜ hidden layer ì°¨ì›\n","    channel_dim_list: ê° feature mapìœ„ ì±„ë„ ì°¨ì›ìˆ˜ ë¦¬ìŠ¤íŠ¸ [1/32, 1/16, 1/8, 1/4]\n","    mask_dim : 1/4 í•´ìƒë„ì˜ C ì°¨ì›\n","    L: ë””ì½”ë” ë°˜ë³µ íšŸìˆ˜ ë”°ë¼ì„œ ì´ 3Lê°œì˜ ë ˆì´ì–´ê°€ ìˆë‹¤.(ë…¼ë¬¸ì— 3ì„ ì‚¬ìš©í•˜ì—¬ defaultë¥¼ 3ìœ¼ë¡œ ì„¤ì •)\n","    dropout: ë“œë¡­ì•„ì›ƒ (ë…¼ë¬¸ì—ì„œ 0ì¼ë•Œê°€ í¼í¬ë¨¼ìŠ¤ê°€ ê°€ì¥ ì¢‹ì•˜ë‹¤ê³  í•´ì„œ defaultë¥¼ 0ìœ¼ë¡œ ì„¤ì •)\n","    n_query: ì¿¼ë¦¬ì˜ ìˆ˜ (ë…¼ë¬¸ì—ì„œ 100ì„ ì‚¬ìš©í•˜ì—¬ defaultë¥¼ 100ìœ¼ë¡œ ì„¤ì •)\n","    n_class: êµ¬ë¶„í•´ì•¼ í•˜ëŠ” í´ë˜ìˆ˜ ìˆ˜(ê±´ë¬¼ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë§Œ í•  ê²ƒì´ë¯€ë¡œ default 1ë¡œ ì„¤ì •)\n","    \"\"\"\n","    def __init__(self,\n","                C_dim: int,\n","                n_head: int,\n","                dim_feedforward: int,\n","                channel_dim_list: list,\n","                mask_dim: int,\n","                L: int = 3,\n","                dropout: float = 0.0,\n","                n_query: int = 100,\n","                n_class: int = 1):\n","        super(Transformer_Decoder, self).__init__()\n","\n","        #ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì €ì¥\n","        self.channel_dim_list = channel_dim_list\n","        self.L = L\n","        self.C_dim = C_dim\n","        self.n_head = n_head\n","        self.dim_feedforward = dim_feedforward\n","        self.dropout = dropout\n","        self.mask_dim = mask_dim\n","        self.n_class = n_class\n","\n","        # ë””ì½”ë” ë ˆì´ì–´ ë§Œë“¤ê¸°, 3ê°œ ì¸µì„ Lë²ˆ ë°˜ë³µ\n","        self.masked_attn_layers = nn.ModuleList()\n","        self.self_attn_layers = nn.ModuleList()\n","        self.ffn_layers = nn.ModuleList()\n","        self.make_layers()\n","\n","        #ì¿¼ë¦¬ ì„ë² ë”© ë§Œë“¤ê¸°(í•™ìŠµê°€ëŠ¥ ì„ë² ë”©ìœ¼ë¡œ ë§Œë“¤ê¸°)\n","        self.n_query = n_query\n","        self.query_feat_embed = nn.Embedding(n_query, C_dim)\n","        self.query_pos_embed = nn.Embedding(n_query, C_dim)\n","\n","        #í•´ìƒë„ ì„ë² ë”© ë§Œë“¤ê¸°(3ê°€ì§€ í•´ìƒë„ì— ëŒ€í•´ í•™ìŠµê°€ëŠ¥ ì„ë² ë”©ìœ¼ë¡œ ë§Œë“¤ê¸°)\n","        self.resolution_embed = nn.Embedding(3, C_dim)\n","\n","        #ê° feature mapì˜ ì±„ë„ê³¼ C_dimì´ ë‹¤ë¥¼ ë•Œ ì°¨ì›ì„ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ ë§Œë“¤ê¸°\n","        self.projection_modules = self.make_projection_modules()\n","\n","        #í”½ì…€ ìœ„ì¹˜ ì„ë² ë”© ëª¨ë“ˆ ë§Œë“¤ê¸°\n","        self.pixel_pos_embedding = pixel_position_embedding(d_model=C_dim)\n","\n","\n","        self.Cdim_norm = nn.LayerNorm(C_dim)\n","\n","        #class + ë°°ê²½ ë°±í„°\n","        self.class_predic = nn.Linear(C_dim, self.n_class+1)\n","        #mask embed: maske_dim(1/4 feature mapì˜ C)ë¡œ ë°”ê¿ˆ\n","        self.mask_embed = mlp(n_layers=3, input_dim=self.C_dim,\n","                              hidden_dim=self.C_dim, output_dim=self.mask_dim)\n","\n","\n","\n","    def make_layers(self):\n","        for _ in range(self.L * 3):\n","            self.masked_attn_layers.append(masked_attention(\n","                C_dim = self.C_dim,\n","                n_head = self.n_head,\n","                dropout = self.dropout\n","                ))\n","            self.self_attn_layers.append(self_attention(\n","                C_dim = self.C_dim,\n","                n_head = self.n_head,\n","                dropout = self.dropout\n","                ))\n","            self.ffn_layers.append(FFN(\n","                C_dim = self.C_dim,\n","                dim_feedforward = self.dim_feedforward,\n","                dropout = self.dropout\n","                ))\n","\n","    def make_projection_modules(self):\n","        output = nn.ModuleList()\n","        for channel_dim in self.channel_dim_list:\n","            if channel_dim != self.C_dim:\n","                output.append(nn.Conv2d(channel_dim, self.C_dim, kernel_size=1))\n","                weight_init.c2_xavier_fill(output[-1])\n","            else:\n","                output.append(nn.Identity())\n","        return output\n","\n","    def predic_class_mask_masking(self, x, feat_map4: Tensor, masking_size):\n","        x_normed = self.Cdim_norm(x)\n","        class_predic = self.class_predic(x_normed)\n","        mask_embed = self.mask_embed(x_normed)\n","\n","        # 1/4 í•´ìƒë„ feature mapê³¼ mask embed ë‚´ì \n","        # (B, Q, C) @ (B, C, H/4, W/4) -> (B, Q, H/4, W/4)\n","        mask_predic = torch.einsum(\"bqc, bchw -> bqhw\", mask_embed, feat_map4)\n","\n","        #(B, Q, H_i, W_i) -> (B, Q, H_i*W_i)\n","        masking = F.interpolate(mask_predic, size=masking_size, mode='bilinear', align_corners= False).sigmoid().flatten(2)\n","        masking = masking.unsqueeze(0).repeat(self.n_head, 1, 1, 1).flatten(0,1)\n","        #0.5ì´ìƒì´ë©´ F ë¯¸ë§Œì´ë©´ T\n","        #Tì´ë©´ ë§ˆìŠ¤í‚¹ì„ í•´ë¼, Fë©´ attentionì„ í•˜ë¼\n","        masking = (masking < 0.5).bool()\n","        #maskingì´ mask_predicìœ¼ë¡œ ë¶€í„° íŒŒìƒë˜ëŠ” ê°’ì´ë¯€ë¡œ detach ì‹¤í–‰\n","        masking = masking.detach()\n","\n","        return class_predic, mask_predic, masking\n","\n","    #feature_pyramid: í”½ì…€ ë””ì½”ë”ì—ì„œ ë§Œë“  [1/32, 1/16, 1/8, 1/4] í•´ìƒë„ì˜ feature map ë¦¬ìŠ¤íŠ¸\n","    #ê° feature mapì€ (B, C, H, W) í˜•íƒœ\n","    def forward(self, feature_pyramid: list[Tensor]):\n","        #feature mapì˜ ì±„ë„ ì°¨ì› í™•ì¸\n","        assert [feature_pyramid[i].size(1) for i in range(len(feature_pyramid))] == self.channel_dim_list, \"feature mapì˜ ì±„ë„ ì°¨ì›ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n","\n","        #ì¿¼ë¦¬ì— ë§ê²Œ ì°¨ì› ë³€í™˜ëœ feature map + í•´ìƒë„ embed\n","        proj_feat_list = []\n","        #pixelì— ë”°ë¥¸ embed ë¦¬ìŠ¤íŠ¸(ìƒìˆ˜)\n","        pos_emb_list = []\n","        #í•´ìƒë„ í¬ê¸° ë¦¬ìŠ¤íŠ¸ (H, W)\n","        HW_list = []\n","        #ë°°ì¹˜ì‚¬ì´ì¦ˆ\n","        B = feature_pyramid[0].size(0)\n","\n","        for i in range(3):\n","            #(B, C, H, W) -> (B, C_dim, HxW) + (1, C_dim, 1) -> (B, C_dim, HxW)\n","            proj_feat_list.append(\n","                self.projection_modules[i](feature_pyramid[i]).flatten(2)\n","                + self.resolution_embed.weight[i][None, :, None])\n","            #(B, C_dim, HxW)->(B, HxW, C_dim)\n","            proj_feat_list[-1] = proj_feat_list[-1].permute(0, 2, 1)\n","\n","            #(B, C_dim, H, W) -> (B, C_dim, HxW)->(B, HxW, C_dim)\n","            pos_emb_list.append(self.pixel_pos_embedding(feature_pyramid[i]).flatten(2).permute(0, 2, 1))\n","            #HWì˜ intê°’ì„ ê°€ì§€ëŠ” ë¦¬ìŠ¤íŠ¸\n","            HW_list.append(feature_pyramid[i].shape[-2:])\n","\n","        # (Q, C_dim)-> (B, Q, C_dim)\n","        # layerì— ì…ë ¥í•  ì¿¼ë¦¬ì— layerì—ì„œ ë”í•  ì¿¼ë¦¬ ìœ„ì¹˜ embed ì„¤ì •\n","        x = self.query_feat_embed.weight.unsqueeze(0).repeat(B, 1, 1)\n","        query_pos_embed = self.query_pos_embed.weight.unsqueeze(0).repeat(B, 1, 1)\n","\n","        class_predic_list = []\n","        mask_predic_list = []\n","\n","        class_predic, mask_predic, masking = self.predic_class_mask_masking(x, feat_map4=feature_pyramid[-1], masking_size=HW_list[0])\n","\n","        class_predic_list.append(class_predic)\n","        mask_predic_list.append(mask_predic)\n","\n","        #ì´ 3L ë ˆì´ì–´ : (1/32->1/16->1/8)*3\n","        for i in range(self.L):\n","            for j in range(3):\n","                masking[torch.where(torch.all(masking, dim=-1))] = False\n","\n","                index = 3*i + j\n","                #mask attention layer\n","                x = self.masked_attn_layers[index](\n","                    x_last=x,\n","                    img_feat=proj_feat_list[j],\n","                    mask = masking,\n","                    query_pos = query_pos_embed,\n","                    pixel_pos = pos_emb_list[j]\n","                    )\n","                #self attention layer\n","                x = self.self_attn_layers[index](\n","                    x_last=x,\n","                    query_pos=query_pos_embed,\n","                    mask = None\n","                )\n","                #FFN layer\n","                x = self.ffn_layers[index](\n","                    x_last=x\n","                )\n","\n","                # ë‹¤ìŒ ë‹¨ê³„ ë§ˆìŠ¤í‚¹ ì˜ˆìƒ + ë§ˆìŠ¤í¬, í´ë˜ìŠ¤ ì˜ˆìƒ\n","                class_predic, mask_predic, masking = self.predic_class_mask_masking(x, feat_map4=feature_pyramid[-1], masking_size=HW_list[(j+1)%3])\n","\n","                #ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n","                class_predic_list.append(class_predic)\n","                mask_predic_list.append(mask_predic)\n","\n","        assert len(class_predic_list) == 3*self.L + 1, \"ì˜ˆì¸¡ í´ë˜ìŠ¤ ê°œìˆ˜ ì•ˆ ë§ìŒ\"\n","        assert len(mask_predic_list) == 3*self.L + 1, \"ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ê°œìˆ˜ ì•ˆ ë§ìŒ\"\n","\n","        final_out = {\n","                'predicted_class' : class_predic_list[-1],\n","                'predicted_mask' : mask_predic_list[-1],\n","                'auxiliary_out' : [\n","                                   {\"predicted_class\":c, \"predicted_mask\":m}\n","                                   for c, m in zip(class_predic_list[:-1], mask_predic_list[:-1])\n","                                  ]\n","\n","                    }\n","        return final_out"],"metadata":{"id":"cXElTn8L6qfu","executionInfo":{"status":"ok","timestamp":1762799217725,"user_tz":-540,"elapsed":52,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# --- 7. ğŸš€ ëª¨ë¸: 4ë¶€ (Mask2Former ì¡°ë¦½) ---\n","# (ëˆ„ë½ëœ ë¶€ë¶„ì´ë¯€ë¡œ, í‘œì¤€ ì•„í‚¤í…ì²˜ì— ë§ì¶° ìƒˆë¡œ ì‘ì„±)\n","\n","class Mask2Former(nn.Module):\n","    \"\"\"\n","    Backbone (Swin) + Pixel Decoder (TEncoderPixelDecoder) + Transformer Decoder (Transformer_Decoder)\n","    ë¥¼ ì¡°ë¦½í•˜ëŠ” ìµœì¢… Mask2Former ëª¨ë¸ì…ë‹ˆë‹¤.\n","    \"\"\"\n","\n","    # ì´ í´ë˜ìŠ¤ëŠ” from_configë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ,\n","    # í•„ìš”í•œ ì„¤ì •ê°’(cfg)ì„ ì§ì ‘ ë°›ì•„ í•˜ìœ„ ëª¨ë“ˆì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n","    def __init__(self, backbone_cfg, pixel_decoder_cfg, transformer_decoder_cfg, input_shape):\n","        super().__init__()\n","\n","        # --- 1. Backbone (Swin) ì´ˆê¸°í™” ---\n","        # input_shapeì€ (C, H, W)ë¥¼ ê°€ì • (e.g., ShapeSpec(channels=3))\n","        # D2SwinTransformer.from_configëŠ” cfgì™€ input_shapeì„ ë°›ìŒ\n","        self.backbone = D2SwinTransformer(\n","            **D2SwinTransformer.from_config(backbone_cfg, input_shape)\n","        )\n","\n","        # ë°±ë³¸ì˜ ì¶œë ¥ shapeì„ ê°€ì ¸ì˜´ (e.g., {\"res2\": ShapeSpec, ...})\n","        backbone_output_shape = self.backbone.output_shape()\n","\n","        # --- 2. Pixel Decoder (FPN + Encoder) ì´ˆê¸°í™” ---\n","        # TransformerEncoderPixelDecoder.from_configëŠ” cfgì™€ backbone_output_shapeì„ ë°›ìŒ\n","        pixel_decoder_params = TransformerEncoderPixelDecoder.from_config(\n","            pixel_decoder_cfg, backbone_output_shape\n","        )\n","        self.pixel_decoder = TransformerEncoderPixelDecoder(**pixel_decoder_params)\n","\n","        # --- 3. Transformer Decoder ì´ˆê¸°í™” ---\n","        # Transformer_DecoderëŠ” cfgê°€ ì•„ë‹Œ ê°œë³„ ì¸ìë¥¼ ë°›ìŠµë‹ˆë‹¤.\n","        # (ì£¼ì˜: channel_dim_listì™€ C_dim, mask_dimì´ Pixel Decoderì™€ ì¼ì¹˜í•´ì•¼ í•¨)\n","\n","        # Pixel Decoderì˜ FPN ì¶œë ¥ ì±„ë„ (conv_dim)\n","        # (res5 -> 1/32, res4 -> 1/16, res3 -> 1/8)\n","        # (ì£¼ì˜: TransformerEncoderPixelDecoderì˜ multi_scale_featuresëŠ” 3ê°œë§Œ ë°˜í™˜í•¨)\n","        conv_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","\n","        # Pixel Decoderì˜ ìµœì¢… ë§ˆìŠ¤í¬ íŠ¹ì§• ì±„ë„ (mask_dim)\n","        # (res2 -> 1/4)\n","        mask_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","\n","        # Transformer_Decoderì— ì „ë‹¬í•  ì±„ë„ ë¦¬ìŠ¤íŠ¸\n","        # [1/32, 1/16, 1/8, 1/4] ìŠ¤ì¼€ì¼ì— í•´ë‹¹\n","        # (ì£¼ì˜: Transformer_Decoderì˜ projection_modules[i]ëŠ” feature_pyramid[i]ì— ì ìš©ë¨)\n","        # (ì£¼ì˜: Transformer_Decoderì˜ forwardëŠ” 3ê°œì˜ feature map(0,1,2)ê³¼ 1ê°œì˜ mask map(3)ì„ ì‚¬ìš©í•¨)\n","        # (ì£¼ì˜: TransformerEncoderPixelDecoderì˜ multi_scale_featuresëŠ” 3ê°œ(conv_dim)ë¥¼,\n","        #  mask_featuresëŠ” 1ê°œ(mask_dim)ë¥¼ ë°˜í™˜í•¨. channel_dim_listì™€ ìˆœì„œ/ê°œìˆ˜ê°€ ë§ì§€ ì•ŠìŒ)\n","\n","        # --- [ì¶©ëŒ í•´ê²°] ---\n","        # ì œê³µëœ Transformer_Decoderì˜ forwardëŠ” 4ê°œì˜ feature_pyramid ì…ë ¥ì„ ë°›ì•„\n","        # [0], [1], [2] (3ê°œ)ëŠ” cross-attentionì—, [3] (1ê°œ)ëŠ” ë§ˆìŠ¤í¬ ì˜ˆì¸¡ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","        # TransformerEncoderPixelDecoderì˜ multi_scale_features (3ê°œ)ë¥¼ [0, 1, 2]ì—,\n","        # mask_features (1ê°œ)ë¥¼ [3]ì— ë§¤í•‘í•©ë‹ˆë‹¤.\n","\n","        # (Pixel Decoderì˜ multi_scale_featuresëŠ” [res5(1/32), res4(1/16), res3(1/8)] ìˆœì„œ)\n","        # (Pixel Decoderì˜ mask_featuresëŠ” [res2(1/4)] ìŠ¤ì¼€ì¼)\n","\n","        # channel_dim_list (Transformer_Decoderê°€ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì±„ë„)\n","        # [1/32, 1/16, 1/8, 1/4]\n","        # (res5, res4, res3ì˜ ì±„ë„ì€ pixel_decoderì˜ conv_dim)\n","        # (res2ì˜ ì±„ë„ì€ pixel_decoderì˜ mask_dim)\n","        decoder_channel_dim_list = [conv_dim, conv_dim, conv_dim, mask_dim]\n","\n","        # Transformer_Decoderì˜ C_dim (ë‚´ë¶€ Hidden Dim)\n","        # pixel_decoder_cfgì˜ conv_dimê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n","        decoder_C_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","\n","        # Transformer_Decoderì˜ mask_dim (MLP ì¶œë ¥ Dim)\n","        # pixel_decoder_cfgì˜ mask_dimê³¼ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.\n","        decoder_mask_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","\n","        self.transformer_decoder = Transformer_Decoder(\n","            C_dim=decoder_C_dim,\n","            n_head=transformer_decoder_cfg.MODEL.MASK_FORMER.NHEADS,\n","            dim_feedforward=transformer_decoder_cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD,\n","            channel_dim_list=decoder_channel_dim_list,\n","            mask_dim=decoder_mask_dim,\n","            L=transformer_decoder_cfg.MODEL.MASK_FORMER.DEC_LAYERS, # (cfgì— ì´ í•­ëª©ì´ í•„ìš”í•©ë‹ˆë‹¤)\n","            dropout=transformer_decoder_cfg.MODEL.MASK_FORMER.DROPOUT,\n","            n_query=transformer_decoder_cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES, # (cfgì— ì´ í•­ëª©ì´ í•„ìš”í•©ë‹ˆë‹¤)\n","            n_class=transformer_decoder_cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES # (cfgì— ì´ í•­ëª©ì´ í•„ìš”í•©ë‹ˆë‹¤)\n","        )\n","\n","    def forward(self, x):\n","        # 1. ë°±ë³¸ ì‹¤í–‰ (Swin)\n","        # (Input: [B, 3, H, W])\n","        # (Output: {\"res2\": [B,C2,H/4,W/4], \"res3\": [B,C3,H/8,W/8], ...})\n","        features = self.backbone(x)\n","\n","        # 2. í”½ì…€ ë””ì½”ë” ì‹¤í–‰ (TransformerEncoderPixelDecoder)\n","        # (Input: features dict)\n","        # (Output: mask_features [B, mask_dim, H/4, W/4],\n","        #          transformer_encoder_features [B, conv_dim, H/32, W/32],\n","        #          multi_scale_features [List of 3 Tensors: 1/32, 1/16, 1/8 scale])\n","        mask_features, _, multi_scale_features = self.pixel_decoder.forward_features(features)\n","\n","        # 3. íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ì‹¤í–‰ (Transformer_Decoder)\n","        # Transformer_DecoderëŠ” [1/32, 1/16, 1/8, 1/4] ìŠ¤ì¼€ì¼ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.\n","\n","        # (Pixel Decoderì˜ multi_scale_featuresëŠ” [res5(1/32), res4(1/16), res3(1/8)])\n","        # (Pixel Decoderì˜ mask_featuresëŠ” res2(1/4) ìŠ¤ì¼€ì¼)\n","\n","        # (ì£¼ì˜) TransformerEncoderPixelDecoderì˜ multi_scale_featuresëŠ”\n","        # [res5, res4, res3] (High-res to Low-resê°€ ì•„ë‹˜)\n","        # ì›ë³¸ BasePixelDecoderì˜ ìˆœì„œëŠ” low-to-high ([res5, res4, res3]) ì…ë‹ˆë‹¤.\n","        # Transformer_Decoderì˜ forwardëŠ” [1/32, 1/16, 1/8] ìˆœì„œë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.\n","\n","        # feature_pyramid ë¦¬ìŠ¤íŠ¸ ìƒì„±: [feat_1/32, feat_1/16, feat_1/8, feat_1/4]\n","        feature_pyramid = [\n","            multi_scale_features[0], # 1/32 (res5)\n","            multi_scale_features[1], # 1/16 (res4)\n","            multi_scale_features[2], # 1/8 (res3)\n","            mask_features            # 1/4 (res2, mask_dim ì±„ë„)\n","        ]\n","\n","        # (Input: [List of 4 Tensors])\n","        # (Output: {'predicted_class': ..., 'predicted_mask': ..., 'auxiliary_out': ...})\n","        outputs = self.transformer_decoder(feature_pyramid)\n","\n","        return outputs"],"metadata":{"id":"g4aJhOu99Ssk","executionInfo":{"status":"ok","timestamp":1762799217740,"user_tz":-540,"elapsed":13,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["# --- 8. ğŸš€ ì‹¤í–‰ ë° ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ---\n","#\n","\n","# SimpleNamespaceë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ import\n","import types\n","\n","# 1.1: ë°±ë³¸(Swin) ì„¤ì •\n","# (backboneì½”ë“œ.ipynbì˜ D2SwinTransformer.from_configê°€ ì°¸ì¡°í•  ê°’ë“¤)\n","backbone_cfg = _CfgNode() # fvcore CfgNode ì‚¬ìš©\n","backbone_cfg.MODEL = _CfgNode()\n","backbone_cfg.MODEL.SWIN = _CfgNode()\n","backbone_cfg.MODEL.SWIN.PRETRAIN_IMG_SIZE = 224\n","backbone_cfg.MODEL.SWIN.PATCH_SIZE = 4\n","backbone_cfg.MODEL.SWIN.EMBED_DIM = 96\n","backbone_cfg.MODEL.SWIN.DEPTHS = [2, 2, 6, 2]\n","backbone_cfg.MODEL.SWIN.NUM_HEADS = [3, 6, 12, 24]\n","backbone_cfg.MODEL.SWIN.WINDOW_SIZE = 7\n","backbone_cfg.MODEL.SWIN.MLP_RATIO = 4.0\n","backbone_cfg.MODEL.SWIN.QKV_BIAS = True\n","backbone_cfg.MODEL.SWIN.QK_SCALE = None\n","backbone_cfg.MODEL.SWIN.DROP_RATE = 0.0\n","backbone_cfg.MODEL.SWIN.ATTN_DROP_RATE = 0.0\n","backbone_cfg.MODEL.SWIN.DROP_PATH_RATE = 0.3\n","backbone_cfg.MODEL.SWIN.APE = False\n","backbone_cfg.MODEL.SWIN.PATCH_NORM = True\n","backbone_cfg.MODEL.SWIN.USE_CHECKPOINT = False\n","backbone_cfg.MODEL.SWIN.OUT_INDICES = (0, 1, 2, 3) # Transformer_Decoderê°€ 4ê°œ ìŠ¤ì¼€ì¼ì„ ê°€ì •\n","\n","# 1.2: í”½ì…€ ë””ì½”ë” (TEncoderPixelDecoder) ì„¤ì •\n","# (pixeldecoder_tem_fpn...ipynbì˜ TransformerEncoderPixelDecoder.from_configê°€ ì°¸ì¡°)\n","pixel_decoder_cfg = _CfgNode()\n","pixel_decoder_cfg.MODEL = _CfgNode()\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD = _CfgNode()\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM = 256\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.MASK_DIM = 256\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.NORM = \"GN\"\n","# (TransformerEncoderPixelDecoder ë‚´ë¶€ì˜ ì¸ì½”ë” ì„¤ì •)\n","pixel_decoder_cfg.MODEL.MASK_FORMER = _CfgNode()\n","pixel_decoder_cfg.MODEL.MASK_FORMER.DROPOUT = 0.1\n","pixel_decoder_cfg.MODEL.MASK_FORMER.NHEADS = 8\n","pixel_decoder_cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD = 1024\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS = 3\n","pixel_decoder_cfg.MODEL.MASK_FORMER.PRE_NORM = False\n","\n","# 1.3: íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” (Transformer_Decoder) ì„¤ì •\n","# (TransformDecoder.pyì˜ Transformer_Decoderê°€ ì°¸ì¡°)\n","transformer_decoder_cfg = _CfgNode()\n","transformer_decoder_cfg.MODEL = _CfgNode()\n","transformer_decoder_cfg.MODEL.MASK_FORMER = _CfgNode()\n","transformer_decoder_cfg.MODEL.MASK_FORMER.NHEADS = 8\n","transformer_decoder_cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD = 2048 # (TransformDecoder.py ê¸°ë³¸ê°’)\n","transformer_decoder_cfg.MODEL.MASK_FORMER.DEC_LAYERS = 3 # (TransformDecoder.py ê¸°ë³¸ê°’ L=3)\n","transformer_decoder_cfg.MODEL.MASK_FORMER.DROPOUT = 0.0 # (TransformDecoder.py ê¸°ë³¸ê°’)\n","transformer_decoder_cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES = 100 # (TransformDecoder.py ê¸°ë³¸ê°’)\n","transformer_decoder_cfg.MODEL.SEM_SEG_HEAD = _CfgNode()\n","transformer_decoder_cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 1 # (TransformDecoder.py ê¸°ë³¸ê°’: ê±´ë¬¼ 1ê°œ í´ë˜ìŠ¤)\n","\n","# 2. ëª¨ë¸ ì´ˆê¸°í™”\n","print(\"ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n","# (ì£¼ì˜: ì „ì²˜ë¦¬ì—ì„œ 1024x1024 íƒ€ì¼ì„ ì‚¬ìš©í–ˆì§€ë§Œ,\n","# ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê¸° ìœ„í•´ 256x256ìœ¼ë¡œ ì§„í–‰)\n","input_shape = ShapeSpec(channels=3, height=256, width=256)\n","\n","model = Mask2Former(\n","    backbone_cfg,\n","    pixel_decoder_cfg,\n","    transformer_decoder_cfg,\n","    input_shape\n",")\n","\n","# 3. ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ (Smoketest)\n","# 256x256 í¬ê¸°ì˜ ì„ì˜ ì´ë¯¸ì§€ 1ì¥ (ë°°ì¹˜ 1)\n","dummy_input = torch.randn(1, 3, 256, 256)\n","\n","print(\"\\nâœ… ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ!\")\n","print(\"--- ëª¨ë¸ì— [1, 3, 256, 256] í…ì„œ ì…ë ¥ ---\")\n","\n","# 4. ëª¨ë¸ ì‹¤í–‰ (Forward pass)\n","model.eval() # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •\n","with torch.no_grad(): # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨\n","    outputs = model(dummy_input)\n","\n","print(\"\\nâœ… Forward Pass ì„±ê³µ!\")\n","print(\"--- ìµœì¢… ì¶œë ¥(outputs) í˜•íƒœ ---\")\n","print(f\"predicted_class: {outputs['predicted_class'].shape}\")\n","print(f\"predicted_mask: {outputs['predicted_mask'].shape}\")\n","print(f\"auxiliary_out (ë³´ì¡° ì¶œë ¥ ê°œìˆ˜): {len(outputs['auxiliary_out'])}\")\n","if len(outputs['auxiliary_out']) > 0:\n","    print(f\"  -> (ì˜ˆ: ë³´ì¡°ì¶œë ¥ 0ë²ˆ) class: {outputs['auxiliary_out'][0]['predicted_class'].shape}\")\n","    print(f\"  -> (ì˜ˆ: ë³´ì¡°ì¶œë ¥ 0ë²ˆ) mask: {outputs['auxiliary_out'][0]['predicted_mask'].shape}\")\n","\n","print(\"\\nğŸ‰ ëª¨ë“  ë¼ˆëŒ€ ì½”ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì¡°ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZRXHoxvAbK4","executionInfo":{"status":"ok","timestamp":1762799219354,"user_tz":-540,"elapsed":1609,"user":{"displayName":"ê¹€ë¯¼ê· ","userId":"09841089427860745418"}},"outputId":"97b77611-d20b-442d-844f-1670683a6431"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n","\n","âœ… ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ!\n","--- ëª¨ë¸ì— [1, 3, 256, 256] í…ì„œ ì…ë ¥ ---\n","\n","âœ… Forward Pass ì„±ê³µ!\n","--- ìµœì¢… ì¶œë ¥(outputs) í˜•íƒœ ---\n","predicted_class: torch.Size([1, 100, 2])\n","predicted_mask: torch.Size([1, 100, 64, 64])\n","auxiliary_out (ë³´ì¡° ì¶œë ¥ ê°œìˆ˜): 9\n","  -> (ì˜ˆ: ë³´ì¡°ì¶œë ¥ 0ë²ˆ) class: torch.Size([1, 100, 2])\n","  -> (ì˜ˆ: ë³´ì¡°ì¶œë ¥ 0ë²ˆ) mask: torch.Size([1, 100, 64, 64])\n","\n","ğŸ‰ ëª¨ë“  ë¼ˆëŒ€ ì½”ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì¡°ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤!\n"]}]}]}