{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3cc6ea3ed30f45d7a838473a91995458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_627dcaff44d248be8d8c40a631de3c35",
              "IPY_MODEL_48364aebddde4b42922deb28af320950",
              "IPY_MODEL_c62ed108c4ce494283c4d04e9f1e1df6"
            ],
            "layout": "IPY_MODEL_ce62a3f3e20746b29e8a0e732d9b6912"
          }
        },
        "627dcaff44d248be8d8c40a631de3c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2c0e71831e470283d5b6bccbfdb726",
            "placeholder": "​",
            "style": "IPY_MODEL_15d677a6dd194b05ba101efdc1a92566",
            "value": "model.safetensors: 100%"
          }
        },
        "48364aebddde4b42922deb28af320950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a3f9150f4b43b9a188c6cc0f234f4e",
            "max": 352685652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b97fda84014d40cfb36c197f651efca5",
            "value": 352685652
          }
        },
        "c62ed108c4ce494283c4d04e9f1e1df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c408f5099bd4c97bccea0005170b5cc",
            "placeholder": "​",
            "style": "IPY_MODEL_98351677384a4ac5b14ac64bf9db9de8",
            "value": " 353M/353M [00:01&lt;00:00, 194MB/s]"
          }
        },
        "ce62a3f3e20746b29e8a0e732d9b6912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d2c0e71831e470283d5b6bccbfdb726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d677a6dd194b05ba101efdc1a92566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10a3f9150f4b43b9a188c6cc0f234f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97fda84014d40cfb36c197f651efca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c408f5099bd4c97bccea0005170b5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98351677384a4ac5b14ac64bf9db9de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import timm\n",
        "for model in timm.list_models():\n",
        "  print(model)"
      ],
      "metadata": {
        "id": "-T-cehBemnzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore\n",
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RfvopED6upqD",
        "outputId": "e5647c58-7b43-4f51-b4f6-aeabbc455453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=130b42bc0c2c0d268771609ec084258c327af6c6fde32ec8a0cfcb00d4a01b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=a61053fce864af48bfa8cf1a5108e46f05eb5250e0776b13f6749ce346c9cfec\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/lht1321-netizen/ml_project.git\n",
        "%cd ml_project\n",
        "!git checkout 3887cb3acd60c8e6731d7b44bc15344607adabe9\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "%cd detectron2\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/ml_project/Mask2Former')\n",
        "sys.path.append('/content/detectron2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nlXPKhSyt29-",
        "outputId": "24c14e09-f7d4-45f3-ef6e-221ad3bb23c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'ml_project'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 418 (delta 85), reused 6 (delta 6), pack-reused 199 (from 1)\u001b[K\n",
            "Receiving objects: 100% (418/418), 11.17 MiB | 9.10 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n",
            "/content/ml_project\n",
            "Note: switching to '3887cb3acd60c8e6731d7b44bc15344607adabe9'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 3887cb3 Add files via upload\n",
            "/content\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15943, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 15943 (delta 5), reused 5 (delta 4), pack-reused 15930 (from 2)\u001b[K\n",
            "Receiving objects: 100% (15943/15943), 6.71 MiB | 12.47 MiB/s, done.\n",
            "Resolving deltas: 100% (11334/11334), done.\n",
            "/content/detectron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch import nn,Tensor\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "import torch.distributed as tdist\n",
        "\n",
        "from collections import OrderedDict\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops.boxes import box_area\n",
        "import timm\n",
        "\n",
        "from detectron2.modeling import ShapeSpec\n",
        "from ml_project.Mask2Former.pixel_decoder.pixel_decoder_v2 import MSDeform_attention_pixel_decoder"
      ],
      "metadata": {
        "id": "H5fxcvCluj04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0b565e07-7bf1-4335-91c4-32de73a44a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/ml_project/Mask2Former/pixel_decoder/ops/modules/ms_deform_attn.py:94: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  :param input_flatten               (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l, C)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EFDTRModel"
      ],
      "metadata": {
        "id": "2OuVgVUy2GTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"D2SwinTransformer\"\"\"\n",
        "\n",
        "class D2SwinTransformer(nn.Module):\n",
        "  def __init__(self,model_name='swin_base_patch4_window7_224'):\n",
        "    super().__init__()\n",
        "    self.backbone=timm.create_model(model_name,\n",
        "                                    pretrained=True,\n",
        "                                    features_only=True,\n",
        "                                    out_indices=(0,1,2,3))\n",
        "    self._out_features = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
        "    self._out_feature_strides = {\"res2\": 4, \"res3\": 8, \"res4\": 16, \"res5\": 32}\n",
        "    self._out_feature_channels = {\n",
        "        \"res2\": self.backbone.feature_info[0][\"num_chs\"],\n",
        "        \"res3\": self.backbone.feature_info[1][\"num_chs\"],\n",
        "        \"res4\": self.backbone.feature_info[2][\"num_chs\"],\n",
        "        \"res5\": self.backbone.feature_info[3][\"num_chs\"]}\n",
        "\n",
        "  def forward(self,x):\n",
        "    features=self.backbone(x)\n",
        "    features=[f.permute(0,3,1,2).contiguous() for f in features]\n",
        "    return{\n",
        "        \"res2\":features[0],\n",
        "        \"res3\":features[1],\n",
        "        \"res4\":features[2],\n",
        "        \"res5\":features[3]\n",
        "    }\n",
        "\n",
        "  def output_shape(self):\n",
        "    return {\n",
        "        name: ShapeSpec(channels=self._out_feature_channels[name], stride=self._out_feature_strides[name])\n",
        "        for name in self._out_features\n",
        "    }\n",
        "\n",
        "# @title\n",
        "def get_activation(act: str, inpace: bool=True):\n",
        "    act = act.lower()\n",
        "    if act == 'silu':\n",
        "        m = nn.SiLU()\n",
        "    elif act == 'relu':\n",
        "        m = nn.ReLU()\n",
        "    elif act == 'leaky_relu':\n",
        "        m = nn.LeakyReLU()\n",
        "    elif act == 'silu':\n",
        "        m = nn.SiLU()\n",
        "    elif act == 'gelu':\n",
        "        m = nn.GELU()\n",
        "    elif act is None:\n",
        "        m = nn.Identity()\n",
        "    elif isinstance(act, nn.Module):\n",
        "        m = act\n",
        "    else:\n",
        "        raise RuntimeError('')\n",
        "    if hasattr(m, 'inplace'):\n",
        "        m.inplace = inpace\n",
        "    return m\n",
        "\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "    x0, y0, x1, y1 = x.unbind(-1)\n",
        "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "         (x1 - x0), (y1 - y0)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "def inverse_sigmoid(x: torch.Tensor, eps: float=1e-5) -> torch.Tensor:\n",
        "    x = x.clip(min=0., max=1.)\n",
        "    return torch.log(x.clip(min=eps) / (1 - x).clip(min=eps))\n",
        "\n",
        "def get_contrastive_denoising_training_group(targets,\n",
        "                                             num_classes,\n",
        "                                             num_queries,\n",
        "                                             class_embed,\n",
        "                                             num_denoising=100,\n",
        "                                             label_noise_ratio=0.5,\n",
        "                                             box_noise_scale=1.0,):\n",
        "    \"\"\"cnd\"\"\"\n",
        "    if num_denoising <= 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    num_gts = [len(t['labels']) for t in targets]\n",
        "    device = targets[0]['labels'].device\n",
        "\n",
        "    max_gt_num = max(num_gts)\n",
        "    if max_gt_num == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    num_group = num_denoising // max_gt_num\n",
        "    num_group = 1 if num_group == 0 else num_group\n",
        "    # pad gt to max_num of a batch\n",
        "    bs = len(num_gts)\n",
        "\n",
        "    input_query_class = torch.full([bs, max_gt_num], num_classes, dtype=torch.int32, device=device)\n",
        "    input_query_bbox = torch.zeros([bs, max_gt_num, 4], device=device)\n",
        "    pad_gt_mask = torch.zeros([bs, max_gt_num], dtype=torch.bool, device=device)\n",
        "\n",
        "    for i in range(bs):\n",
        "        num_gt = num_gts[i]\n",
        "        if num_gt > 0:\n",
        "            input_query_class[i, :num_gt] = targets[i]['labels']\n",
        "            input_query_bbox[i, :num_gt] = targets[i]['boxes']\n",
        "            pad_gt_mask[i, :num_gt] = 1\n",
        "    # each group has positive and negative queries.\n",
        "    input_query_class = input_query_class.tile([1, 2 * num_group])\n",
        "    input_query_bbox = input_query_bbox.tile([1, 2 * num_group, 1])\n",
        "    pad_gt_mask = pad_gt_mask.tile([1, 2 * num_group])\n",
        "    # positive and negative mask\n",
        "    negative_gt_mask = torch.zeros([bs, max_gt_num * 2, 1], device=device)\n",
        "    negative_gt_mask[:, max_gt_num:] = 1\n",
        "    negative_gt_mask = negative_gt_mask.tile([1, num_group, 1])\n",
        "    positive_gt_mask = 1 - negative_gt_mask\n",
        "    # contrastive denoising training positive index\n",
        "    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n",
        "    dn_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n",
        "    dn_positive_idx = torch.split(dn_positive_idx, [n * num_group for n in num_gts])\n",
        "    # total denoising queries\n",
        "    num_denoising = int(max_gt_num * 2 * num_group)\n",
        "\n",
        "    if label_noise_ratio > 0:\n",
        "        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n",
        "        # randomly put a new one here\n",
        "        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n",
        "        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n",
        "\n",
        "    # if label_noise_ratio > 0:\n",
        "    #     input_query_class = input_query_class.flatten()\n",
        "    #     pad_gt_mask = pad_gt_mask.flatten()\n",
        "    #     # half of bbox prob\n",
        "    #     # mask = torch.rand(input_query_class.shape, device=device) < (label_noise_ratio * 0.5)\n",
        "    #     mask = torch.rand_like(input_query_class) < (label_noise_ratio * 0.5)\n",
        "    #     chosen_idx = torch.nonzero(mask * pad_gt_mask).squeeze(-1)\n",
        "    #     # randomly put a new one here\n",
        "    #     new_label = torch.randint_like(chosen_idx, 0, num_classes, dtype=input_query_class.dtype)\n",
        "    #     # input_query_class.scatter_(dim=0, index=chosen_idx, value=new_label)\n",
        "    #     input_query_class[chosen_idx] = new_label\n",
        "    #     input_query_class = input_query_class.reshape(bs, num_denoising)\n",
        "    #     pad_gt_mask = pad_gt_mask.reshape(bs, num_denoising)\n",
        "\n",
        "    if box_noise_scale > 0:\n",
        "        known_bbox = box_cxcywh_to_xyxy(input_query_bbox)\n",
        "        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n",
        "        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n",
        "        rand_part = torch.rand_like(input_query_bbox)\n",
        "        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n",
        "        rand_part *= rand_sign\n",
        "        known_bbox += rand_part * diff\n",
        "        known_bbox.clip_(min=0.0, max=1.0)\n",
        "        input_query_bbox = box_xyxy_to_cxcywh(known_bbox)\n",
        "        input_query_bbox = inverse_sigmoid(input_query_bbox)\n",
        "\n",
        "    # class_embed = torch.concat([class_embed, torch.zeros([1, class_embed.shape[-1]], device=device)])\n",
        "    # input_query_class = torch.gather(\n",
        "    #     class_embed, input_query_class.flatten(),\n",
        "    #     axis=0).reshape(bs, num_denoising, -1)\n",
        "    # input_query_class = class_embed(input_query_class.flatten()).reshape(bs, num_denoising, -1)\n",
        "    input_query_class = class_embed(input_query_class)\n",
        "\n",
        "    tgt_size = num_denoising + num_queries\n",
        "    # attn_mask = torch.ones([tgt_size, tgt_size], device=device) < 0\n",
        "    attn_mask = torch.full([tgt_size, tgt_size], False, dtype=torch.bool, device=device)\n",
        "    # match query cannot see the reconstruction\n",
        "    attn_mask[num_denoising:, :num_denoising] = True\n",
        "\n",
        "    # reconstruct cannot see each other\n",
        "    for i in range(num_group):\n",
        "        if i == 0:\n",
        "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1): num_denoising] = True\n",
        "        if i == num_group - 1:\n",
        "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), :max_gt_num * i * 2] = True\n",
        "        else:\n",
        "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1): num_denoising] = True\n",
        "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), :max_gt_num * 2 * i] = True\n",
        "\n",
        "    dn_meta = {\n",
        "        \"dn_positive_idx\": dn_positive_idx,\n",
        "        \"dn_num_group\": num_group,\n",
        "        \"dn_num_split\": [num_denoising, num_queries]\n",
        "    }\n",
        "\n",
        "    # print(input_query_class.shape) # torch.Size([4, 196, 256])\n",
        "    # print(input_query_bbox.shape) # torch.Size([4, 196, 4])\n",
        "    # print(attn_mask.shape) # torch.Size([496, 496])\n",
        "\n",
        "    return input_query_class, input_query_bbox, attn_mask, dn_meta\n",
        "\n",
        "\"\"\"MLP\"\"\"\n",
        "\n",
        "# @title\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, act='relu'):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "        self.act = nn.Identity() if act is None else get_activation(act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = self.act(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\"\"\"_get_encoder_input\"\"\"\n",
        "\n",
        "# @title\n",
        "class _get_encoder_input(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, feats):\n",
        "      feat_flatten = []\n",
        "      spatial_shapes = []\n",
        "      level_start_index = [0, ]\n",
        "      for i, feat in enumerate(feats):\n",
        "          _, _, h, w = feat.shape\n",
        "          feat_flatten.append(feat.flatten(2).permute(0, 2, 1))\n",
        "          spatial_shapes.append([h, w])\n",
        "          level_start_index.append(h * w + level_start_index[-1])\n",
        "\n",
        "      feat_flatten = torch.cat(feat_flatten, 1)\n",
        "      level_start_index.pop()\n",
        "      return (feat_flatten, spatial_shapes, level_start_index)\n",
        "\n",
        "\"\"\"_generate_anchors\"\"\"\n",
        "\n",
        "# @title\n",
        "class _generate_anchors(nn.Module):\n",
        "  def __init__(self,\n",
        "                eps=1e-2,\n",
        "                feat_strides=[4, 8, 16, 32],\n",
        "                eval_spatial_size=[224, 224],\n",
        "                ):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.feat_strides = feat_strides\n",
        "    self.eval_spatial_size = eval_spatial_size\n",
        "  def forward(self,\n",
        "              spatial_shapes=None,\n",
        "              grid_size=0.05,\n",
        "              dtype=torch.float32,\n",
        "              device='cpu'):\n",
        "      if spatial_shapes is None:\n",
        "          spatial_shapes = [[int(self.eval_spatial_size[0] / s), int(self.eval_spatial_size[1] / s)]\n",
        "              for s in self.feat_strides\n",
        "          ]\n",
        "      anchors = []\n",
        "      for lvl, (h, w) in enumerate(spatial_shapes):\n",
        "          grid_y, grid_x = torch.meshgrid(\\\n",
        "              torch.arange(end=h, dtype=dtype), \\\n",
        "              torch.arange(end=w, dtype=dtype), indexing='ij')\n",
        "          grid_xy = torch.stack([grid_x, grid_y], -1)\n",
        "          valid_WH = torch.tensor([w, h]).to(dtype)\n",
        "          grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_WH\n",
        "          wh = torch.ones_like(grid_xy) * grid_size * (2.0 ** lvl)\n",
        "          anchors.append(torch.concat([grid_xy, wh], -1).reshape(-1, h * w, 4))\n",
        "\n",
        "      anchors = torch.concat(anchors, 1).to(device)\n",
        "      valid_mask = ((anchors > self.eps) * (anchors < 1 - self.eps)).all(-1, keepdim=True)\n",
        "      anchors = torch.log(anchors / (1 - anchors))\n",
        "      # anchors = torch.where(valid_mask, anchors, float('inf'))\n",
        "      # anchors[valid_mask] = torch.inf # valid_mask [1, 8400, 1]\n",
        "      anchors = torch.where(valid_mask, anchors, torch.inf)\n",
        "\n",
        "      return anchors, valid_mask\n",
        "\n",
        "\"\"\"_get_decoder_input\"\"\"\n",
        "\n",
        "# @title\n",
        "class _get_decoder_input(nn.Module):\n",
        "  def __init__(self,\n",
        "               hidden_dim=256,\n",
        "               num_queries=300,\n",
        "               num_classes=80,\n",
        "               learnt_init_query=False,\n",
        "               ):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.num_queries = num_queries\n",
        "\n",
        "    self.enc_output = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.LayerNorm(hidden_dim,)\n",
        "    )\n",
        "\n",
        "    self.enc_score_head = nn.Linear(hidden_dim, num_classes)\n",
        "    self.enc_bbox_head = MLP(hidden_dim, hidden_dim, 4, num_layers=3)\n",
        "\n",
        "    self.learnt_init_query = learnt_init_query\n",
        "    if learnt_init_query:\n",
        "      self.tgt_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "\n",
        "  def forward(self,\n",
        "              memory,\n",
        "              anchors,\n",
        "              valid_mask,\n",
        "              denoising_class=None,\n",
        "              denoising_bbox_unact=None):\n",
        "      bs, _, _ = memory.shape\n",
        "\n",
        "      # memory = torch.where(valid_mask, memory, 0)\n",
        "      memory = valid_mask.to(memory.dtype) * memory  # TODO fix type error for onnx export\n",
        "\n",
        "      output_memory = self.enc_output(memory)\n",
        "\n",
        "      enc_outputs_class = self.enc_score_head(output_memory)\n",
        "      enc_outputs_coord_unact = self.enc_bbox_head(output_memory) + anchors\n",
        "\n",
        "      _, topk_ind = torch.topk(enc_outputs_class.max(-1).values, self.num_queries, dim=1)\n",
        "\n",
        "      reference_points_unact = enc_outputs_coord_unact.gather(dim=1, \\\n",
        "          index=topk_ind.unsqueeze(-1).repeat(1, 1, enc_outputs_coord_unact.shape[-1]))\n",
        "\n",
        "      enc_topk_bboxes = F.sigmoid(reference_points_unact)\n",
        "      if denoising_bbox_unact is not None:\n",
        "          reference_points_unact = torch.concat(\n",
        "              [denoising_bbox_unact, reference_points_unact], 1)\n",
        "\n",
        "      enc_topk_logits = enc_outputs_class.gather(dim=1, \\\n",
        "          index=topk_ind.unsqueeze(-1).repeat(1, 1, enc_outputs_class.shape[-1]))\n",
        "\n",
        "      # extract region features\n",
        "      if self.learnt_init_query:\n",
        "          target = self.tgt_embed.weight.unsqueeze(0).tile([bs, 1, 1])\n",
        "      else:\n",
        "          target = output_memory.gather(dim=1, \\\n",
        "              index=topk_ind.unsqueeze(-1).repeat(1, 1, output_memory.shape[-1]))\n",
        "          target = target.detach()\n",
        "\n",
        "      if denoising_class is not None:\n",
        "          target = torch.concat([denoising_class, target], 1)\n",
        "\n",
        "      return target, reference_points_unact.detach(), enc_topk_bboxes, enc_topk_logits\n",
        "\n",
        "\"\"\"EFDEncoder\"\"\"\n",
        "\n",
        "# @title\n",
        "class EFDEncoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim: int = 256,      # query feature 차원 (C)\n",
        "        hidden_dim: int = 256,  # EFD 토큰의 임베딩 차원 (D)\n",
        "        num_harmonics: int = 1  # n (order) → L = 4n + 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_harmonics = num_harmonics\n",
        "\n",
        "        # EFD 계수 개수 (논문 figure: Length = 4n + 1)\n",
        "        self.num_coeffs = 4 * num_harmonics + 1  # cx, cy, A1,B1,D1,...,Dn\n",
        "\n",
        "        # 1) query feature → EFD 계수 벡터 [B,Q,L]\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, self.num_coeffs)\n",
        "        )\n",
        "\n",
        "        # 2) scalar coefficient → vector token 변환 (공유 projection)\n",
        "        self.scalar_to_vec = nn.Linear(1, hidden_dim)\n",
        "\n",
        "        # 3) 각 coefficient slot(0~L-1)에 대한 위치 임베딩\n",
        "        self.slot_pos_embed = nn.Embedding(self.num_coeffs, hidden_dim)\n",
        "\n",
        "    def forward(self, query_feats: torch.Tensor):\n",
        "        \"\"\"\n",
        "        query_feats: [B, Q, C]\n",
        "\n",
        "        return:\n",
        "            efd_coeffs: [B, Q, L]\n",
        "            efd_tokens: [B, Q, L, hidden_dim]\n",
        "        \"\"\"\n",
        "        B, Q, C = query_feats.shape\n",
        "        device = query_feats.device\n",
        "\n",
        "        # (1) EFD 계수 예측: [B,Q,C] → [B,Q,L]\n",
        "        efd_coeffs = self.mlp(query_feats)  # [B, Q, L]\n",
        "\n",
        "        # (2) scalar → vector 변환\n",
        "        #     각 scalar coeff를 hidden_dim짜리 벡터로 만들고,\n",
        "        #     slot별 positional embedding을 더해 EFD Decoder용 토큰 생성\n",
        "        L = self.num_coeffs\n",
        "\n",
        "        coeffs_flat = efd_coeffs.view(B * Q * L, 1)           # [B*Q*L, 1]\n",
        "        vecs_flat = self.scalar_to_vec(coeffs_flat)           # [B*Q*L, D]\n",
        "        vecs = vecs_flat.view(B, Q, L, self.hidden_dim)       # [B,Q,L,D]\n",
        "\n",
        "        slot_ids = torch.arange(L, device=device)             # [L]\n",
        "        slot_pos = self.slot_pos_embed(slot_ids)              # [L,D]\n",
        "        slot_pos = slot_pos.view(1, 1, L, self.hidden_dim)    # [1,1,L,D]\n",
        "\n",
        "        efd_tokens = vecs + slot_pos                          # [B,Q,L,D]\n",
        "\n",
        "        return efd_coeffs, efd_tokens\n",
        "\n",
        "\"\"\"self attention + add & norm\"\"\"\n",
        "\n",
        "# @title\n",
        "\n",
        "class self_attention(nn.Module):\n",
        "    #c_dim: feature dimension = query dimension\n",
        "    #n_head: 멀티해더 수\n",
        "    #논문에서 dropout이 0일 때가 퍼포먼스가 가장 좋았다고 해서 default를 0으로 설정\n",
        "    def __init__(self, C_dim, n_head, dropout=0.0):\n",
        "        super(self_attention, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(C_dim, n_head, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(C_dim)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    #parameter 초기화, xavier_uniform_ 사용\n",
        "    def reset_parameters(self):\n",
        "        for par in self.parameters():\n",
        "            if par.dim() > 1:\n",
        "                nn.init.xavier_uniform_(par)\n",
        "\n",
        "    # query와 key에 위치 임베딩 더하기 위해 구현\n",
        "    def add_pos_embedding(self, x: Tensor, pos: Tensor) -> Tensor:\n",
        "        return x if pos is None else x + pos\n",
        "\n",
        "    # x_last: 그전 단계의 출력값\n",
        "    # mask: 마스크 텐서 / 키 마스크는 없는 것으로 설정(input으로 넣기 전에 전처리 따로 할 예정)\n",
        "    # query_pos: 쿼리 포지션 임베딩 (쿼리가 어떤 위치를 나타내는지 알려주는 임베딩)\n",
        "    # self attention이기 때문에 key = value\n",
        "    def forward(self, x_last, mask = None, query_pos =None):\n",
        "        # self attention\n",
        "        Query_and_Key = self.add_pos_embedding(x_last, query_pos)\n",
        "\n",
        "        self_attened = self.self_attn(\n",
        "            query = Query_and_Key,\n",
        "            key = Query_and_Key,\n",
        "            value = x_last,\n",
        "            attn_mask = mask,\n",
        "            key_padding_mask = None\n",
        "            )[0]\n",
        "\n",
        "        # add & norm\n",
        "        # 논문에 나오는 add&norm 부분을 self attention 모듈 안에 구현\n",
        "        # add: x + f(x) / norm : 정규화(LayerNorm)\n",
        "        x_next = x_last + self.dropout(self_attened)\n",
        "        x_next = self.norm(x_next)\n",
        "\n",
        "        return x_next\n",
        "\n",
        "\"\"\"deformable_attention_core_func\"\"\"\n",
        "\n",
        "# @title\n",
        "def deformable_attention_core_func(value, value_spatial_shapes, sampling_locations, attention_weights):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        value (Tensor): [bs, value_length, n_head, c]\n",
        "        value_spatial_shapes (Tensor|List): [n_levels, 2]\n",
        "        value_level_start_index (Tensor|List): [n_levels]\n",
        "        sampling_locations (Tensor): [bs, query_length, n_head, n_levels, n_points, 2]\n",
        "        attention_weights (Tensor): [bs, query_length, n_head, n_levels, n_points]\n",
        "\n",
        "    Returns:\n",
        "        output (Tensor): [bs, Length_{query}, C]\n",
        "    \"\"\"\n",
        "    bs, _, n_head, c = value.shape\n",
        "    _, Len_q, _, n_levels, n_points, _ = sampling_locations.shape\n",
        "\n",
        "    split_shape = [h * w for h, w in value_spatial_shapes]\n",
        "    value_list = value.split(split_shape, dim=1)\n",
        "    sampling_grids = 2 * sampling_locations - 1\n",
        "    sampling_value_list = []\n",
        "    for level, (h, w) in enumerate(value_spatial_shapes):\n",
        "        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_\n",
        "        value_l_ = value_list[level].flatten(2).permute(\n",
        "            0, 2, 1).reshape(bs * n_head, c, h, w)\n",
        "        # N_, Lq_, M_, P_, 2 -> N_, M_, Lq_, P_, 2 -> N_*M_, Lq_, P_, 2\n",
        "        sampling_grid_l_ = sampling_grids[:, :, :, level].permute(\n",
        "            0, 2, 1, 3, 4).flatten(0, 1)\n",
        "        # N_*M_, D_, Lq_, P_\n",
        "        sampling_value_l_ = F.grid_sample(\n",
        "            value_l_,\n",
        "            sampling_grid_l_,\n",
        "            mode='bilinear',\n",
        "            padding_mode='zeros',\n",
        "            align_corners=False)\n",
        "        sampling_value_list.append(sampling_value_l_)\n",
        "    # (N_, Lq_, M_, L_, P_) -> (N_, M_, Lq_, L_, P_) -> (N_*M_, 1, Lq_, L_*P_)\n",
        "    attention_weights = attention_weights.permute(0, 2, 1, 3, 4).reshape(\n",
        "        bs * n_head, 1, Len_q, n_levels * n_points)\n",
        "    output = (torch.stack(\n",
        "        sampling_value_list, dim=-2).flatten(-2) *\n",
        "              attention_weights).sum(-1).reshape(bs, n_head * c, Len_q)\n",
        "\n",
        "    return output.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "class MSDeformableAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=256, num_heads=8, num_levels=4, num_points=4,):\n",
        "        \"\"\"\n",
        "        Multi-Scale Deformable Attention Module\n",
        "        \"\"\"\n",
        "        super(MSDeformableAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_levels = num_levels\n",
        "        self.num_points = num_points\n",
        "        self.total_points = num_heads * num_levels * num_points\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.sampling_offsets = nn.Linear(embed_dim, self.total_points * 2,)\n",
        "        self.attention_weights = nn.Linear(embed_dim, self.total_points)\n",
        "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.ms_deformable_attn_core = deformable_attention_core_func\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # sampling_offsets\n",
        "        init.constant_(self.sampling_offsets.weight, 0)\n",
        "        thetas = torch.arange(self.num_heads, dtype=torch.float32) * (2.0 * math.pi / self.num_heads)\n",
        "        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n",
        "        grid_init = grid_init / grid_init.abs().max(-1, keepdim=True).values\n",
        "        grid_init = grid_init.reshape(self.num_heads, 1, 1, 2).tile([1, self.num_levels, self.num_points, 1])\n",
        "        scaling = torch.arange(1, self.num_points + 1, dtype=torch.float32).reshape(1, 1, -1, 1)\n",
        "        grid_init *= scaling\n",
        "        self.sampling_offsets.bias.data[...] = grid_init.flatten()\n",
        "\n",
        "        # attention_weights\n",
        "        init.constant_(self.attention_weights.weight, 0)\n",
        "        init.constant_(self.attention_weights.bias, 0)\n",
        "\n",
        "        # proj\n",
        "        init.xavier_uniform_(self.value_proj.weight)\n",
        "        init.constant_(self.value_proj.bias, 0)\n",
        "        init.xavier_uniform_(self.output_proj.weight)\n",
        "        init.constant_(self.output_proj.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                query,\n",
        "                reference_points,\n",
        "                value,\n",
        "                value_spatial_shapes,\n",
        "                value_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query (Tensor): [bs, query_length, C]\n",
        "            reference_points (Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),\n",
        "                bottom-right (1, 1), including padding area\n",
        "            value (Tensor): [bs, value_length, C]\n",
        "            value_spatial_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n",
        "            value_level_start_index (List): [n_levels], [0, H_0*W_0, H_0*W_0+H_1*W_1, ...]\n",
        "            value_mask (Tensor): [bs, value_length], True for non-padding elements, False for padding elements\n",
        "\n",
        "        Returns:\n",
        "            output (Tensor): [bs, Length_{query}, C]\n",
        "        \"\"\"\n",
        "        bs, Len_q = query.shape[:2]\n",
        "        Len_v = value.shape[1]\n",
        "\n",
        "        value = self.value_proj(value)\n",
        "        if value_mask is not None:\n",
        "            value_mask = value_mask.astype(value.dtype).unsqueeze(-1)\n",
        "            value *= value_mask\n",
        "        value = value.reshape(bs, Len_v, self.num_heads, self.head_dim)\n",
        "\n",
        "        sampling_offsets = self.sampling_offsets(query).reshape(\n",
        "            bs, Len_q, self.num_heads, self.num_levels, self.num_points, 2)\n",
        "        attention_weights = self.attention_weights(query).reshape(\n",
        "            bs, Len_q, self.num_heads, self.num_levels * self.num_points)\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1).reshape(\n",
        "            bs, Len_q, self.num_heads, self.num_levels, self.num_points)\n",
        "\n",
        "        if reference_points.shape[-1] == 2:\n",
        "            offset_normalizer = torch.tensor(value_spatial_shapes)\n",
        "            offset_normalizer = offset_normalizer.flip([1]).reshape(\n",
        "                1, 1, 1, self.num_levels, 1, 2)\n",
        "            sampling_locations = reference_points.reshape(\n",
        "                bs, Len_q, 1, self.num_levels, 1, 2\n",
        "            ) + sampling_offsets / offset_normalizer\n",
        "        elif reference_points.shape[-1] == 4:\n",
        "            sampling_locations = (\n",
        "                reference_points[:, :, None, :, None, :2] + sampling_offsets /\n",
        "                self.num_points * reference_points[:, :, None, :, None, 2:] * 0.5)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Last dim of reference_points must be 2 or 4, but get {} instead.\".\n",
        "                format(reference_points.shape[-1]))\n",
        "\n",
        "        output = self.ms_deformable_attn_core(value, value_spatial_shapes, sampling_locations, attention_weights)\n",
        "\n",
        "        output = self.output_proj(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\"\"\"deformable_attention+ add & norm\"\"\"\n",
        "\n",
        "# @title\n",
        "class deformable_attention(nn.Module):\n",
        "\n",
        "    def __init__(self, C_dim, n_head, n_levels, n_points=4, dropout=0.0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.deform_attn = MSDeformableAttention(embed_dim=C_dim,\n",
        "                                                  num_heads=n_head,\n",
        "                                                  num_levels=n_levels,\n",
        "                                                  num_points=n_points\n",
        "                                                  )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(C_dim)\n",
        "\n",
        "    def add_pos_embedding(self, x: Tensor, pos: Tensor) -> Tensor:\n",
        "        return x if pos is None else x + pos\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x_last,                 # (B, Len_q, C)\n",
        "        reference_points,       # (B, Len_q, n_levels, 2 or 4)\n",
        "        img_feat,               # (B, Len_v, C)\n",
        "        spatial_shapes,         # (n_levels, 2)\n",
        "        mask=None,\n",
        "        query_pos=None,\n",
        "    ):\n",
        "        query = self.add_pos_embedding(x_last, query_pos)\n",
        "        deformable_attened = self.deform_attn(query=query,reference_points=reference_points,value=img_feat,value_spatial_shapes=spatial_shapes,value_mask=mask, )\n",
        "        x_next = x_last + self.dropout(deformable_attened)\n",
        "        x_next = self.norm(x_next)\n",
        "\n",
        "        return x_next\n",
        "\n",
        "\"\"\"FFN+ add & norm\"\"\"\n",
        "\n",
        "# @title\n",
        "class FFN(nn.Module):\n",
        "    #C_dim: feature dimension\n",
        "    #dim_feedforward: FFN 내부의 hidden layer 차원\n",
        "    #논문에서 dropout이 0일 때가 퍼포먼스가 가장 좋았다고 해서 default를 0으로 설정\n",
        "    def __init__(self, C_dim, dim_feedforward=2048, dropout=0.0):\n",
        "        super(FFN, self).__init__()\n",
        "\n",
        "        self.first_linear = nn.Linear(C_dim, dim_feedforward)\n",
        "        self.second_linear = nn.Linear(dim_feedforward, C_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = F.relu\n",
        "\n",
        "        self.norm = nn.LayerNorm(C_dim)\n",
        "        #parameter 초기화, xavier_uniform_ 사용\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for par in self.parameters():\n",
        "            if par.dim() > 1:\n",
        "                nn.init.xavier_uniform_(par)\n",
        "\n",
        "    def forward(self, x_last):\n",
        "        # FFN\n",
        "        # FFN = relu(w1x + b1)W2 + b2\n",
        "        ffn = self.second_linear(self.dropout(self.activation(self.first_linear(x_last))))\n",
        "\n",
        "        # add & norm\n",
        "        # 논문에 나오는 add&norm 부분을 FFN 모듈 안에 구현\n",
        "        # add: x + f(x) / norm : 정규화(LayerNorm)\n",
        "        x_next = x_last + self.dropout(ffn)\n",
        "        x_next = self.norm(x_next)\n",
        "\n",
        "        return x_next\n",
        "\n",
        "\"\"\"EFDDecoder\"\"\"\n",
        "\n",
        "# @title\n",
        "class EFDDecoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        C_dim=256,\n",
        "        n_head=8,\n",
        "        dim_feedforward=1024,\n",
        "        n_levels=3,\n",
        "        n_points=4,\n",
        "        dropout=0.0,\n",
        "        num_classes=2,\n",
        "        num_harmonics=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = self_attention(\n",
        "            C_dim=C_dim,\n",
        "            n_head=n_head,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.deform_attn = deformable_attention(\n",
        "            C_dim=C_dim,\n",
        "            n_head=n_head,\n",
        "            n_levels=n_levels,\n",
        "            n_points=n_points,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.ffn = FFN(\n",
        "            C_dim=C_dim,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.class_head = nn.Linear(C_dim, 1)  # binary (building / not)\n",
        "        self.bbox_head = MLP(C_dim, C_dim, 4, num_layers=3)\n",
        "        self.efd_head  = MLP(C_dim, C_dim, 4 * num_harmonics + 1, num_layers=2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt,                 # (B, Q_total, C)\n",
        "        ref_points_unact,    # (B, Q_total, 4)\n",
        "        memory,              # (B, S, C)\n",
        "        spatial_shapes,      # (L, 2)\n",
        "        query_pos_head,      # MLP(4→C)\n",
        "        attn_mask=None,      # (Q_total, Q_total) or None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Q_total = DN 쿼리 + 메인 쿼리 개수\n",
        "        \"\"\"\n",
        "\n",
        "        # 1) reference points → pos embedding\n",
        "        ref_points = ref_points_unact.sigmoid()            # (B,Q,4)\n",
        "        query_pos = query_pos_head(ref_points)            # (B,Q,C)\n",
        "\n",
        "        # 2) self-attention (DN mask 반영 가능)\n",
        "        x = self.self_attn(\n",
        "            tgt,\n",
        "            mask=attn_mask,        # ★ 여기서 attn_mask 사용 가능\n",
        "            query_pos=query_pos,\n",
        "        )\n",
        "\n",
        "        # 3) deformable attention\n",
        "        if torch.is_tensor(spatial_shapes):\n",
        "            num_levels = spatial_shapes.shape[0]\n",
        "        else:\n",
        "            num_levels = len(spatial_shapes)\n",
        "\n",
        "        ref_input = ref_points.unsqueeze(2).expand(-1, -1, num_levels, -1)\n",
        "\n",
        "        x = self.deform_attn(\n",
        "            x,               # x_last\n",
        "            ref_input,       # reference_points\n",
        "            memory,          # img_feat\n",
        "            spatial_shapes,  # spatial_shapes\n",
        "            mask=None,\n",
        "            query_pos=query_pos,\n",
        "        )\n",
        "\n",
        "        # 4) FFN\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 5) heads\n",
        "        class_logits = self.class_head(x)         # (B,Q,1)\n",
        "\n",
        "        delta_bbox = self.bbox_head(x)           # (B,Q,4)\n",
        "        new_ref_unact = delta_bbox + ref_points_unact\n",
        "        new_ref = new_ref_unact.sigmoid()        # (B,Q,4) in [0,1]\n",
        "\n",
        "        efd_params = self.efd_head(x)            # (B,Q,4n+1)\n",
        "\n",
        "        return x, new_ref_unact, new_ref, class_logits, efd_params\n",
        "\n",
        "\"\"\"EFDinversetransform\"\"\"\n",
        "\n",
        "# @title\n",
        "class EFDinversetransform(nn.Module):\n",
        "    \"\"\"\n",
        "    EFD 기반 곡선 복원 모듈.\n",
        "\n",
        "    입력:\n",
        "        efd_params: (B, Q, 4n+1)\n",
        "            - 첫 번째 채널: scale (스칼라)\n",
        "            - 나머지: [a1..an, b1..bn, c1..cn, d1..dn] 순서라고 가정\n",
        "        boxes: (B, Q, 4) = (cx, cy, w, h)  [선택]\n",
        "        local:\n",
        "            True  → EFD 로컬 좌표 ([-1, 1] 근처)로 리턴\n",
        "        map_to_box:\n",
        "            True  → boxes 정보로 (cx,cy,w,h)에 맞게 매핑한 폴리곤 리턴\n",
        "\n",
        "    출력:\n",
        "        polys: (B, Q, T, 2)  where T = num_points\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_harmonics: int = None, num_points: int = 128):\n",
        "        super().__init__()\n",
        "        self.num_harmonics = num_harmonics  # None이면 forward에서 자동 추론\n",
        "        self.num_points = num_points\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        efd_params: torch.Tensor,   # (B, Q, 4n+1)\n",
        "        boxes: torch.Tensor = None, # (B, Q, 4)\n",
        "        local: bool = False,\n",
        "        map_to_box: bool = True,\n",
        "    ) -> torch.Tensor:\n",
        "        B, Q, D = efd_params.shape\n",
        "\n",
        "        # ===== 0) harmonic 수 자동 추론 =====\n",
        "        if self.num_harmonics is None:\n",
        "            assert (D - 1) % 4 == 0, f\"EFD params 마지막 차원 D={D} 는 4n+1 형태여야 합니다.\"\n",
        "            n = (D - 1) // 4\n",
        "        else:\n",
        "            n = self.num_harmonics\n",
        "            expected_D = 4 * n + 1\n",
        "            assert D == expected_D, f\"efd_params 마지막 차원 D={D} 는 4n+1 (={expected_D}) 이어야 합니다.\"\n",
        "\n",
        "        device = efd_params.device\n",
        "        dtype = efd_params.dtype\n",
        "\n",
        "        # ===== 1) scale + 계수 분리 =====\n",
        "        scale = efd_params[..., 0:1]        # (B, Q, 1)\n",
        "        coeffs = efd_params[..., 1:]        # (B, Q, 4n)\n",
        "        coeffs = coeffs.view(B, Q, n, 4)    # (B, Q, n, 4)\n",
        "\n",
        "        # a_n, b_n, c_n, d_n  (각각 (B, Q, n))\n",
        "        a, b, c, d = coeffs.unbind(dim=-1)\n",
        "\n",
        "        # ===== 2) t 샘플링 (0 ~ 2π) =====\n",
        "        #   t: (1, 1, 1, T)\n",
        "        T = self.num_points\n",
        "        t = torch.linspace(0, 2 * math.pi, T, device=device, dtype=dtype)\n",
        "        t = t.view(1, 1, 1, T)\n",
        "\n",
        "        #   n_idx: (1, 1, n, 1)  → n * t 브로드캐스트\n",
        "        n_idx = torch.arange(1, n + 1, device=device, dtype=dtype).view(1, 1, n, 1)\n",
        "\n",
        "        cos_nt = torch.cos(n_idx * t)   # (B=1, Q=1, n, T)\n",
        "        sin_nt = torch.sin(n_idx * t)\n",
        "\n",
        "        # (B, Q, n, 1) 로 확장\n",
        "        a = a.unsqueeze(-1)\n",
        "        b = b.unsqueeze(-1)\n",
        "        c = c.unsqueeze(-1)\n",
        "        d = d.unsqueeze(-1)\n",
        "\n",
        "        # ===== 3) Fourier 역변환 (로컬 좌표계 곡선) =====\n",
        "        # x(t) = Σ [ a_n cos(n t) + b_n sin(n t) ]\n",
        "        # y(t) = Σ [ c_n cos(n t) + d_n sin(n t) ]\n",
        "        x = (a * cos_nt + b * sin_nt).sum(dim=2)  # (B, Q, T)\n",
        "        y = (c * cos_nt + d * sin_nt).sum(dim=2)  # (B, Q, T)\n",
        "\n",
        "        # ===== 4) scale 적용 (global shape 조절) =====\n",
        "        #   scale: (B, Q, 1) → 브로드캐스트\n",
        "        x = x * scale\n",
        "        y = y * scale\n",
        "\n",
        "        # 현재 polys_local은 EFD 로컬 좌표 (centered, scale 적용됨)\n",
        "        polys_local = torch.stack([x, y], dim=-1)  # (B, Q, T, 2)\n",
        "\n",
        "        # ===== 5) 로컬 좌표로만 뽑고 싶을 때 =====\n",
        "        if local and not map_to_box:\n",
        "            # [-1, 1] 근처에 맞게 정규화 (gradient 유지)\n",
        "            #   (x, y) 전체에서 최대 절댓값 기준으로 나눔\n",
        "            max_val = polys_local.abs().amax(dim=2, keepdim=True).clamp(min=1e-4)\n",
        "            polys_norm = polys_local / max_val\n",
        "            return polys_norm  # (B, Q, T, 2)\n",
        "\n",
        "        # ===== 6) boxes 기반으로 (cx,cy,w,h) 공간에 매핑 =====\n",
        "        if map_to_box:\n",
        "            assert boxes is not None, \"map_to_box=True 인 경우 boxes (B,Q,4)가 필요합니다.\"\n",
        "            assert boxes.shape[:2] == (B, Q), f\"boxes shape={boxes.shape}, efd_params shape={efd_params.shape}\"\n",
        "\n",
        "            cx, cy, w, h = boxes.unbind(dim=-1)  # 각 (B, Q)\n",
        "\n",
        "            # (B, Q, 1) 로 reshape 후 브로드캐스트\n",
        "            cx = cx.unsqueeze(-1)\n",
        "            cy = cy.unsqueeze(-1)\n",
        "            w = w.unsqueeze(-1)\n",
        "            h = h.unsqueeze(-1)\n",
        "\n",
        "            # polys_local: (B, Q, T, 2)\n",
        "            x_local = polys_local[..., 0]   # (B, Q, T)\n",
        "            y_local = polys_local[..., 1]\n",
        "\n",
        "            # [-1,1] 근처라고 보고 w/2, h/2 스케일로 박스에 매핑\n",
        "            px = cx + x_local * (w / 2.0)\n",
        "            py = cy + y_local * (h / 2.0)\n",
        "\n",
        "            polys_box = torch.stack([px, py], dim=-1)  # (B, Q, T, 2)\n",
        "            return polys_box\n",
        "\n",
        "        # 기본값: scale까지 적용된 로컬 좌표 (박스 매핑 X)\n",
        "        return polys_local\n",
        "\n",
        "\"\"\"PolygonGroupSampler\"\"\"\n",
        "\n",
        "# @title\n",
        "\n",
        "class PolygonGroupSampler(nn.Module):\n",
        "    \"\"\"\n",
        "    논문 EFDTR 기반 Polygon Decoder 전처리 모듈.\n",
        "    EFD inverse transform으로 만든 128개 버텍스를\n",
        "    group_size(기본 4)로 묶어 32개 그룹을 만들고\n",
        "    각 그룹의 대표 xy(center)을 계산한다.\n",
        "\n",
        "    입력:\n",
        "        polys_box: (B, Q, T, 2)  — 128 vertices\n",
        "    출력:\n",
        "        polys_grouped: (B, Q, num_groups, group_size, 2)\n",
        "        group_centers: (B, Q, num_groups, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, group_size=4):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def forward(self, polys_box):\n",
        "        \"\"\"\n",
        "        polys_box: (B, Q, T, 2)\n",
        "        \"\"\"\n",
        "        B, Q, T, _ = polys_box.shape\n",
        "        g = self.group_size\n",
        "\n",
        "        assert T % g == 0, f\"Vertex count T={T} must be divisible by group_size={g}\"\n",
        "        num_groups = T // g  # 128/4 = 32\n",
        "\n",
        "        # (B, Q, 128, 2) → (B, Q, 32, 4, 2)\n",
        "        polys_grouped = polys_box.view(B, Q, num_groups, g, 2)\n",
        "\n",
        "        # group centroid: (B, Q, 32, 2)\n",
        "        group_centers = polys_grouped.mean(dim=3)\n",
        "\n",
        "        return polys_grouped, group_centers\n",
        "\n",
        "\"\"\"MultiScaleFeatureSampler\"\"\"\n",
        "\n",
        "# @title\n",
        "class MultiScaleFeatureSampler(nn.Module):\n",
        "    \"\"\"\n",
        "    EFDTR 스타일 멀티스케일 피처 샘플러.\n",
        "    - 32개 그룹 중심점(group_centers)을 이용해 P2~P5에서 grid_sample\n",
        "    - EFD 계수 + class logits로 4-scale fusion weight 예측\n",
        "    - weighted sum으로 단일 스케일 쿼리 피처 생성\n",
        "\n",
        "    Args:\n",
        "        in_channels:   각 피처맵 채널 수 (모든 scale에서 동일하다고 가정, C)\n",
        "        num_levels:    멀티 스케일 피처 개수 (기본 4: P2~P5)\n",
        "        efd_dim:       EFD 계수 차원 (4n+1)\n",
        "        cls_dim:       class 로짓 차원 (num_classes 또는 1)\n",
        "        hidden_dim:    weight 예측용 MLP hidden dim\n",
        "        out_dim:       최종 출력 채널 수 (None이면 in_channels 유지)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_levels: int = 4,\n",
        "        efd_dim: int = 5,\n",
        "        cls_dim: int = 2,\n",
        "        hidden_dim: int = 128,\n",
        "        out_dim: int = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_levels = num_levels\n",
        "        self.efd_dim = efd_dim\n",
        "        self.cls_dim = cls_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.out_dim = out_dim or in_channels\n",
        "\n",
        "        cond_dim = efd_dim + cls_dim  # [EFD, class] concat\n",
        "\n",
        "        # EFD + class → 4-scale weight 예측 MLP\n",
        "        self.scale_mlp = nn.Sequential(\n",
        "            nn.Linear(cond_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, num_levels),\n",
        "        )\n",
        "\n",
        "        # 채널 축소/변환 (optional)\n",
        "        self.output_proj = nn.Linear(in_channels, self.out_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        multi_scale_feats,\n",
        "        group_centers: torch.Tensor,  # (B, Q, G, 2) in [0,1]\n",
        "        efd_params: torch.Tensor,     # (B, Q, D_efd)\n",
        "        class_logits: torch.Tensor,   # (B, Q, D_cls)\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            multi_scale_feats: list of length L=num_levels\n",
        "                each tensor: (B, C, H_l, W_l)\n",
        "            group_centers: (B, Q, G, 2) - normalized [0,1] coords\n",
        "            efd_params:    (B, Q, D_efd)\n",
        "            class_logits:  (B, Q, D_cls)\n",
        "\n",
        "        Returns:\n",
        "            fused_feats: (B, Q, G, C_out)\n",
        "        \"\"\"\n",
        "        assert len(multi_scale_feats) == self.num_levels, \\\n",
        "            f\"Expected {self.num_levels} feature levels, got {len(multi_scale_feats)}\"\n",
        "\n",
        "        B, Q, G, _ = group_centers.shape\n",
        "        C = self.in_channels\n",
        "        device = group_centers.device\n",
        "        dtype = group_centers.dtype\n",
        "\n",
        "        # ----- 1) group_centers [0,1] → grid_sample 좌표 [-1,1] -----\n",
        "        # grid_sample expects (N, H_out, W_out, 2) with coords in [-1, 1]\n",
        "        # 여기서 H_out = G, W_out = 1 로 사용\n",
        "        x = group_centers[..., 0]  # (B, Q, G)\n",
        "        y = group_centers[..., 1]  # (B, Q, G)\n",
        "\n",
        "        grid_x = x * 2.0 - 1.0\n",
        "        grid_y = y * 2.0 - 1.0\n",
        "        grid = torch.stack([grid_x, grid_y], dim=-1)  # (B, Q, G, 2)\n",
        "\n",
        "        # (B, Q, G, 2) → (B*Q, G, 1, 2)\n",
        "        grid_flat = grid.view(B * Q, G, 1, 2)\n",
        "\n",
        "        # ----- 2) 각 scale에서 group_centers 기준으로 grid_sample -----\n",
        "        level_feats = []  # list of (B, Q, G, C)\n",
        "\n",
        "        for lvl, feat in enumerate(multi_scale_feats):\n",
        "            # feat: (B, C, H_l, W_l) → (B*Q, C, H_l, W_l)\n",
        "            assert feat.shape[0] == B, \"Batch size mismatch between feats and group_centers\"\n",
        "            assert feat.shape[1] == C, \"in_channels mismatch with feat channels\"\n",
        "\n",
        "            Bf, Cf, Hf, Wf = feat.shape\n",
        "\n",
        "            # Q 차원 확장: (B, 1, C, H, W) → (B, Q, C, H, W) → (B*Q, C, H, W)\n",
        "            feat_expanded = feat.unsqueeze(1).expand(Bf, Q, Cf, Hf, Wf)\n",
        "            feat_flat = feat_expanded.reshape(Bf * Q, Cf, Hf, Wf)\n",
        "\n",
        "            # grid_sample: (B*Q, C, G, 1)\n",
        "            sampled = F.grid_sample(\n",
        "                feat_flat,      # (B*Q, C, H, W)\n",
        "                grid_flat,      # (B*Q, G, 1, 2)\n",
        "                mode=\"bilinear\",\n",
        "                padding_mode=\"zeros\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "\n",
        "            # (B*Q, C, G, 1) → (B, Q, C, G) → (B, Q, G, C)\n",
        "            sampled = sampled.squeeze(-1)               # (B*Q, C, G)\n",
        "            sampled = sampled.view(B, Q, Cf, G)         # (B, Q, C, G)\n",
        "            sampled = sampled.permute(0, 1, 3, 2)       # (B, Q, G, C)\n",
        "\n",
        "            level_feats.append(sampled)\n",
        "\n",
        "        # ----- 3) EFD + class로 4-scale fusion weight 예측 -----\n",
        "        # cond: (B, Q, cond_dim) → broadcast to (B, Q, G, cond_dim)\n",
        "        cond = torch.cat([efd_params, class_logits], dim=-1)  # (B, Q, D_efd + D_cls)\n",
        "        cond = cond.unsqueeze(2).expand(B, Q, G, cond.shape[-1])  # (B, Q, G, cond_dim)\n",
        "\n",
        "        # (B, Q, G, L)\n",
        "        scale_logits = self.scale_mlp(cond)\n",
        "        scale_weights = F.softmax(scale_logits, dim=-1)  # normalize over levels\n",
        "\n",
        "        # ----- 4) level_feats stack & weighted sum -----\n",
        "        # level_feats: list of L tensors each (B, Q, G, C)\n",
        "        # → stacked: (B, Q, G, L, C)\n",
        "        stacked = torch.stack(level_feats, dim=3)\n",
        "\n",
        "        # scale_weights: (B, Q, G, L) → (B, Q, G, L, 1)\n",
        "        w = scale_weights.unsqueeze(-1)\n",
        "\n",
        "        # weighted sum over levels L\n",
        "        fused = (stacked * w).sum(dim=3)  # (B, Q, G, C)\n",
        "\n",
        "        # ----- 5) optional channel projection -----\n",
        "        fused = self.output_proj(fused)   # (B, Q, G, C_out)\n",
        "\n",
        "        return fused  # Polygon Decoder 쿼리 피처\n",
        "\n",
        "\"\"\"PolygonDecoder\"\"\"\n",
        "\n",
        "# @title\n",
        "class PolygonDecoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        C_dim=256,\n",
        "        n_head=8,\n",
        "        dim_feedforward=1024,\n",
        "        n_levels=4,\n",
        "        n_points=4,\n",
        "        dropout=0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = self_attention(\n",
        "            C_dim=C_dim,\n",
        "            n_head=n_head,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.deform_attn = deformable_attention(\n",
        "            C_dim=C_dim,\n",
        "            n_head=n_head,\n",
        "            n_levels=n_levels,\n",
        "            n_points=n_points,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.ffn = FFN(\n",
        "            C_dim=C_dim,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.vertex_head = nn.Linear(C_dim, n_points * 2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt,               # (B, Q, G, C)\n",
        "        reference_points,  # (B, Q, G, 2)  group center (normalized)\n",
        "        polys_grouped,     # (B, Q, G, n_points, 2)\n",
        "        memory,            # (B, S, C)\n",
        "        spatial_shapes,    # (L,2)\n",
        "        query_pos=None,    # unused or optional\n",
        "        attn_mask=None,    # (Q_total, Q_total) or None\n",
        "    ):\n",
        "        B, Q, G, C = tgt.shape\n",
        "\n",
        "        # 1) flatten group dimension\n",
        "        tgt_flat = tgt.view(B, Q * G, C)\n",
        "        ref = reference_points.view(B, Q * G, 1, 2)\n",
        "\n",
        "        if query_pos is not None:\n",
        "            query_pos_flat = query_pos.view(B, Q * G, C)\n",
        "        else:\n",
        "            query_pos_flat = None\n",
        "\n",
        "        if torch.is_tensor(spatial_shapes):\n",
        "            num_levels = spatial_shapes.shape[0]\n",
        "        else:\n",
        "            num_levels = len(spatial_shapes)\n",
        "\n",
        "        # 2) self-attn\n",
        "        x = self.self_attn(\n",
        "            tgt_flat,\n",
        "            mask=None,\n",
        "            query_pos=query_pos_flat,\n",
        "        )\n",
        "\n",
        "        # 3) deformable attn\n",
        "        x = self.deform_attn(\n",
        "            x,\n",
        "            ref.expand(-1, -1, num_levels, -1),\n",
        "            memory,\n",
        "            spatial_shapes,\n",
        "            mask=None,\n",
        "            query_pos=query_pos_flat,\n",
        "        )\n",
        "\n",
        "        x = x.view(B, Q, G, C)\n",
        "\n",
        "        # 4) FFN\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 5) vertex offsets\n",
        "        v = self.vertex_head(x)          # (B,Q,G,2*n_points)\n",
        "        v = v.view(B, Q, G, -1, 2)       # (B,Q,G,n_points,2)\n",
        "\n",
        "        refined_vertices = polys_grouped + v\n",
        "\n",
        "        return x, refined_vertices\n",
        "\n",
        "\"\"\"EFDTRTransformerDecoder\"\"\"\n",
        "\n",
        "# @title\n",
        "class EFDTRTransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    전체 EFDTR-style Transformer Decoder (DN 완전 적용 버전)\n",
        "\n",
        "    구성:\n",
        "      1) RT-DETR-style top-k query selection (_get_decoder_input)\n",
        "      2) EFDDecoder stage  (bbox + EFD + class 예측)\n",
        "      3) EFD inverse transform → 128 vertices\n",
        "      4) PolygonGroupSampler → 32 groups + group centers\n",
        "      5) MultiScaleFeatureSampler → 멀티스케일 피처 + 4-scale weight fusion\n",
        "      6) PolygonDecoder (multi-layer) → 128 vertices iterative refine\n",
        "      7) Denoising (Contrastive Denoising, CDN) 완전 통합\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_queries=300,\n",
        "        num_classes=2,\n",
        "        C_dim=256,\n",
        "        n_head=8,\n",
        "        dim_feedforward=1024,\n",
        "        num_efd_layers=1,\n",
        "        num_poly_layers=3,\n",
        "        num_levels_enc=3,\n",
        "        num_levels_ms=4,\n",
        "        num_harmonics=1,\n",
        "        num_points_poly=128,\n",
        "        group_size=4,\n",
        "        dropout=0.0,\n",
        "        # DN 관련\n",
        "        num_denoising=100,\n",
        "        label_noise_ratio=0.5,\n",
        "        box_noise_scale=1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_queries = num_queries\n",
        "        self.num_classes = num_classes\n",
        "        self.C_dim = C_dim\n",
        "        self.num_harmonics = num_harmonics\n",
        "        self.num_points_poly = num_points_poly\n",
        "        self.group_size = group_size\n",
        "\n",
        "        self.num_denoising = num_denoising\n",
        "        self.label_noise_ratio = label_noise_ratio\n",
        "        self.box_noise_scale = box_noise_scale\n",
        "\n",
        "        # 0) Denoising 쿼리 임베딩 (RT-DETR와 동일 방식)\n",
        "        if self.num_denoising > 0:\n",
        "            # num_classes+1: 마지막 인덱스를 padding용으로 사용\n",
        "            self.denoising_class_embed = nn.Embedding(\n",
        "                num_classes + 1, C_dim, padding_idx=num_classes\n",
        "            )\n",
        "        else:\n",
        "            self.denoising_class_embed = None\n",
        "\n",
        "        # 1) RT-DETR top-k decoder input (anchors, encoder feat → query)\n",
        "        self.dec_input = _get_decoder_input(\n",
        "            hidden_dim=C_dim,\n",
        "            num_queries=num_queries,\n",
        "            num_classes=num_classes,\n",
        "            learnt_init_query=False,\n",
        "        )\n",
        "\n",
        "        # 2) EFD Decoder layers\n",
        "        self.efd_layers = nn.ModuleList([\n",
        "            EFDDecoder(\n",
        "                C_dim=C_dim,\n",
        "                n_head=n_head,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                n_levels=num_levels_enc,\n",
        "                n_points=4,\n",
        "                dropout=dropout,\n",
        "                num_classes=num_classes,\n",
        "                num_harmonics=num_harmonics,\n",
        "            )\n",
        "            for _ in range(num_efd_layers)\n",
        "        ])\n",
        "\n",
        "        # bbox → query_pos 변환용 MLP\n",
        "        self.query_pos_head = MLP(\n",
        "            input_dim=4,\n",
        "            hidden_dim=C_dim * 2,\n",
        "            output_dim=C_dim,\n",
        "            num_layers=2,\n",
        "        )\n",
        "\n",
        "        # 3) EFD inverse transform\n",
        "        self.efd_inverse = EFDinversetransform(\n",
        "            num_harmonics=num_harmonics,\n",
        "            num_points=num_points_poly,\n",
        "        )\n",
        "\n",
        "        # 4) group sampler (128 → 32×4)\n",
        "        self.group_sampler = PolygonGroupSampler(group_size=group_size)\n",
        "\n",
        "        # 5) multi-scale sampler (P0~P3)\n",
        "        efd_dim = 4 * num_harmonics + 1\n",
        "        cls_dim = 1  # EFDDecoder class_head output dim\n",
        "\n",
        "        self.ms_sampler = MultiScaleFeatureSampler(\n",
        "            in_channels=C_dim,\n",
        "            num_levels=num_levels_ms,\n",
        "            efd_dim=efd_dim,\n",
        "            cls_dim=cls_dim,\n",
        "            hidden_dim=128,\n",
        "            out_dim=C_dim,\n",
        "        )\n",
        "\n",
        "        # 6) Polygon Decoder layers (iterative refine)\n",
        "        self.polygon_layers = nn.ModuleList([\n",
        "            PolygonDecoder(\n",
        "                C_dim=C_dim,\n",
        "                n_head=n_head,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                n_levels=num_levels_enc,\n",
        "                n_points=group_size,   # group_size=4 → vertex_head out dim=8\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            for _ in range(num_poly_layers)\n",
        "        ])\n",
        "\n",
        "    def _prepare_denoising(self, targets):\n",
        "        \"\"\"\n",
        "        RT-DETR의 get_contrastive_denoising_training_group를 호출.\n",
        "        \"\"\"\n",
        "        if (not self.training) or (self.num_denoising <= 0) or (targets is None):\n",
        "            return None, None, None, None\n",
        "\n",
        "        dn_class, dn_bbox_unact, attn_mask, dn_meta = \\\n",
        "            get_contrastive_denoising_training_group(\n",
        "                targets=targets,\n",
        "                num_classes=self.num_classes,\n",
        "                num_queries=self.num_queries,\n",
        "                class_embed=self.denoising_class_embed,\n",
        "                num_denoising=self.num_denoising,\n",
        "                label_noise_ratio=self.label_noise_ratio,\n",
        "                box_noise_scale=self.box_noise_scale,\n",
        "            )\n",
        "        return dn_class, dn_bbox_unact, attn_mask, dn_meta\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        memory,            # (B, S, C)  from _get_encoder_input(pixel_outs_3)\n",
        "        spatial_shapes,    # python list or tensor [(H1,W1), (H2,W2), ...]\n",
        "        anchors,           # (B, S, 4)\n",
        "        valid_mask,        # (B, S, 1)\n",
        "        multi_scale_feats, # list of feature maps for ms sampling (e.g., pixel_outs)\n",
        "        targets=None,      # 학습 시 GT (DN 용)\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Returns (train mode):\n",
        "            out_main:\n",
        "              {\n",
        "                'class_logits': (B, Q, 1),\n",
        "                'boxes':        (B, Q, 4),\n",
        "                'efd_params':   (B, Q, 4n+1),\n",
        "                'polygons':     (B, Q, 128, 2),\n",
        "              }\n",
        "            out_dn:\n",
        "              {\n",
        "                'class_logits': (B, K, 1),\n",
        "                'boxes':        (B, K, 4),\n",
        "                'efd_params':   (B, K, 4n+1),\n",
        "                'polygons':     (B, K, 128, 2),\n",
        "              }\n",
        "            dn_meta: DN 관련 메타 (dn_num_split 등)\n",
        "\n",
        "        Returns (eval mode):\n",
        "            out_main만 반환\n",
        "        \"\"\"\n",
        "        device = memory.device\n",
        "        if not torch.is_tensor(spatial_shapes):\n",
        "            spatial_shapes_tensor = torch.as_tensor(\n",
        "                spatial_shapes, device=device, dtype=torch.long\n",
        "            )\n",
        "        else:\n",
        "            spatial_shapes_tensor = spatial_shapes.to(device)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 0) DN 쿼리 생성 (training+num_denoising>0+targets 있을 때만)\n",
        "        # --------------------------------------------------\n",
        "        dn_class, dn_bbox_unact, attn_mask, dn_meta = \\\n",
        "            self._prepare_denoising(targets)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 1) RT-DETR top-k query selection (+ DN 붙이기)\n",
        "        # --------------------------------------------------\n",
        "        target, ref_points_unact, topk_bboxes, topk_logits = self.dec_input(\n",
        "            memory=memory,\n",
        "            anchors=anchors,\n",
        "            valid_mask=valid_mask,\n",
        "            denoising_class=dn_class,\n",
        "            denoising_bbox_unact=dn_bbox_unact,\n",
        "        )\n",
        "        # target:         (B, K+Q, C)\n",
        "        # ref_points_unact: (B, K+Q, 4)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 2) EFD Decoder stage (iterative)\n",
        "        # --------------------------------------------------\n",
        "        x = target\n",
        "        ref_unact = ref_points_unact\n",
        "        class_logits = None\n",
        "        efd_params = None\n",
        "        ref_sigmoid = None\n",
        "\n",
        "        for layer in self.efd_layers:\n",
        "            x, ref_unact, ref_sigmoid, class_logits, efd_params = layer(\n",
        "                tgt=x,\n",
        "                ref_points_unact=ref_unact,\n",
        "                memory=memory,\n",
        "                spatial_shapes=spatial_shapes_tensor,\n",
        "                query_pos_head=self.query_pos_head,\n",
        "                attn_mask=attn_mask,             # ★ CDN 마스크 전달 가능\n",
        "            )\n",
        "            ref_unact = ref_unact\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 3) EFD inverse → 128 vertices\n",
        "        # --------------------------------------------------\n",
        "        polys_box = self.efd_inverse(\n",
        "            efd_params,       # (B, K+Q, 4n+1)\n",
        "            boxes=ref_sigmoid,\n",
        "            local=False,\n",
        "            map_to_box=True,\n",
        "        )  # (B, K+Q, 128, 2)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 4) Group sampling\n",
        "        # --------------------------------------------------\n",
        "        polys_grouped, group_centers = self.group_sampler(polys_box)\n",
        "        # polys_grouped: (B, K+Q, G, 4, 2)\n",
        "        # group_centers: (B, K+Q, G, 2)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 5) Multi-scale feature sampling + fusion\n",
        "        # --------------------------------------------------\n",
        "        fused_queries = self.ms_sampler(\n",
        "            multi_scale_feats=multi_scale_feats,\n",
        "            group_centers=group_centers,\n",
        "            efd_params=efd_params,\n",
        "            class_logits=class_logits,\n",
        "        )  # (B, K+Q, G, C)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 6) PolygonDecoder (multi-layer iterative refine)\n",
        "        # --------------------------------------------------\n",
        "        tgt_poly = fused_queries          # (B,K+Q,G,C)\n",
        "        ref_pts = group_centers           # (B,K+Q,G,2)\n",
        "        polys_grp = polys_grouped         # (B,K+Q,G,4,2)\n",
        "\n",
        "        for layer in self.polygon_layers:\n",
        "            tgt_poly, polys_grp = layer(\n",
        "                tgt=tgt_poly,\n",
        "                reference_points=ref_pts,\n",
        "                polys_grouped=polys_grp,\n",
        "                memory=memory,\n",
        "                spatial_shapes=spatial_shapes_tensor,\n",
        "                query_pos=None,\n",
        "                attn_mask=None,     # ★ 여기도 None\n",
        "            )\n",
        "            ref_pts = polys_grp.mean(dim=3)  # (B,K+Q,G,2)\n",
        "\n",
        "        final_polys = polys_grp.reshape(\n",
        "            polys_grp.shape[0], polys_grp.shape[1], -1, 2\n",
        "        )  # (B, K+Q, 128, 2)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 7) DN / Main Query split\n",
        "        # --------------------------------------------------\n",
        "        if (self.training) and (dn_meta is not None):\n",
        "            dn_num, main_num = dn_meta[\"dn_num_split\"]  # [K, Q]\n",
        "            # K+Q 확인용 assert\n",
        "            assert class_logits.shape[1] == dn_num + main_num\n",
        "\n",
        "            # 앞 K = DN, 뒤 Q = main\n",
        "            cls_dn   = class_logits[:, :dn_num]\n",
        "            box_dn   = ref_sigmoid[:, :dn_num]\n",
        "            efd_dn   = efd_params[:, :dn_num]\n",
        "            poly_dn  = final_polys[:, :dn_num]\n",
        "\n",
        "            cls_main  = class_logits[:, dn_num:]\n",
        "            box_main  = ref_sigmoid[:, dn_num:]\n",
        "            efd_main  = efd_params[:, dn_num:]\n",
        "            poly_main = final_polys[:, dn_num:]\n",
        "\n",
        "            out_main = {\n",
        "                \"class_logits\": cls_main,\n",
        "                \"boxes\":        box_main,\n",
        "                \"efd_params\":   efd_main,\n",
        "                \"polygons\":     poly_main,\n",
        "            }\n",
        "            out_dn = {\n",
        "                \"class_logits\": cls_dn,\n",
        "                \"boxes\":        box_dn,\n",
        "                \"efd_params\":   efd_dn,\n",
        "                \"polygons\":     poly_dn,\n",
        "            }\n",
        "            return out_main, out_dn, dn_meta\n",
        "\n",
        "        else:\n",
        "            # eval 모드 or DN 사용 안할 때는 main만 (K+Q 전체를 main 취급)\n",
        "            out = {\n",
        "                \"class_logits\": class_logits,\n",
        "                \"boxes\":        ref_sigmoid,\n",
        "                \"efd_params\":   efd_params,\n",
        "                \"polygons\":     final_polys,\n",
        "            }\n",
        "            return out\n",
        "# EFDTRModel\n",
        "\n",
        "class EFDTRModel(nn.Module):\n",
        "    \"\"\"\n",
        "    D2Swin 백본 + MSDeform Pixel Decoder + EFDTR Transformer Decoder\n",
        "    전부 한 번에 묶은 최종 모델 래퍼.\n",
        "\n",
        "    forward(images, targets=None) 으로 바로 호출 가능.\n",
        "\n",
        "    Args:\n",
        "        backbone_name: timm Swin 모델 이름\n",
        "        num_classes:   클래스 수 (건물=1 + 배경)\n",
        "        num_queries:   detection query 개수 (RT-DETR top-k)\n",
        "        num_denoising: DN query 개수 (0이면 DN 비활성)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone_name: str = \"swin_base_patch4_window7_224\",\n",
        "        num_classes: int = 2,\n",
        "        num_queries: int = 100,\n",
        "        num_denoising: int = 20,\n",
        "        C_dim: int = 256,\n",
        "        num_efd_layers: int = 2,\n",
        "        num_poly_layers: int = 2,\n",
        "        num_levels_enc: int = 3,   # encoder에 쓰는 pixel_out level 수 (P0~P2)\n",
        "        num_levels_ms: int = 4,    # multi-scale sampler에 쓰는 level 수 (P0~P3)\n",
        "        num_harmonics: int = 1,\n",
        "        num_points_poly: int = 128,\n",
        "        group_size: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_queries = num_queries\n",
        "        self.num_denoising = num_denoising\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 1) Backbone\n",
        "        # --------------------------------------------------\n",
        "        self.backbone = D2SwinTransformer(model_name=backbone_name)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 2) Pixel Decoder (Mask2Former MSDeformAttention PixelDecoder)\n",
        "        # --------------------------------------------------\n",
        "        # feature_channels는 backbone 출력 채널과 맞춰야 함\n",
        "        # 현재 D2SwinTransformer: res2=128, res3=256, res4=512, res5=1024\n",
        "        self.pixel_decoder = MSDeform_attention_pixel_decoder(\n",
        "            feature_channels=[128, 256, 512, 1024]\n",
        "        )\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 3) Encoder Input / Anchors 생성 모듈\n",
        "        # --------------------------------------------------\n",
        "        self.encoder_input = _get_encoder_input()\n",
        "        self.anchor_gen = _generate_anchors(\n",
        "            eps=1e-2,\n",
        "            feat_strides=[4, 8, 16, 32],   # spatial_shapes를 넘겨줄 거라 큰 의미는 없음\n",
        "            eval_spatial_size=[224, 224],  # spatial_shapes=None일 때만 사용\n",
        "        )\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 4) EFDTR Transformer Decoder (DN 내장)\n",
        "        # --------------------------------------------------\n",
        "        self.decoder = EFDTRTransformerDecoder(\n",
        "            num_queries=num_queries,\n",
        "            num_classes=num_classes,\n",
        "            C_dim=C_dim,\n",
        "            n_head=8,\n",
        "            dim_feedforward=1024,\n",
        "            num_efd_layers=num_efd_layers,\n",
        "            num_poly_layers=num_poly_layers,\n",
        "            num_levels_enc=num_levels_enc,\n",
        "            num_levels_ms=num_levels_ms,\n",
        "            num_harmonics=num_harmonics,\n",
        "            num_points_poly=num_points_poly,\n",
        "            group_size=group_size,\n",
        "            dropout=0.0,\n",
        "            num_denoising=num_denoising,\n",
        "            label_noise_ratio=0.5,\n",
        "            box_noise_scale=0.4,\n",
        "        )\n",
        "\n",
        "    def forward(self, images: torch.Tensor, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images:  (B,3,H,W) 입력 이미지\n",
        "            targets: 학습 시 GT 리스트\n",
        "                [\n",
        "                  {\n",
        "                    \"boxes\":  (Ni,4)  cx,cy,w,h in [0,1]\n",
        "                    \"labels\": (Ni,)   class index\n",
        "                    ...\n",
        "                  },\n",
        "                  ...\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            train 모드 & DN ON:\n",
        "                out_main, out_dn, dn_meta\n",
        "            그 외:\n",
        "                out  (decoder에서 나온 dict 그대로)\n",
        "        \"\"\"\n",
        "        device = images.device\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 1) Backbone\n",
        "        # --------------------------------------------------\n",
        "        feats = self.backbone(images)\n",
        "        # feats: {\"res2\":(B,128,H/4,W/4), ..., \"res5\":(B,1024,H/32,W/32)}\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 2) Pixel Decoder\n",
        "        # --------------------------------------------------\n",
        "        pixel_outs = self.pixel_decoder.forward_features(feats)\n",
        "        # pixel_outs: [P0, P1, P2, P3]\n",
        "        # P0: (B,256,H/32,W/32)\n",
        "        # P1: (B,256,H/16,W/16)\n",
        "        # P2: (B,256,H/8, W/8)\n",
        "        # P3: (B,256,H/4, W/4)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 3) Encoder Input 만들기 (RT-DETR-style)\n",
        "        #    여기서는 P0~P2, 즉 1/32, 1/16, 1/8만 사용\n",
        "        # --------------------------------------------------\n",
        "        memory, spatial_shapes, level_start_index = self.encoder_input(pixel_outs[:3])\n",
        "        # memory: (B, S, C=256)\n",
        "        # spatial_shapes: [[H0,W0],[H1,W1],[H2,W2]]\n",
        "        # level_start_index: [0, H0*W0, H0*W0+H1*W1]\n",
        "\n",
        "        # spatial_shapes를 tensor/리스트 둘 다 대응\n",
        "        if torch.is_tensor(spatial_shapes):\n",
        "            spatial_shapes_list = spatial_shapes.tolist()\n",
        "            spatial_shapes_tensor = spatial_shapes.to(device=device, dtype=torch.long)\n",
        "        else:\n",
        "            spatial_shapes_list = spatial_shapes\n",
        "            spatial_shapes_tensor = torch.as_tensor(\n",
        "                spatial_shapes_list, dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 4) Anchors + valid_mask 생성\n",
        "        # --------------------------------------------------\n",
        "        anchors_single, valid_mask_single = self.anchor_gen(\n",
        "            spatial_shapes=spatial_shapes_list,\n",
        "            device=device,\n",
        "        )\n",
        "        # anchors_single: (1, S, 4)\n",
        "        # valid_mask_single: (1, S, 1)\n",
        "\n",
        "        B, S, C = memory.shape\n",
        "        anchors = anchors_single.expand(B, -1, -1)      # (B,S,4)\n",
        "        valid_mask = valid_mask_single.expand(B, -1, -1)  # (B,S,1)\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 5) EFDTR Transformer Decoder (DN 포함)\n",
        "        # --------------------------------------------------\n",
        "        # decoder는 학습 모드 + targets 있을 때 DN을 켠다고 가정\n",
        "        if self.training:\n",
        "            outputs = self.decoder(\n",
        "                memory=memory,\n",
        "                spatial_shapes=spatial_shapes_tensor,\n",
        "                anchors=anchors,\n",
        "                valid_mask=valid_mask,\n",
        "                multi_scale_feats=pixel_outs,\n",
        "                targets=targets,     # DN용\n",
        "            )\n",
        "        else:\n",
        "            # 평가/추론일 때는 DN 없이\n",
        "            outputs = self.decoder(\n",
        "                memory=memory,\n",
        "                spatial_shapes=spatial_shapes_tensor,\n",
        "                anchors=anchors,\n",
        "                valid_mask=valid_mask,\n",
        "                multi_scale_feats=pixel_outs,\n",
        "                targets=None,\n",
        "            )\n",
        "\n",
        "        # decoder가 (out_main, out_dn, dn_meta) 튜플을 리턴하는 경우 / dict 하나만 리턴하는 경우 둘 다 지원\n",
        "        return outputs\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KrVCYUU51w30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matcher"
      ],
      "metadata": {
        "id": "IWHljQdH9IYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "    union = area1[:, None] + area2 - inter\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    __share__ = ['use_focal_loss', ]\n",
        "\n",
        "    def __init__(self, weight_dict, use_focal_loss=False, alpha=0.25, gamma=2.0):\n",
        "        \"\"\"Creates the matcher\n",
        "\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = weight_dict['cost_class']\n",
        "        self.cost_bbox = weight_dict['cost_bbox']\n",
        "        self.cost_giou = weight_dict['cost_giou']\n",
        "\n",
        "        self.use_focal_loss = use_focal_loss\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "        assert self.cost_class != 0 or self.cost_bbox != 0 or self.cost_giou != 0, \"all costs cant be 0\"\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # 1) 시그모이드 확률 (B*Q, 1)\n",
        "        logits = outputs[\"pred_logits\"].flatten(0, 1)      # [B*Q, 1]\n",
        "        out_prob = logits.sigmoid().squeeze(-1)           # [B*Q]\n",
        "\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)    # [B*Q, 4]\n",
        "\n",
        "        # 2) 타겟 모으기\n",
        "        #   labels는 어차피 전부 1(건물)이라고 가정\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]        # 각 batch당 GT 개수\n",
        "\n",
        "        # 3) classification cost:\n",
        "        #   모든 GT는 \"건물\"이라고 생각하고,\n",
        "        #   예측 확률이 높을수록 cost를 낮게 (-p)로 설정\n",
        "        #   (B*Q, 1) → (B*Q, num_total_gt)\n",
        "        cost_class = -out_prob.unsqueeze(1).expand(-1, tgt_bbox.shape[0])\n",
        "\n",
        "        # 4) bbox L1 cost\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # 5) GIoU cost\n",
        "        cost_giou = -generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(out_bbox),\n",
        "            box_cxcywh_to_xyxy(tgt_bbox),\n",
        "        )\n",
        "\n",
        "        # 6) 최종 cost\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [\n",
        "            (torch.as_tensor(i, dtype=torch.int64),\n",
        "            torch.as_tensor(j, dtype=torch.int64))\n",
        "            for i, j in indices\n",
        "        ]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mtTKee2-3O9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criterion"
      ],
      "metadata": {
        "id": "qk8nqp7N-wYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"\n",
        "    Project-Final unified Criterion:\n",
        "    - vfl: classification\n",
        "    - boxes: L1 + GIoU (same as RT-DETR)\n",
        "    - efd: L1 regression of Fourier coefficients\n",
        "    - polygon: phase-aligned polygon regression (EFDTR 3.4)\n",
        "\n",
        "    Only these 4 losses exist.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, matcher, weight_dict, losses,\n",
        "                 alpha=0.2, gamma=2.0, eos_coef=1e-4, num_classes=80):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.losses = losses\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "        empty_weight = torch.ones(num_classes + 1)\n",
        "        empty_weight[-1] = eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_weight)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Utility: src indices for matched queries\n",
        "    # ------------------------------------------------\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        batch_idx = torch.cat([\n",
        "            torch.full_like(src, b, dtype=torch.int64)\n",
        "            for b, (src, _) in enumerate(indices)\n",
        "        ])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    # ============================================\n",
        "    # 1) VFL Classification loss\n",
        "    # ============================================\n",
        "    def loss_vfl(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        assert 'pred_logits' in outputs and 'pred_boxes' in outputs\n",
        "\n",
        "        src_logits = outputs[\"pred_logits\"]     # (B, Q, C)\n",
        "        src_boxes  = outputs[\"pred_boxes\"]      # (B, Q, 4)\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        matched_pred_boxes = src_boxes[idx]\n",
        "\n",
        "        # GT boxes\n",
        "        target_boxes = torch.cat(\n",
        "            [t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # IoU as supervision score\n",
        "        from torchvision.ops import box_iou\n",
        "        iou, _ = box_iou(box_cxcywh_to_xyxy(matched_pred_boxes),\n",
        "                         box_cxcywh_to_xyxy(target_boxes))\n",
        "        iou_diag = torch.diag(iou).detach()\n",
        "\n",
        "        # GT label map\n",
        "        target_classes_o = torch.cat([\n",
        "            t[\"labels\"][J] for t, (_, J) in zip(targets, indices)\n",
        "        ])\n",
        "        target_classes = torch.full(\n",
        "            src_logits.shape[:2], self.num_classes,\n",
        "            dtype=torch.int64, device=src_logits.device\n",
        "        )\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        # One-hot & score\n",
        "        target_onehot = F.one_hot(target_classes, self.num_classes + 1)[..., :-1]\n",
        "        score_map = torch.zeros_like(target_classes, dtype=src_logits.dtype)\n",
        "        score_map[idx] = iou_diag\n",
        "        score_map = score_map.unsqueeze(-1) * target_onehot\n",
        "\n",
        "        pred_sigmoid = torch.sigmoid(src_logits).detach()\n",
        "        weight = (\n",
        "            self.alpha * pred_sigmoid.pow(self.gamma) * (1 - target_onehot)\n",
        "            + score_map\n",
        "        )\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            src_logits, score_map, weight=weight, reduction=\"none\"\n",
        "        )\n",
        "        loss = loss.mean(1).sum() / max(num_boxes, 1)\n",
        "\n",
        "        return {\"loss_vfl\": loss}\n",
        "\n",
        "    # ============================================\n",
        "    # 2) Box regression loss (L1 + GIoU)\n",
        "    # ============================================\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        assert 'pred_boxes' in outputs\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "\n",
        "        target_boxes = torch.cat(\n",
        "            [t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)], dim=0\n",
        "        )\n",
        "\n",
        "        # L1\n",
        "        loss_l1 = F.l1_loss(src_boxes, target_boxes, reduction='none').sum()\n",
        "\n",
        "        # GIoU\n",
        "        from torchvision.ops import generalized_box_iou\n",
        "        giou = generalized_box_iou(box_cxcywh_to_xyxy(src_boxes),\n",
        "                                   box_cxcywh_to_xyxy(target_boxes))\n",
        "        loss_giou = (1 - torch.diag(giou)).sum()\n",
        "\n",
        "        loss = loss_l1 / num_boxes + loss_giou / num_boxes\n",
        "\n",
        "        return {\n",
        "            \"loss_bbox\": loss_l1 / num_boxes,\n",
        "            \"loss_giou\": loss_giou / num_boxes,\n",
        "        }\n",
        "\n",
        "    # ============================================\n",
        "    # 🔥 EFDTR EXTENSION PART\n",
        "    # 3) EFD L1 Loss\n",
        "    # 4) Phase-aligned Polygon Loss\n",
        "    # ============================================\n",
        "\n",
        "    # ---------- arc length ----------\n",
        "    @staticmethod\n",
        "    def _compute_arc_length_t(poly):\n",
        "        next_pts = torch.roll(poly, -1, dims=0)\n",
        "        seg = (next_pts - poly).norm(dim=-1)\n",
        "        return torch.cat([\n",
        "            torch.zeros(1, device=poly.device, dtype=poly.dtype),\n",
        "            seg.cumsum(0)\n",
        "        ])  # (P+1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _phases_from_t(t):\n",
        "        return 2 * math.pi * t[:-1] / t[-1].clamp(min=1e-6)\n",
        "\n",
        "    @staticmethod\n",
        "    def _snap(pred_phi, gt_phi, n_pred):\n",
        "        diff = (pred_phi[:, None] - gt_phi[None, :]).abs()\n",
        "        diff = torch.minimum(diff, 2 * math.pi - diff)\n",
        "        min_val, min_idx = diff.min(dim=1)\n",
        "        threshold = math.pi / n_pred\n",
        "        snapped = pred_phi.clone()\n",
        "        mask = min_val < threshold\n",
        "        snapped[mask] = gt_phi[min_idx[mask]]\n",
        "        return snapped\n",
        "\n",
        "    # ---------- interpolate polygon ----------\n",
        "    @staticmethod\n",
        "    def _interp(poly, t, phases):\n",
        "        T = t[-1]\n",
        "        t_phase = phases * T / (2 * math.pi)\n",
        "\n",
        "        idx = torch.searchsorted(t, t_phase, right=True) - 1\n",
        "        idx = idx.clamp(0, poly.size(0) - 1)\n",
        "\n",
        "        t0 = t[idx]\n",
        "        t1 = t[idx + 1]\n",
        "        w = ((t_phase - t0) / (t1 - t0).clamp(min=1e-6)).unsqueeze(-1)\n",
        "\n",
        "        p0 = poly[idx]\n",
        "        p1 = poly[(idx + 1) % poly.size(0)]\n",
        "        return (1 - w) * p0 + w * p1\n",
        "\n",
        "    # ---------- full polygon loss ----------\n",
        "    def loss_polygon(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        pred_polys = outputs['pred_polygons']    # (B, Q, P, 2)\n",
        "        all_losses = []\n",
        "\n",
        "        for b, (src_idx, tgt_idx) in enumerate(indices):\n",
        "            if len(src_idx) == 0:\n",
        "                continue\n",
        "\n",
        "            pred_list = [pred_polys[b, s] for s in src_idx]\n",
        "            gt_list = [targets[b][\"polygons\"][t] for t in tgt_idx]\n",
        "\n",
        "            for pred_poly, gt_poly in zip(pred_list, gt_list):\n",
        "                t_gt = self._compute_arc_length_t(gt_poly)\n",
        "                phi_gt = self._phases_from_t(t_gt)\n",
        "\n",
        "                n = pred_poly.shape[0]\n",
        "                pred_phi = torch.linspace(\n",
        "                    0, 2 * math.pi * (1 - 1/n),\n",
        "                    n, device=pred_poly.device\n",
        "                )\n",
        "\n",
        "                snapped = self._snap(pred_phi, phi_gt, n)\n",
        "\n",
        "                t_pred = self._compute_arc_length_t(pred_poly)\n",
        "\n",
        "                # L1\n",
        "                p_pred_L1 = self._interp(pred_poly, t_pred, snapped)\n",
        "                p_gt_L1   = self._interp(gt_poly,  t_gt,   snapped)\n",
        "                L1 = F.smooth_l1_loss(p_pred_L1, p_gt_L1, reduction='mean')\n",
        "\n",
        "                # L2\n",
        "                p_pred_L2 = self._interp(pred_poly, t_pred, phi_gt)\n",
        "                p_gt_L2   = self._interp(gt_poly,  t_gt,   phi_gt)\n",
        "                L2 = F.smooth_l1_loss(p_pred_L2, p_gt_L2, reduction='mean')\n",
        "\n",
        "                all_losses.append(0.5 * (L1 + L2))\n",
        "\n",
        "        if len(all_losses) == 0:\n",
        "            return {\"loss_polygon\": torch.tensor(0.0, device=pred_polys.device)}\n",
        "\n",
        "        loss = torch.stack(all_losses).sum() / max(num_boxes, 1)\n",
        "        return {\"loss_polygon\": loss}\n",
        "\n",
        "    # ---------- EFD L1 ----------\n",
        "    def loss_efd(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        src_efd = outputs['pred_efd']\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        pred = src_efd[idx]\n",
        "\n",
        "        target = torch.cat(\n",
        "            [t[\"efd_params\"][J] for t, (_, J) in zip(targets, indices)],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        loss = F.l1_loss(pred, target, reduction='none').sum() / max(num_boxes, 1)\n",
        "        return {\"loss_efd\": loss}\n",
        "\n",
        "    # ============================================\n",
        "    # get_loss — only the 4 used losses\n",
        "    # ============================================\n",
        "    def get_loss(self, name, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            \"vfl\": self.loss_vfl,\n",
        "            \"boxes\": self.loss_boxes,\n",
        "            \"efd\": self.loss_efd,\n",
        "            \"polygon\": self.loss_polygon,\n",
        "        }\n",
        "        return loss_map[name](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    # ============================================\n",
        "    # forward — standard DETR Criterion forward\n",
        "    # ============================================\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        outputs: dict of:\n",
        "            pred_logits (B,Q,C)\n",
        "            pred_boxes  (B,Q,4)\n",
        "            pred_efd    (B,Q,D)\n",
        "            pred_polygons (B,Q,P,2)\n",
        "        targets: list of dicts\n",
        "        \"\"\"\n",
        "        # 1) Hungarian matching\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # number of GTs\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = max(num_boxes, 1)\n",
        "\n",
        "        # 2) compute each loss\n",
        "        loss_dict = {}\n",
        "        for loss_name in self.losses:\n",
        "            loss_dict.update(\n",
        "                self.get_loss(loss_name, outputs, targets, indices, num_boxes)\n",
        "            )\n",
        "\n",
        "        # 3) weight losses\n",
        "        final = {}\n",
        "        for k, v in loss_dict.items():\n",
        "            if k in self.weight_dict:\n",
        "                final[k] = v * self.weight_dict[k]\n",
        "\n",
        "        return final\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"\n",
        "    EFDTR용 통합 loss 모듈\n",
        "\n",
        "    losses:\n",
        "      - 'vfl'     : Varifocal loss (classification, IoU soft label)\n",
        "      - 'boxes'   : L1 + GIoU bbox regression\n",
        "      - 'efd'     : EFD coefficient L1\n",
        "      - 'polygon' : cyclic-shift 정렬 Smooth-L1 polygon loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        matcher,                # HungarianMatcher\n",
        "        weight_dict,            # {\"loss_vfl\":1, \"loss_bbox\":5, ...}\n",
        "        losses,\n",
        "        alpha=0.75,             # VFL alpha\n",
        "        gamma=2.0,              # VFL gamma\n",
        "        eos_coef=1e-4,\n",
        "        num_classes=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.losses = losses\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # background class weight (거의 안 쓰지만 일단 유지)\n",
        "        empty_weight = torch.ones(num_classes + 1)\n",
        "        empty_weight[-1] = eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_weight)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # permutation index helpers (DETR 스타일)\n",
        "    # ------------------------------------------------\n",
        "    @staticmethod\n",
        "    def _get_src_permutation_idx(indices):\n",
        "        # indices: list[(src_idx, tgt_idx)] (batch 길이)\n",
        "        batch_idx = torch.cat(\n",
        "            [torch.full_like(src, i) for i, (src, _) in enumerate(indices)]\n",
        "        )\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_tgt_permutation_idx(indices):\n",
        "        batch_idx = torch.cat(\n",
        "            [torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)]\n",
        "        )\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 1) Varifocal classification loss\n",
        "    #     - soft label = IoU( pred_box, gt_box )\n",
        "    # ------------------------------------------------\n",
        "    def loss_vfl(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"\n",
        "        outputs: dict with key 'pred_logits', 'pred_boxes'\n",
        "          pred_logits: (B, Q, 1)  -- 건물 vs background 이진\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs and 'pred_boxes' in outputs\n",
        "\n",
        "        src_logits = outputs['pred_logits']      # (B, Q, 1)\n",
        "        src_boxes  = outputs['pred_boxes']       # (B, Q, 4)\n",
        "\n",
        "        bs, num_queries, _ = src_logits.shape\n",
        "\n",
        "        # 매칭된 쿼리 인덱스\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        matched_pred_boxes = src_boxes[idx]      # (N_pos, 4)\n",
        "\n",
        "        # 대응 GT box (cxcywh)\n",
        "        target_boxes = torch.cat(\n",
        "            [t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)], dim=0\n",
        "        )\n",
        "\n",
        "        # IoU (axis-aligned) → soft target score\n",
        "        iou, _ = box_iou(\n",
        "            box_cxcywh_to_xyxy(matched_pred_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes),\n",
        "        )\n",
        "        iou_diag = torch.diag(iou).detach()      # (N_pos,)\n",
        "\n",
        "        # GT class: 0..num_classes-1, background = num_classes\n",
        "        target_classes_o = torch.cat(\n",
        "            [t[\"labels\"][J] for t, (_, J) in zip(targets, indices)]\n",
        "        )\n",
        "        target_classes = torch.full(\n",
        "            src_logits.shape[:2],\n",
        "            self.num_classes,\n",
        "            dtype=torch.int64,\n",
        "            device=src_logits.device,\n",
        "        )   # (B,Q)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        # one-hot, background 제거\n",
        "        target_onehot = F.one_hot(\n",
        "            target_classes, num_classes=self.num_classes + 1\n",
        "        )[..., :-1]    # (B,Q,C)\n",
        "\n",
        "        # score map: positive 위치에 IoU, 나머지 0\n",
        "        score_map = torch.zeros_like(target_classes, dtype=src_logits.dtype)\n",
        "        score_map[idx] = iou_diag\n",
        "        score_map = score_map.unsqueeze(-1) * target_onehot  # (B,Q,C)\n",
        "\n",
        "        # Varifocal weight\n",
        "        pred_sigmoid = src_logits.sigmoid().detach()\n",
        "        weight = self.alpha * pred_sigmoid.pow(self.gamma) * (1 - target_onehot) \\\n",
        "                 + score_map\n",
        "\n",
        "        # BCE with logits, class-wise\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            src_logits, score_map, weight=weight, reduction='none'\n",
        "        )  # (B,Q,C)\n",
        "        # query 차원을 평균, 전체 sum 후 박스 개수로 정규화\n",
        "        loss = loss.mean(-1).sum() / max(num_boxes, 1)\n",
        "\n",
        "        return {\"loss_vfl\": loss}\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 2) Box regression: L1 + GIoU\n",
        "    # ------------------------------------------------\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        assert 'pred_boxes' in outputs\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "\n",
        "        target_boxes = torch.cat(\n",
        "            [t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)], dim=0\n",
        "        )\n",
        "\n",
        "        # L1\n",
        "        loss_l1 = F.l1_loss(src_boxes, target_boxes, reduction='none').sum()\n",
        "\n",
        "        # GIoU\n",
        "        giou = generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes),\n",
        "        )\n",
        "        loss_giou = (1. - torch.diag(giou)).sum()\n",
        "\n",
        "        return {\n",
        "            \"loss_bbox\": loss_l1 / max(num_boxes, 1),\n",
        "            \"loss_giou\": loss_giou / max(num_boxes, 1),\n",
        "        }\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 3) EFD coefficient L1\n",
        "    # ------------------------------------------------\n",
        "    def loss_efd(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        if 'pred_efd' not in outputs:\n",
        "            return {\"loss_efd\": torch.tensor(0.0, device=outputs['pred_boxes'].device)}\n",
        "\n",
        "        src_efd = outputs['pred_efd']   # (B,Q,L)\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        pred = src_efd[idx]\n",
        "\n",
        "        target = torch.cat(\n",
        "            [t[\"efd_params\"][J] for t, (_, J) in zip(targets, indices)],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        loss = F.l1_loss(pred, target, reduction='none').sum() / max(num_boxes, 1)\n",
        "        return {\"loss_efd\": loss}\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Polygon helper: arc-length, phase, interpolate\n",
        "    # ------------------------------------------------\n",
        "    @staticmethod\n",
        "    def _compute_arc_length_t(poly: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        poly: (P,2)   닫힌 폴리곤 (마지막점!=첫점이라 가정, 내부에서 자동 loop)\n",
        "        return: (P,) 누적 arc length (eq.3)\n",
        "        \"\"\"\n",
        "        P = poly.shape[0]\n",
        "        diffs = poly[(torch.arange(P) + 1) % P] - poly\n",
        "        seg_len = torch.norm(diffs, dim=-1)      # (P,)\n",
        "        t = torch.cumsum(seg_len, dim=0)         # (P,)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def _phases_from_t(t: torch.Tensor) -> torch.Tensor:\n",
        "        T = t[-1]\n",
        "        return 2 * math.pi * t / T\n",
        "\n",
        "    @staticmethod\n",
        "    def _interp(poly: torch.Tensor, t: torch.Tensor, phases: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        poly : (P,2)\n",
        "        t    : (P,) arc-length cumulative\n",
        "        phases: (K,) 원하는 phase들 (0~2π)\n",
        "        → poly를 arc-length 기준 선형보간해서 (K,2) 반환\n",
        "        \"\"\"\n",
        "        T = t[-1]\n",
        "        t_phase = phases * T / (2 * math.pi)     # (K,)\n",
        "\n",
        "        # 각 t_phase가 속하는 segment index 찾기\n",
        "        idx = torch.searchsorted(t, t_phase, right=True) - 1\n",
        "        idx = idx.clamp(0, poly.size(0) - 1)\n",
        "\n",
        "        t0 = t[idx]\n",
        "        t1 = t[(idx + 1) % poly.size(0)]\n",
        "        w = ((t_phase - t0) / (t1 - t0).clamp(min=1e-6)).unsqueeze(-1)\n",
        "\n",
        "        p0 = poly[idx]\n",
        "        p1 = poly[(idx + 1) % poly.size(0)]\n",
        "        return (1. - w) * p0 + w * p1\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 4) Polygon loss (간단한 phase snapping 구현)\n",
        "    # ------------------------------------------------\n",
        "    def loss_polygon(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        if 'pred_polygons' not in outputs:\n",
        "            return {\"loss_polygon\": torch.tensor(0.0, device=outputs['pred_boxes'].device)}\n",
        "\n",
        "        pred_polys = outputs['pred_polygons']    # (B, Q, P, 2)\n",
        "        all_losses = []\n",
        "\n",
        "        for b, (src_idx, tgt_idx) in enumerate(indices):\n",
        "            if len(src_idx) == 0:\n",
        "                continue\n",
        "\n",
        "            # 매칭된 쿼리/GT 쌍들\n",
        "            pred_list = [pred_polys[b, s] for s in src_idx]\n",
        "            gt_list   = [targets[b][\"polygons\"][t] for t in tgt_idx]\n",
        "\n",
        "            for pred_poly, gt_poly in zip(pred_list, gt_list):\n",
        "                # pred: (P,2)  — EFD inverse로 나온 128 포인트\n",
        "                P = pred_poly.shape[0]\n",
        "\n",
        "                # GT 폴리곤을 pred와 같은 포인트 개수로 재샘플링\n",
        "                gt_poly = gt_poly.to(pred_poly.device)\n",
        "                t_gt = self._compute_arc_length_t(gt_poly)\n",
        "                phi_uniform = torch.linspace(\n",
        "                    0, 2 * math.pi * (1 - 1.0 / P),\n",
        "                    P, device=pred_poly.device\n",
        "                )\n",
        "                gt_resampled = self._interp(gt_poly, t_gt, phi_uniform)  # (P,2)\n",
        "\n",
        "                # ---- L1: cyclic shift 정렬 (phase snapping 근사) ----\n",
        "                # 모든 shift 에 대해 pred vs rolled(gt) SmoothL1 계산 후 최소 선택\n",
        "                losses_shift = []\n",
        "                for shift in range(P):\n",
        "                    gt_shift = gt_resampled.roll(-shift, dims=0)\n",
        "                    loss_shift = F.smooth_l1_loss(\n",
        "                        pred_poly, gt_shift, reduction='mean'\n",
        "                    )\n",
        "                    losses_shift.append(loss_shift)\n",
        "                L1 = torch.stack(losses_shift).min()\n",
        "\n",
        "                # ---- L2: 정렬 없이 직접 정합 (global constraint) ----\n",
        "                L2 = F.smooth_l1_loss(pred_poly, gt_resampled, reduction='mean')\n",
        "\n",
        "                all_losses.append(0.5 * (L1 + L2))\n",
        "\n",
        "        if len(all_losses) == 0:\n",
        "            # 이 batch에 positive가 하나도 없을 때\n",
        "            device = outputs['pred_boxes'].device\n",
        "            return {\"loss_polygon\": torch.tensor(0.0, device=device)}\n",
        "\n",
        "        loss = torch.stack(all_losses).sum() / max(num_boxes, 1)\n",
        "        return {\"loss_polygon\": loss}\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # loss dispatcher\n",
        "    # ------------------------------------------------\n",
        "    def get_loss(self, name, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            \"vfl\": self.loss_vfl,\n",
        "            \"boxes\": self.loss_boxes,\n",
        "            \"efd\": self.loss_efd,\n",
        "            \"polygon\": self.loss_polygon,\n",
        "        }\n",
        "        return loss_map[name](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # forward\n",
        "    # ------------------------------------------------\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        outputs:\n",
        "          - decoder 그대로 쓴 경우:\n",
        "              {\"class_logits\",\"boxes\",\"efd_params\",\"polygons\"}  (EFDTR decoder out)\n",
        "          - 이미 매핑한 경우:\n",
        "              {\"pred_logits\",\"pred_boxes\",\"pred_efd\",\"pred_polygons\"}\n",
        "\n",
        "        targets: list[dict], 각 dict 안에\n",
        "          \"labels\": (Ni,), \"boxes\": (Ni,4), \"polygons\": (Ni,P_i,2), \"efd_params\": (Ni,L)\n",
        "        \"\"\"\n",
        "\n",
        "        # EFDTR decoder raw 출력이면, DETR 스타일 키로 매핑\n",
        "        if \"pred_logits\" not in outputs and \"class_logits\" in outputs:\n",
        "            outputs = {\n",
        "                **outputs,\n",
        "                \"pred_logits\":  outputs[\"class_logits\"],\n",
        "                \"pred_boxes\":   outputs[\"boxes\"],\n",
        "                \"pred_efd\":     outputs.get(\"efd_params\"),\n",
        "                \"pred_polygons\": outputs.get(\"polygons\"),\n",
        "            }\n",
        "\n",
        "        # 1) Hungarian matching\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # 2) number of GT boxes (정규화용)\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = max(num_boxes, 1)\n",
        "\n",
        "        # 3) loss 계산\n",
        "        loss_dict = {}\n",
        "        for loss_name in self.losses:\n",
        "            loss_dict.update(\n",
        "                self.get_loss(loss_name, outputs, targets, indices, num_boxes)\n",
        "            )\n",
        "\n",
        "        # 4) weight 적용\n",
        "        final = {}\n",
        "        for k, v in loss_dict.items():\n",
        "            if k in self.weight_dict:\n",
        "                final[k] = v * self.weight_dict[k]\n",
        "\n",
        "        return final"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mtBOy1lBUr8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== SMOKE TEST FOR EFDTR ======\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 만약 다른 파일에 있다면 from XXX import EFDTRModel, HungarianMatcher, SetCriterion\n",
        "# 여기서는 현재 노트북에 이미 정의됐다고 가정\n",
        "\n",
        "def build_model_matcher_criterion(\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    # 1) 모델\n",
        "    model = EFDTRModel(\n",
        "        backbone_name=\"swin_base_patch4_window7_224\",  # 네가 쓰는 이름으로 맞춰도 됨\n",
        "        num_classes=2,        # (배경+건물) 셋팅이면 2, 순수 건물만 1로 바꿔도 됨\n",
        "        num_queries=100,\n",
        "        num_denoising=20,\n",
        "        num_harmonics=1,\n",
        "        num_points_poly=128,\n",
        "    ).to(device)\n",
        "\n",
        "    # 2) 매쳐\n",
        "    matcher = HungarianMatcher(\n",
        "        weight_dict={\n",
        "            \"cost_class\": 1.0,\n",
        "            \"cost_bbox\": 5.0,\n",
        "            \"cost_giou\": 2.0,\n",
        "        },\n",
        "        use_focal_loss=True,\n",
        "    )\n",
        "\n",
        "    # 3) 크리테리온 (SetCriterion는 내가 방금 짜준 버전으로 교체해둔 상태라고 가정)\n",
        "    weight_dict = {\n",
        "        \"loss_vfl\": 1.0,\n",
        "        \"loss_bbox\": 5.0,\n",
        "        \"loss_giou\": 2.0,\n",
        "        \"loss_efd\": 1.0,\n",
        "        \"loss_polygon\": 2.0,\n",
        "    }\n",
        "    criterion = SetCriterion(\n",
        "        matcher=matcher,\n",
        "        weight_dict=weight_dict,\n",
        "        losses=[\"vfl\", \"boxes\", \"efd\", \"polygon\"],\n",
        "        num_classes=1,   # 실제 클래스 개수 (건물 1개라고 가정)\n",
        "    ).to(device)\n",
        "\n",
        "    return model, matcher, criterion\n",
        "\n",
        "\n",
        "def make_dummy_targets(\n",
        "    batch_size,\n",
        "    num_gt_per_image=3,\n",
        "    num_harmonics=1,\n",
        "    num_points_poly=128,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    targets = []\n",
        "    efd_dim = 4 * num_harmonics + 1  # 4n+1\n",
        "\n",
        "    for _ in range(batch_size):\n",
        "        N = num_gt_per_image\n",
        "\n",
        "        # GT boxes: cx,cy,w,h in [0,1]\n",
        "        boxes = torch.rand(N, 4, device=device)\n",
        "        boxes[:, 2:] *= 0.5  # w,h 를 너무 크게 만들지 않게\n",
        "\n",
        "        # GT labels: 전부 class 0 (건물)\n",
        "        labels = torch.zeros(N, dtype=torch.long, device=device)\n",
        "\n",
        "        # GT EFD params (phase 정규화 전용 dummy)\n",
        "        efd_params = torch.rand(N, efd_dim, device=device)\n",
        "\n",
        "        # GT polygons: (N, 128, 2), 좌표 [0,1]\n",
        "        polygons = torch.rand(N, num_points_poly, 2, device=device)\n",
        "\n",
        "        targets.append(\n",
        "            {\n",
        "                \"boxes\": boxes,\n",
        "                \"labels\": labels,\n",
        "                \"efd_params\": efd_params,\n",
        "                \"polygons\": polygons,\n",
        "            }\n",
        "        )\n",
        "    return targets\n",
        "\n",
        "\n",
        "def smoke_train_step_EFDTR():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"[SMOKE] Device: {device}\")\n",
        "\n",
        "    # 1) 모델/매쳐/크리테리온 생성\n",
        "    model, matcher, criterion = build_model_matcher_criterion(device)\n",
        "    model.train()\n",
        "\n",
        "    # 2) 더미 입력 / 타깃 생성\n",
        "    batch_size = 2\n",
        "    H = W = 224  # swin_base_patch4_window7_224 에 맞춰 224 사용\n",
        "    images = torch.randn(batch_size, 3, H, W, device=device)\n",
        "\n",
        "    targets = make_dummy_targets(\n",
        "        batch_size=batch_size,\n",
        "        num_gt_per_image=3,\n",
        "        num_harmonics=1,\n",
        "        num_points_poly=128,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # 3) Forward\n",
        "    outputs = model(images, targets)  # training mode → (out_main, out_dn, dn_meta) 또는 dict 하나\n",
        "\n",
        "    if isinstance(outputs, tuple):\n",
        "        out_main, out_dn, dn_meta = outputs\n",
        "        print(\"[Decoder Output] (training with DN)\")\n",
        "        print(\" main class_logits:\", out_main[\"class_logits\"].shape)\n",
        "        print(\" main boxes      :\", out_main[\"boxes\"].shape)\n",
        "        print(\" main efd_params :\", out_main[\"efd_params\"].shape)\n",
        "        print(\" main polygons   :\", out_main[\"polygons\"].shape)\n",
        "\n",
        "        print(\" dn class_logits :\", out_dn[\"class_logits\"].shape)\n",
        "        print(\" dn boxes        :\", out_dn[\"boxes\"].shape)\n",
        "        print(\" dn efd_params   :\", out_dn[\"efd_params\"].shape)\n",
        "        print(\" dn polygons     :\", out_dn[\"polygons\"].shape)\n",
        "\n",
        "        outputs_for_loss = out_main  # 보통 메인 쿼리에만 loss 걸기\n",
        "    else:\n",
        "        outputs_for_loss = outputs\n",
        "        print(\"[Decoder Output] (no DN)\")\n",
        "        print(\" class_logits:\", outputs_for_loss[\"class_logits\"].shape)\n",
        "        print(\" boxes      :\", outputs_for_loss[\"boxes\"].shape)\n",
        "        print(\" efd_params :\", outputs_for_loss[\"efd_params\"].shape)\n",
        "        print(\" polygons   :\", outputs_for_loss[\"polygons\"].shape)\n",
        "\n",
        "    # 4) Loss 계산\n",
        "    loss_dict = criterion(outputs_for_loss, targets)\n",
        "    total_loss = sum(loss_dict.values())\n",
        "\n",
        "    print(\"\\n[Loss components]\")\n",
        "    for k, v in loss_dict.items():\n",
        "        print(f\" {k}: {float(v):.4f}\")\n",
        "    print(f\" total_loss: {float(total_loss):.4f}\")\n",
        "\n",
        "    # 5) Backward & optimizer.step() 테스트\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"\\n[SMOKE] ✔ Backward & optimizer.step() success!\")\n",
        "\n",
        "\n",
        "# 실제로 한 번 호출\n",
        "smoke_train_step_EFDTR()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632,
          "referenced_widgets": [
            "3cc6ea3ed30f45d7a838473a91995458",
            "627dcaff44d248be8d8c40a631de3c35",
            "48364aebddde4b42922deb28af320950",
            "c62ed108c4ce494283c4d04e9f1e1df6",
            "ce62a3f3e20746b29e8a0e732d9b6912",
            "8d2c0e71831e470283d5b6bccbfdb726",
            "15d677a6dd194b05ba101efdc1a92566",
            "10a3f9150f4b43b9a188c6cc0f234f4e",
            "b97fda84014d40cfb36c197f651efca5",
            "2c408f5099bd4c97bccea0005170b5cc",
            "98351677384a4ac5b14ac64bf9db9de8"
          ]
        },
        "id": "e26x-K9YktIS",
        "outputId": "5e079c4c-61ec-449b-c414-009b9442e8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cc6ea3ed30f45d7a838473a91995458"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2143581798.py:585: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  offset_normalizer = torch.tensor(value_spatial_shapes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Decoder Output] (training with DN)\n",
            " main class_logits: torch.Size([2, 100, 1])\n",
            " main boxes      : torch.Size([2, 100, 4])\n",
            " main efd_params : torch.Size([2, 100, 5])\n",
            " main polygons   : torch.Size([2, 100, 128, 2])\n",
            " dn class_logits : torch.Size([2, 36, 1])\n",
            " dn boxes        : torch.Size([2, 36, 4])\n",
            " dn efd_params   : torch.Size([2, 36, 5])\n",
            " dn polygons     : torch.Size([2, 36, 128, 2])\n",
            "\n",
            "[Loss components]\n",
            " loss_vfl: 19.9074\n",
            " loss_bbox: 1.9924\n",
            " loss_giou: 1.8952\n",
            " loss_efd: 2.3718\n",
            " loss_polygon: 0.7861\n",
            " total_loss: 26.9528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-114033574.py:138: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  print(f\" {k}: {float(v):.4f}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SMOKE] ✔ Backward & optimizer.step() success!\n"
          ]
        }
      ]
    }
  ]
}