{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vuR53lX15wLJ_pfwaiDKUHGCyid7VRm5","timestamp":1763191367329}],"gpuType":"T4","authorship_tag":"ABX9TyMJeuk6LjuFTLEAm1O5uZlr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#ÌõÑÏ≤òÎ¶¨, ÌèâÍ∞Ä Ï∂îÍ∞ÄÌñàÏäµÎãàÎã§(Ïò§Î•òÌôïÏù∏ÌõÑ ÏàòÏ†ï ÏòàÏ†ï)\n","\n","# 1.1: ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò (ÌïÑÏöîÏãú Ï£ºÏÑù Ìï¥Ï†ú)\n","!pip install torch torchvision\n","!pip install timm\n","!pip install 'git+https://github.com/facebookresearch/fvcore'\n","!pip install geopandas rasterio\n","\n","# 1.2: Í∏∞Î≥∏ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import glob\n","import numpy as np\n","import math\n","import copy\n","import inspect\n","import functools\n","import logging\n","from typing import Callable, Dict, List, Optional, Tuple, Union\n","\n","# 1.3: Î™®Îç∏ ÏΩîÎìúÏóê ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n","import fvcore.nn.weight_init as weight_init\n","from fvcore.common.config import CfgNode as _CfgNode\n","from timm.layers import trunc_normal_, to_2tuple, DropPath\n","from torch.cuda.amp import autocast\n","from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n","import torch.utils.checkpoint as checkpoint\n","from torch import Tensor\n","\n","# 1.4: (Ï∞∏Í≥†) Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨Ïóê ÏÇ¨Ïö©ÌñàÎçò ÎùºÏù¥Î∏åÎü¨Î¶¨\n","import geopandas as gpd\n","import rasterio\n","from rasterio.windows import Window\n","from rasterio.features import rasterize\n","from shapely.geometry import box"],"metadata":{"id":"EDuNFUyHp-zT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763207775067,"user_tz":-540,"elapsed":19537,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}},"outputId":"3d445320-9028-4ce2-a5dc-daebcecc72ff"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\n","Collecting git+https://github.com/facebookresearch/fvcore\n","  Cloning https://github.com/facebookresearch/fvcore to /tmp/pip-req-build-ko2dd_98\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/fvcore /tmp/pip-req-build-ko2dd_98\n","  Resolved https://github.com/facebookresearch/fvcore to commit 70e69fb3ff7a82631f6a82061a7650c4f11da876\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (2.0.2)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (0.1.8)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (6.0.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (4.67.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (3.2.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (11.3.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (0.9.0)\n","Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (0.1.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore==0.1.6) (4.15.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore==0.1.6) (3.2.0)\n","Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n","Requirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.4.3)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.0.2)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n","Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.1.2)\n","Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio) (1.1.1.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["# 2. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ (Î≥ÑÎèÑ Ïã§Ìñâ Í∞ÄÏ†ï)\n","# ------------------------------------\n","\n","def convert_to_target_format(label_mask):\n","    \"\"\"\n","    [Ìó¨Ìçº Ìï®Ïàò]\n","    'ÌÜµÌï© ÎùºÎ≤® ÎßàÏä§ÌÅ¨'([H, W])Î•º Î™®Îç∏ ÌïôÏäµÏóê ÌïÑÏöîÌïú 'targets' ÎîïÏÖîÎÑàÎ¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n","    Ïù¥ Ìï®ÏàòÎäî GIS Îç∞Ïù¥ÌÑ∞(Ï†ïÏàò ID)Î•º Îî•Îü¨Îãù Î™®Îç∏Ïù¥ Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÏãù(Ïù¥ÏßÑ ÎßàÏä§ÌÅ¨)ÏúºÎ°ú Î≤àÏó≠ÌïòÎäî Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§.\n","    \"\"\"\n","    # ÌÜµÌï© ÎßàÏä§ÌÅ¨ÏóêÏÑú Í≥†Ïú†Ìïú Í±¥Î¨º ID(1, 2, 3...)Î•º Î™®Îëê Ï∞æÏïÑÎÉÖÎãàÎã§. Î∞∞Í≤Ω Í∞íÏù∏ 0ÏùÄ Ï†úÏô∏Ìï©ÎãàÎã§.\n","    instance_ids = torch.unique(label_mask)\n","    instance_ids = instance_ids[instance_ids != 0]\n","\n","    # ÎßåÏïΩ ÌòÑÏû¨ ÌÉÄÏùºÏóê Í±¥Î¨ºÏù¥ ÌïòÎÇòÎèÑ ÏóÜÎã§Î©¥(IDÍ∞Ä ÏóÜÎã§Î©¥), ÌïôÏäµ Ïãú Ïò§Î•òÍ∞Ä ÎÇòÏßÄ ÏïäÎèÑÎ°ù\n","    # ÎÇ¥Ïö©Ïù¥ ÎπÑÏñ¥ÏûàÎäî ÌÖêÏÑúÎ•º Í∞ÄÏßÑ ÎîïÏÖîÎÑàÎ¶¨Î•º Î∞òÌôòÌï©ÎãàÎã§.\n","    if len(instance_ids) == 0:\n","        return {'instance_class': torch.tensor([], dtype=torch.long), 'mask': torch.tensor([], dtype=torch.uint8)}\n","\n","    #'Î∏åÎ°úÎìúÏ∫êÏä§ÌåÖ'Ïù¥ÎùºÎäî ÌÖêÏÑú Ïó∞ÏÇ∞ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨, Í∞Å Í±¥Î¨º IDÏóê Ìï¥ÎãπÌïòÎäî ÌîΩÏÖÄÎßå 1(True)Ïù¥Í≥†\n","    # ÎÇòÎ®∏ÏßÄÎäî 0(False)Ïù∏ Í∞úÎ≥Ñ ÎßàÏä§ÌÅ¨Îì§ÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏÉùÏÑ±Ìï©ÎãàÎã§.\n","    # Í≤∞Í≥ºÏ†ÅÏúºÎ°ú [N, H, W] ÌÅ¨Í∏∞Ïùò 3Ï∞®Ïõê ÌÖêÏÑúÍ∞Ä ÎßåÎì§Ïñ¥ÏßëÎãàÎã§. (N: Í±¥Î¨º Í∞úÏàò)\n","    masks = (label_mask == instance_ids[:, None, None])\n","\n","    # ÌòÑÏû¨ ÌîÑÎ°úÏ†ùÌä∏ÏóêÏÑúÎäî 'Í±¥Î¨º'Ïù¥ÎùºÎäî Îã®Ïùº ÌÅ¥ÎûòÏä§Îßå ÏÇ¨Ïö©ÌïòÎØÄÎ°ú,\n","    # Í±¥Î¨º Í∞úÏàò(N)ÎßåÌÅº 0ÏúºÎ°ú Ï±ÑÏõåÏßÑ ÌÅ¥ÎûòÏä§ ÎùºÎ≤® ÌÖêÏÑúÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n","    instance_classes = torch.zeros(len(instance_ids), dtype=torch.long)\n","\n","    # ÌåÄÏõêÏù¥ ÏöîÏ≤≠Ìïú ÏµúÏ¢Ö ÌòïÏãùÏóê ÎßûÏ∂∞ 'instance_class'ÏôÄ 'mask'Î•º ÌÇ§Î°ú ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨Î•º Íµ¨ÏÑ±ÌïòÏó¨ Î∞òÌôòÌï©ÎãàÎã§.\n","    return {'instance_class': instance_classes, 'masks': masks.to(torch.uint8)}\n","\n","\n","def create_final_dataset_per_file(\n","    ortho_image_path,\n","    building_shp_path,\n","    output_dir,\n","    tile_size=1024\n","):\n","    \"\"\"\n","    [RAM/Ïä§ÌÜ†Î¶¨ÏßÄ Î¨∏Ï†ú Ìï¥Í≤∞ ÏµúÏ¢Ö Î≤ÑÏ†Ñ]\n","    GIS Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÍ≥µÌïòÏó¨, Ïù¥ÎØ∏ÏßÄ ÌÉÄÏùºÍ≥º Í∑∏Ïóê 1:1Î°ú ÎåÄÏùëÌïòÎäî 'targets' ÎîïÏÖîÎÑàÎ¶¨Î•º\n","    Í∞ÅÍ∞Å Í∞úÎ≥Ñ ÌååÏùºÎ°ú Ï†ÄÏû•ÌïòÏó¨ RAM Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî ÏµúÏ¢Ö Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÏûÑ\n","\n","    [Ï§ëÏöî] Ïôú 'ÎîïÏÖîÎÑàÎ¶¨Ïùò Î¶¨Ïä§Ìä∏' ([{...}, {...}, ...])Î•º ÎßåÎì§ÏßÄ ÏïäÎäîÍ∞Ä?\n","    ----------------------------------------------------------------\n","    Ïù¥ 'ÎîïÏÖîÎÑàÎ¶¨Ïùò Î¶¨Ïä§Ìä∏' ÌòïÏãùÏùÄ Î™®Îì† ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨Î•º ÌïòÎÇòÏùò Í±∞ÎåÄÌïú Î¶¨Ïä§Ìä∏ Î≥ÄÏàòÏóê\n","    Îã¥ÏïÑÎëêÏóàÎã§Í∞Ä ÎßàÏßÄÎßâÏóê Ìïú Î≤àÏóê ÌååÏùºÎ°ú Ï†ÄÏû•ÌïòÎäî Î∞©ÏãùÏûÑ\n","\n","    Î∞úÏÉùÌñàÎçò Î¨∏Ï†ú:\n","    1. Î©îÎ™®Î¶¨ Ï¥àÍ≥º (RAM OOM): 'targets' ÎîïÏÖîÎÑàÎ¶¨Îäî ÎÇ¥Î∂ÄÏóê [N, H, W] ÌÅ¨Í∏∞Ïùò ÌÅ∞ ÎßàÏä§ÌÅ¨ ÌÖêÏÑúÎ•º\n","       Ìè¨Ìï®ÌïòÏó¨ Ïö©ÎüâÏù¥ Îß§Ïö∞ ÌÅΩÎãàÎã§. ÏàòÏ≤ú Í∞úÏùò ÎîïÏÖîÎÑàÎ¶¨Î•º ÌïòÎÇòÏùò Î¶¨Ïä§Ìä∏Ïóê Î™®Îëê Îã¥ÏúºÎ©¥\n","       ColabÏùò Ï†úÌïúÎêú RAM(12GB)ÏùÑ ÏàúÏãùÍ∞ÑÏóê Ï¥àÍ≥ºÌïòÏó¨ Ïª§ÎÑê Ïû¨ÏãúÏûë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏùå\n","    2. Ï†ÄÏû• ÎπÑÌö®Ïú®: ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÎäî Îã®Ïùº Î¶¨Ïä§Ìä∏ ÌååÏùºÏùò ÌÅ¨Í∏∞Í∞Ä Îß§Ïö∞ Ïª§Ï†∏\n","       Google DriveÏùò Ï†ÄÏû• Í≥µÍ∞ÑÎèÑ Îπ†Î•¥Í≤å ÏÜåÏßÑÎê®\n","\n","    ÌòÑÏû¨ ÏΩîÎìúÏùò Ìï¥Í≤∞Ï±Ö:\n","    Ïù¥ Ìï®ÏàòÎäî Ìïú Î≤àÏóê ÌïòÎÇòÏùò ÌÉÄÏùºÎßå Ï≤òÎ¶¨ÌïòÍ≥†, ÏÉùÏÑ±Îêú Ïù¥ÎØ∏ÏßÄÏôÄ ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨Î•º Ï¶âÏãú\n","    Í∞úÎ≥Ñ ÌååÏùºÎ°ú Ï†ÄÏû•Ìï®. Ïù¥ 'ÌååÏùº Îã®ÏúÑ Ï†ÄÏû•' Î∞©ÏãùÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ ÏµúÏÜåÌôîÌïòÏó¨\n","    Colab ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞Î•º ÏïàÏ†ïÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨Ìï®\n","    ----------------------------------------------------------------\n","    \"\"\"\n","    # Í≤∞Í≥ºÎ¨ºÏù¥ Ï†ÄÏû•Îê† 'images'ÏôÄ 'labels_dict' Ìè¥ÎçîÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n","    img_tile_dir = os.path.join(output_dir, \"images\")\n","    lbl_dict_dir = os.path.join(output_dir, \"labels_dict\")\n","    os.makedirs(img_tile_dir, exist_ok=True)\n","    os.makedirs(lbl_dict_dir, exist_ok=True)\n","\n","    # GeoPandasÎ°ú Í±¥Î¨º shapefileÏùÑ, RasterioÎ°ú Ï†ïÏÇ¨ÏòÅÏÉÅ(.tif)ÏùÑ ÏóΩÎãàÎã§.\n","    gdf_buildings = gpd.read_file(building_shp_path)\n","\n","    with rasterio.open(ortho_image_path) as src:\n","        # ÏòÅÏÉÅ Ï†ÑÏ≤¥Î•º 1024x1024 ÌÉÄÏùºÎ°ú ÍπîÎÅîÌïòÍ≤å ÎÇòÎàÑÍ∏∞ ÏúÑÌï¥ Í∞ÄÎ°ú/ÏÑ∏Î°ú ÌÅ¨Í∏∞Î•º 1024Ïùò Î∞∞ÏàòÎ°ú Ï°∞Ï†ïÌï©ÎãàÎã§.\n","        width = (src.width // tile_size) * tile_size\n","        height = (src.height // tile_size) * tile_size\n","        print(f\"Ï†ÑÏ≤¥ ÏòÅÏÉÅ ÌÅ¨Í∏∞: ({src.width}, {src.height}), Ï°∞Ï†ïÎêú ÌÅ¨Í∏∞: ({width}, {height})\")\n","\n","        # ÎßåÏïΩ Îëê Îç∞Ïù¥ÌÑ∞Ïùò Ï¢åÌëúÍ≥Ñ(CRS)Í∞Ä Îã§Î•¥Îã§Î©¥, Í±¥Î¨º Îç∞Ïù¥ÌÑ∞Î•º Ï†ïÏÇ¨ÏòÅÏÉÅ Í∏∞Ï§ÄÏúºÎ°ú ÌÜµÏùºÌï©ÎãàÎã§.\n","        if gdf_buildings.crs != src.crs:\n","            gdf_buildings = gdf_buildings.to_crs(src.crs)\n","\n","        # ÏÉùÏÑ±Îêú ÌååÏùº Í∞úÏàòÎ•º ÏÑ∏Í∏∞ ÏúÑÌïú Ïπ¥Ïö¥ÌÑ∞ÏûÖÎãàÎã§.\n","        tile_count = 0\n","        # Ïù¥Ï§ë forÎ¨∏ÏùÑ ÏÇ¨Ïö©Ìï¥ Ï†ÑÏ≤¥ ÏòÅÏÉÅÏùÑ 1024x1024 ÌÅ¨Í∏∞Ïùò ÌÉÄÏùº(window) Îã®ÏúÑÎ°ú ÏàúÌöåÌï©ÎãàÎã§.\n","        for j in range(0, height, tile_size):\n","            for i in range(0, width, tile_size):\n","                window = Window(i, j, tile_size, tile_size)\n","                tile_transform = src.window_transform(window)\n","\n","                # ÌòÑÏû¨ ÌÉÄÏùºÏùò ÏßÄÎ¶¨Ï†Å Ï¢åÌëú Í≤ΩÍ≥ÑÎ•º Í≥ÑÏÇ∞ÌïòÍ≥†, Ïù¥ Í≤ΩÍ≥ÑÏôÄ Í≤πÏπòÎäî Í±¥Î¨ºÎßå Îπ†Î•¥Í≤å Ï∞æÏïÑÎÉÖÎãàÎã§.\n","                tile_bounds = rasterio.windows.bounds(window, src.transform)\n","                tile_bbox = box(*tile_bounds)\n","                intersecting_buildings = gdf_buildings[gdf_buildings.intersects(tile_bbox)]\n","\n","                # ÌòÑÏû¨ ÌÉÄÏùº ÏïàÏóê Í±¥Î¨ºÏù¥ ÌïòÎÇòÎùºÎèÑ ÏûàÏùÑ Í≤ΩÏö∞ÏóêÎßå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è Ï†ÄÏû•ÏùÑ ÏßÑÌñâÌï©ÎãàÎã§.\n","                if not intersecting_buildings.empty:\n","                    # 1. 'ÌÜµÌï© ÎùºÎ≤® ÎßàÏä§ÌÅ¨'Î•º Î©îÎ™®Î¶¨ ÏÉÅÏóêÏÑú ÏÉùÏÑ±Ìï©ÎãàÎã§. (Î∞∞Í≤Ω=0, Í±¥Î¨º=1, 2, 3...)\n","                    simple_ids = range(1, len(intersecting_buildings) + 1)\n","                    shapes = [(geom, id) for geom, id in zip(intersecting_buildings.geometry, simple_ids)]\n","\n","                    label_mask = rasterize(\n","                        shapes=shapes,\n","                        out_shape=(tile_size, tile_size),\n","                        transform=tile_transform,\n","                        fill=0,\n","                        all_touched=True,\n","                        dtype=rasterio.int32\n","                    )\n","\n","                    # 2. ÏúÑÏóêÏÑú ÏÉùÏÑ±Îêú 'ÌÜµÌï© ÎùºÎ≤® ÎßàÏä§ÌÅ¨'Î•º Ï¶âÏãú 'targets' ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n","                    target_dict = convert_to_target_format(torch.from_numpy(label_mask).long())\n","\n","                    # 3. ÏµúÏ¢Ö Í≤∞Í≥ºÎ¨ºÏù∏ Ïù¥ÎØ∏ÏßÄÏôÄ 'targets' ÎîïÏÖîÎÑàÎ¶¨Î•º 1:1Î°ú Îß§Ïπ≠ÎêòÎäî Í∞úÎ≥Ñ ÌååÏùºÎ°ú Ï¶âÏãú Ï†ÄÏû•Ìï©ÎãàÎã§.\n","                    image_tile = src.read(window=window)\n","                    if image_tile.shape[0] > 3:\n","                        image_tile = image_tile[:3, :, :]\\\n","\n","                    image_tensor = torch.from_numpy(image_tile).float()\n","\n","                    torch.save(image_tensor, os.path.join(img_tile_dir, f\"tile_{tile_count}.pt\"))\n","                    torch.save(target_dict, os.path.join(lbl_dict_dir, f\"tile_{tile_count}.pt\"))\n","\n","                    # ÌååÏùº Ï†ÄÏû• ÌõÑ Ïπ¥Ïö¥ÌÑ∞Î•º 1 Ï¶ùÍ∞ÄÏãúÌÇµÎãàÎã§.\n","                    tile_count += 1\n","\n","        print(f\"\\n--- ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± ÏôÑÎ£å ---\")\n","        print(f\"Ï¥ù {tile_count}Í∞úÏùò [Ïù¥ÎØ∏ÏßÄ ÌååÏùº]Í≥º [ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨ ÌååÏùº] ÏåçÏùÑ ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.\")\n","        print(f\"Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏúÑÏπò: {img_tile_dir}\")\n","        print(f\"ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨ Ï†ÄÏû• ÏúÑÏπò: {lbl_dict_dir}\")\n","\n","\n","\n","# 3Îã®Í≥Ñ: Ìï®Ïàò Ïã§Ìñâ\n","\n","\n","# --- Google Drive Í≤ΩÎ°ú ÏÑ§Ï†ï ---\n","# QGISÏóêÏÑú Ï†ÑÏ≤òÎ¶¨Ìïú ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÌååÏùºÎì§Ïùò Í≤ΩÎ°úÏûÖÎãàÎã§.\n","ORTHO_TIF_PATH = \"/content/drive/MyDrive/gis_project/clipped_Ï†ïÏÇ¨ÏòÅÏÉÅ.tif\"\n","BUILDING_SHP_PATH = \"/content/drive/MyDrive/gis_project/clipped_buildings.shp\"\n","# ÏµúÏ¢Ö Í≤∞Í≥ºÎ¨ºÏù¥ Ï†ÄÏû•Îê† Ìè¥ÎçîÏùò Í≤ΩÎ°úÏûÖÎãàÎã§.\n","OUTPUT_DIR = \"/content/drive/MyDrive/gis_project/final_dataset_per_file\"\n","# --------------------------------\n","\n","# ÏúÑÏóêÏÑú Ï†ïÏùòÌïú ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± Ìï®ÏàòÎ•º Ïã§ÌñâÌï©ÎãàÎã§.\n","# (Ï∞∏Í≥†: Ïù¥ ÏΩîÎìúÎäî ÌïôÏäµ ÎÖ∏Ìä∏Î∂ÅÍ≥ºÎäî Î≥ÑÎèÑÎ°ú *Î®ºÏ†Ä* Ïã§ÌñâÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.)\n","# create_final_dataset_per_file(ORTHO_TIF_PATH, BUILDING_SHP_PATH, OUTPUT_DIR)\n","\n","# preprocess_gis_data_ipynb.ipynb ÌååÏùºÏùÄ\n","# Ïù¥ ÌïôÏäµ ÏΩîÎìúÏôÄ Î≥ÑÎèÑÎ°ú *ÎØ∏Î¶¨ Ïã§Ìñâ*ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n","#\n","# ÏïÑÎûò ÏΩîÎìúÎäî Google DriveÏùò '/content/drive/MyDrive/gis_project/final_dataset_per_file' Í≤ΩÎ°úÏóê\n","# Ïù¥ÎØ∏ÏßÄ ÌÉÄÏùº(images/tile_X.pt)Í≥º ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨(labels_dict/tile_X.pt)Í∞Ä\n","# Ïù¥ÎØ∏ Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ïÌïòÍ≥† ÏßÑÌñâÌï©ÎãàÎã§.\n","# ------------------------------------"],"metadata":{"id":"wc-FhJj-m8gk","executionInfo":{"status":"ok","timestamp":1763207775083,"user_tz":-540,"elapsed":14,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 1.1: GIS ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n","!pip install rasterio geopandas\n","\n","# 1.2: ÎçîÎØ∏Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏΩîÎìú\n","import rasterio\n","import geopandas as gpd\n","import numpy as np\n","from shapely.geometry import box\n","from rasterio.transform import from_origin\n","\n","print(\"--- 1. ÌÖåÏä§Ìä∏Ïö© TIF/SHP ÌååÏùº ÏÉùÏÑ±ÏùÑ ÏãúÏûëÌï©ÎãàÎã§. ---\")\n","\n","# --- ÏÑ§Ï†ïÍ∞í ---\n","TIF_FILENAME = \"dummy_image.tif\"\n","SHP_FILENAME = \"dummy_buildings.shp\"\n","IMAGE_SIZE = 1500  # 1024x1024 ÌÉÄÏùº 1Í∞úÍ∞Ä ÎÇòÏò§ÎèÑÎ°ù 1024Î≥¥Îã§ ÌÅ¨Í≤å ÏÑ§Ï†ï\n","TARGET_CRS = \"EPSG:3857\"\n","\n","# --- 1. Í∞ÄÏßú TIF Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± (3-Band, 1500x1500) ---\n","np_image = np.random.randint(0, 256, (3, IMAGE_SIZE, IMAGE_SIZE), dtype=np.uint8)\n","# YÏ¢åÌëú Î≤îÏúÑ: [north - (height * ysize), north] -> [-500, 1000]\n","# XÏ¢åÌëú Î≤îÏúÑ: [west, west + (width * xsize)] -> [1000, 2500]\n","geo_transform = from_origin(west=1000.0, north=1000.0, xsize=1.0, ysize=1.0)\n","\n","with rasterio.open(\n","    TIF_FILENAME, 'w', driver='GTiff', height=IMAGE_SIZE, width=IMAGE_SIZE,\n","    count=3, dtype=np_image.dtype, crs=TARGET_CRS, transform=geo_transform,\n",") as dst:\n","    dst.write(np_image)\n","print(f\"‚úÖ [1/2] Í∞ÄÏßú Ïù¥ÎØ∏ÏßÄ '{TIF_FILENAME}' ÏÉùÏÑ± ÏôÑÎ£å\")\n","\n","# --- 2. Í∞ÄÏßú SHP ÌååÏùº ÏÉùÏÑ± (Polygon) ---\n","# TIF ÏòÅÏó≠ (X: 1000~2500, Y: -500~1000) ÏïàÏóê Í±¥Î¨º 2Í∞ú ÏÉùÏÑ±\n","# Box 1: (X: 1100~1200, Y: 800~900)\n","building_1 = box(1100, 800, 1200, 900)\n","# Box 2: (X: 1500~1600, Y: 300~500)\n","building_2 = box(1500, 300, 1600, 500)\n","\n","gdf = gpd.GeoDataFrame(\n","    {'id': [1, 2]}, geometry=[building_1, building_2], crs=TARGET_CRS\n",")\n","gdf.to_file(SHP_FILENAME, driver='ESRI Shapefile')\n","\n","print(f\"‚úÖ [2/2] Í∞ÄÏßú Í±¥Î¨º '{SHP_FILENAME}' ÏÉùÏÑ± ÏôÑÎ£å\")\n","print(\"\\n--- üèÅ Î™®Îì† ÌÖåÏä§Ìä∏ ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMLNdrYUQfD9","executionInfo":{"status":"ok","timestamp":1763207784770,"user_tz":-540,"elapsed":9683,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}},"outputId":"297b8bf3-b895-4fff-befe-e904083a8022"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.4.3)\n","Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n","Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio) (1.1.1.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n","Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n","--- 1. ÌÖåÏä§Ìä∏Ïö© TIF/SHP ÌååÏùº ÏÉùÏÑ±ÏùÑ ÏãúÏûëÌï©ÎãàÎã§. ---\n","‚úÖ [1/2] Í∞ÄÏßú Ïù¥ÎØ∏ÏßÄ 'dummy_image.tif' ÏÉùÏÑ± ÏôÑÎ£å\n","‚úÖ [2/2] Í∞ÄÏßú Í±¥Î¨º 'dummy_buildings.shp' ÏÉùÏÑ± ÏôÑÎ£å\n","\n","--- üèÅ Î™®Îì† ÌÖåÏä§Ìä∏ ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å ---\n"]}]},{"cell_type":"code","source":["# (2Î≤à ÏÖÄÏùò ÏïûÎ∂ÄÎ∂Ñ ... Ìï®Ïàò Ï†ïÏùò ... Îäî Í∑∏ÎåÄÎ°ú Îë°ÎãàÎã§)\n","# ... def create_final_dataset_per_file(...) ...\n","# ... (ÏÉùÎûµ) ...\n","# ------------------------------------\n","\n","# 3Îã®Í≥Ñ: Ìï®Ïàò Ïã§Ìñâ\n","\n","# --- Google Drive Í≤ΩÎ°ú ÏÑ§Ï†ï (ÎåÄÏã†, Î∞©Í∏à ÎßåÎì† ÎçîÎØ∏ ÌååÏùº ÏÇ¨Ïö©) ---\n","ORTHO_TIF_PATH = \"dummy_image.tif\"\n","BUILDING_SHP_PATH = \"dummy_buildings.shp\"\n","# ÏµúÏ¢Ö Í≤∞Í≥ºÎ¨ºÏù¥ Ï†ÄÏû•Îê† Ìè¥ÎçîÏùò Í≤ΩÎ°úÏûÖÎãàÎã§.\n","OUTPUT_DIR = \"dummy_dataset_output\"\n","# --------------------------------\n","\n","# ÏúÑÏóêÏÑú Ï†ïÏùòÌïú ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± Ìï®ÏàòÎ•º Ïã§ÌñâÌï©ÎãàÎã§.\n","# (ÏàòÏ†ï) Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÏó¨ Ïã§ÌñâÌï©ÎãàÎã§.\n","print(\"--- ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î°ú Ï†ÑÏ≤òÎ¶¨Î•º ÏãúÏûëÌï©ÎãàÎã§... ---\")\n","create_final_dataset_per_file(ORTHO_TIF_PATH, BUILDING_SHP_PATH, OUTPUT_DIR)\n","print(\"--- ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å. ---\")\n","\n","# (Ïù¥ÌõÑ ... (Ï∞∏Í≥†: Ïù¥ ÏΩîÎìúÎäî...) Î∂ÄÎ∂ÑÏùÄ Í∑∏ÎåÄÎ°ú Îë°ÎãàÎã§)\n","# ..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrHAVSHJQfLt","executionInfo":{"status":"ok","timestamp":1763207785026,"user_tz":-540,"elapsed":254,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}},"outputId":"0979e285-dd86-4826-e5b4-e40a244f280a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["--- ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î°ú Ï†ÑÏ≤òÎ¶¨Î•º ÏãúÏûëÌï©ÎãàÎã§... ---\n","Ï†ÑÏ≤¥ ÏòÅÏÉÅ ÌÅ¨Í∏∞: (1500, 1500), Ï°∞Ï†ïÎêú ÌÅ¨Í∏∞: (1024, 1024)\n","\n","--- ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± ÏôÑÎ£å ---\n","Ï¥ù 1Í∞úÏùò [Ïù¥ÎØ∏ÏßÄ ÌååÏùº]Í≥º [ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨ ÌååÏùº] ÏåçÏùÑ ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.\n","Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏúÑÏπò: dummy_dataset_output/images\n","ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨ Ï†ÄÏû• ÏúÑÏπò: dummy_dataset_output/labels_dict\n","--- ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å. ---\n"]}]},{"cell_type":"code","source":["# --- 3. üß† Î™®Îç∏ Ìó¨Ìçº Ìï®Ïàò Î∞è ÌÅ¥ÎûòÏä§ ---\n","\n","from typing import Optional\n","\n","# @title ShapeSpec (pixeldecoder_tem_fpn.ipynb)\n","class ShapeSpec:\n","    \"\"\"\n","    (backboneÍ≥º pixeldecoderÏóêÏÑú Î™®Îëê ÏÇ¨Ïö©)\n","    \"\"\"\n","    def __init__(self, channels=None, height=None, width=None, stride=None):\n","        self.channels = channels\n","        self.height = height\n","        self.width = width\n","        self.stride = stride\n","\n","    def __str__(self) -> str:\n","        return f\"ShapeSpec(C={self.channels}, H={self.height}, W={self.width}, S={self.stride})\"\n","\n","    __repr__ = __str__\n","\n","# @title _get_clones (pixeldecoder_tem_fpn.ipynb)\n","def _get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","# @title _get_activation_fn (pixeldecoder_tem_fpn.ipynb)\n","def _get_activation_fn(activation):\n","    \"\"\"Return an activation function given a string\"\"\"\n","    if activation == \"relu\":\n","        return F.relu\n","    if activation == \"gelu\":\n","        return F.gelu\n","    if activation == \"glu\":\n","        return F.glu\n","    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n","\n","# @title get_norm (pixeldecoder_tem_fpn.ipynb)\n","def get_norm(norm, out_channels):\n","    if norm is None:\n","        return None\n","    if isinstance(norm, str):\n","        if norm == \"\" or norm.lower() == \"none\":\n","            return None\n","        if norm == \"BN\":\n","            return nn.BatchNorm2d(out_channels)\n","        if norm == \"SyncBN\":\n","            return nn.SyncBatchNorm(out_channels)\n","        if norm == \"GN\":\n","            groups = 32 if out_channels % 32 == 0 else max(1, min(32, out_channels))\n","            return nn.GroupNorm(groups, out_channels)\n","        if norm == \"LN\":\n","            class _ChannelsFirstLayerNorm(nn.Module):\n","                def __init__(self, num_channels, eps=1e-6):\n","                    super().__init__()\n","                    self.weight = nn.Parameter(torch.ones(num_channels))\n","                    self.bias = nn.Parameter(torch.zeros(num_channels))\n","                    self.eps = eps\n","                def forward(self, x):\n","                    mean = x.mean(dim=1, keepdim=True)\n","                    var = (x - mean).pow(2).mean(dim=1, keepdim=True)\n","                    x = (x - mean) / torch.sqrt(var + self.eps)\n","                    return x * self.weight[:, None, None] + self.bias[:, None, None]\n","            return _ChannelsFirstLayerNorm(out_channels)\n","        raise ValueError(...)\n","    if callable(norm):\n","        return norm(out_channels)\n","    if isinstance(norm, nn.Module):\n","        return norm\n","    raise TypeError(...)\n","\n","# @title Conv2d (pixeldecoder_tem_fpn.ipynb)\n","class Conv2d(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size,\n","        stride=1,\n","        padding=0,\n","        dilation=1,\n","        groups=1,\n","        bias=True,\n","        norm=None,\n","        activation=None,\n","    ):\n","        super().__init__()\n","        self.conv = nn.Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            dilation=dilation,\n","            groups=groups,\n","            bias=bias if norm is None else False,\n","        )\n","        self.norm = norm\n","        self.activation = activation\n","\n","    @property\n","    def weight(self):\n","        return self.conv.weight\n","\n","    @property\n","    def bias(self):\n","        return self.conv.bias\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        if self.activation is not None:\n","            x = self.activation(x) if callable(self.activation) else self.activation(x)\n","        return x\n","\n","# @title configurable (pixeldecoder_tem_fpn.ipynb)\n","# fvcoreÏùò configurable Îç∞ÏΩîÎ†àÏù¥ÌÑ∞Î•º ÏúÑÌïú Ìó¨Ìçº Ìï®ÏàòÎì§\n","def configurable(init_func=None, *, from_config=None):\n","    if init_func is not None:\n","        assert (\n","            inspect.isfunction(init_func)\n","            and from_config is None\n","            and init_func.__name__ == \"__init__\"\n","        ), \"Incorrect use of @configurable. Check API documentation for examples.\"\n","\n","        @functools.wraps(init_func)\n","        def wrapped(self, *args, **kwargs):\n","            try:\n","                from_config_func = type(self).from_config\n","            except AttributeError as e:\n","                raise AttributeError(\n","                    \"Class with @configurable must have a 'from_config' classmethod.\"\n","                ) from e\n","            if not inspect.ismethod(from_config_func):\n","                raise TypeError(\"Class with @configurable must have a 'from_config' classmethod.\")\n","\n","            if _called_with_cfg(*args, **kwargs):\n","                explicit_args = _get_args_from_config(from_config_func, *args, **kwargs)\n","                init_func(self, **explicit_args)\n","            else:\n","                init_func(self, *args, **kwargs)\n","\n","        return wrapped\n","    else:\n","        if from_config is None:\n","            return configurable\n","        assert inspect.isfunction(\n","            from_config\n","        ), \"from_config argument of configurable must be a function!\"\n","\n","        def wrapper(orig_func):\n","            @functools.wraps(orig_func)\n","            def wrapped(*args, **kwargs):\n","                if _called_with_cfg(*args, **kwargs):\n","                    explicit_args = _get_args_from_config(from_config, *args, **kwargs)\n","                    return orig_func(**explicit_args)\n","                else:\n","                    return orig_func(*args, **kwargs)\n","\n","            wrapped.from_config = from_config\n","            return wrapped\n","\n","        return wrapper\n","\n","def _get_args_from_config(from_config_func, *args, **kwargs):\n","    signature = inspect.signature(from_config_func)\n","    if list(signature.parameters.keys())[0] != \"cfg\":\n","        if inspect.isfunction(from_config_func):\n","            name = from_config_func.__name__\n","        else:\n","            name = f\"{from_config_func.__self__}.from_config\"\n","        raise TypeError(f\"{name} must take 'cfg' as the first argument!\")\n","    support_var_arg = any(\n","        param.kind in [param.VAR_POSITIONAL, param.VAR_KEYWORD]\n","        for param in signature.parameters.values()\n","    )\n","    if support_var_arg:\n","        ret = from_config_func(*args, **kwargs)\n","    else:\n","        supported_arg_names = set(signature.parameters.keys())\n","        extra_kwargs = {}\n","        for name in list(kwargs.keys()):\n","            if name not in supported_arg_names:\n","                extra_kwargs[name] = kwargs.pop(name)\n","        ret = from_config_func(*args, **kwargs)\n","        ret.update(extra_kwargs)\n","    return ret\n","\n","\n","def _called_with_cfg(*args, **kwargs):\n","    # omegaconfÎäî ÏÑ§ÏπòÍ∞Ä ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.\n","    try:\n","        from omegaconf import DictConfig\n","    except ImportError:\n","        # ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏùÑ Í≤ΩÏö∞Î•º ÎåÄÎπÑÌïú ÏûÑÏãú ÌÅ¥ÎûòÏä§\n","        class DictConfig:\n","            pass\n","\n","    if len(args) and isinstance(args[0], (_CfgNode, DictConfig)):\n","        return True\n","    if isinstance(kwargs.pop(\"cfg\", None), (_CfgNode, DictConfig)):\n","        return True\n","    return False"],"metadata":{"id":"2yudHzGcm8jD","executionInfo":{"status":"ok","timestamp":1763207785049,"user_tz":-540,"elapsed":27,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 4. Î™®Îç∏ Ï†ïÏùò: Backbone (Swin Transformer)\n","# (backboneÏΩîÎìú.ipynb ÌååÏùºÏóêÏÑú Î≥µÏÇ¨)\n","\n","# Ïù¥ÎØ∏ÏßÄ Ï™ºÍ∞úÍ∏∞\n","def window_partition(x,window_size):\n","  B,H,W,C = x.shape\n","  x=x.view(B,H//window_size,window_size,W//window_size,window_size,C)\n","  windows=x.permute(0,1,3,2,4,5).contiguous().view(-1,window_size,window_size,C)\n","  return windows\n","\n","# Ïù¥ÎØ∏ÏßÄ Ìï©ÏπòÍ∏∞\n","def window_reverse(windows,window_size,H,W):\n","  B = int(windows.shape[0] / (H * W / window_size / window_size))\n","  x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","  x = x.permute(0,1,3,2,4,5).contiguous().view(B, H, W, -1)\n","  return x\n","\n","# Ïù¥ÎØ∏ÏßÄ -> ÌÜ†ÌÅ∞ÏúºÎ°ú Î≥ÄÌòï\n","class PatchEmbed(nn.Module):\n","    def __init__(self, patch_size=4, in_chans=3, embed_dim=96,norm_layer=None):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) #Ïù¥ÎØ∏ÏßÄ Î∂ÑÌï†\n","        self.norm = norm_layer\n","\n","    def forward(self, x):\n","        _, _, H, W = x.size()\n","        if W % self.patch_size != 0:\n","            x = F.pad(x, (0, self.patch_size - W % self.patch_size))\n","        if H % self.patch_size != 0:\n","            x = F.pad(x, (0, 0, 0, self.patch_size - H % self.patch_size))\n","        x = self.proj(x)\n","        return x\n","\n","# Ìï¥ÏÉÅÎèÑ Ï†àÎ∞ò, ÌîºÏ≤ò 2Î∞∞\n","class PatchMerging(nn.Module):\n","  def __init__(self,dim,norm_layer=nn.LayerNorm):\n","    super().__init__()\n","    self.dim = dim\n","    self.reduction = nn.Linear(4*dim,2*dim,bias=False)\n","    self.norm = norm_layer(4*dim)\n","\n","  def forward(self,x,H,W):\n","    B,L,C=x.shape\n","    assert L==H*W, \"Swin PatchMergingÌÅ¥ÎûòÏä§ HWÏò§Î•ò\"\n","    x=x.view(B,H,W,C)\n","    pad_input=(H%2==1)or(W%2==1)\n","    if pad_input:\n","      x=F.pad(x,(0,0,0,W%2,0,H%2))\n","\n","    x0 = x[:, 0::2, 0::2, :]\n","    x1 = x[:, 1::2, 0::2, :]\n","    x2 = x[:, 0::2, 1::2, :]\n","    x3 = x[:, 1::2, 1::2, :]\n","    x = torch.cat([x0, x1, x2, x3], -1)\n","    x = x.view(B, -1, 4 * C)\n","\n","    x = self.norm(x)\n","    x = self.reduction(x)\n","    return x\n","\n","# Multilayer perceptron (FFN)\n","class Mlp(nn.Module):\n","    def __init__(\n","        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","# ÏúàÎèÑÏö∞ Î©ÄÌã∞Ìó§Îìú ÏÖÄÌîÑÏñ¥ÌÖêÏÖò\n","class WindowAttention(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        window_size,\n","        num_heads,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        attn_drop=0.0,\n","        proj_drop=0.0,\n","    ):\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )\n","\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n","        coords_flatten = torch.flatten(coords, 1)\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","        relative_coords[:, :, 0] += self.window_size[0] - 1\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        B_, N, C = x.shape\n","        qkv = (\n","            self.qkv(x)\n","            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.view(-1)\n","        ].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )\n","        relative_position_bias = relative_position_bias.permute(\n","            2, 0, 1\n","        ).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","# Swin Transformer Î∏îÎ°ù\n","class SwinTransformerBlock(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        num_heads,\n","        window_size=7,\n","        shift_size=0,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        drop=0.0,\n","        attn_drop=0.0,\n","        drop_path=0.0,\n","        act_layer=nn.GELU,\n","        norm_layer=nn.LayerNorm,\n","    ):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=to_2tuple(self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            qk_scale=qk_scale,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(\n","            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop\n","        )\n","\n","        self.H = None\n","        self.W = None\n","\n","    def forward(self, x, mask_matrix):\n","        B, L, C = x.shape\n","        H, W = self.H, self.W\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        pad_l = pad_t = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n","\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if pad_r > 0 or pad_b > 0:\n","            x = x[:, :H, :W, :].contiguous()\n","\n","        x = x.view(B, H * W, C)\n","\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# Stage: BasicLayer\n","class BasicLayer(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        depth,\n","        num_heads,\n","        window_size=7,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        drop=0.0,\n","        attn_drop=0.0,\n","        drop_path=0.0,\n","        norm_layer=nn.LayerNorm,\n","        downsample=None,\n","        use_checkpoint=False,\n","    ):\n","        super().__init__()\n","        self.window_size = window_size\n","        self.shift_size = window_size // 2\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                SwinTransformerBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    window_size=window_size,\n","                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    qk_scale=qk_scale,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                    norm_layer=norm_layer,\n","                )\n","                for i in range(depth)\n","            ]\n","        )\n","\n","        if downsample is not None:\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def forward(self, x, H, W):\n","        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n","        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n","        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)\n","        h_slices = (\n","            slice(0, -self.window_size),\n","            slice(-self.window_size, -self.shift_size),\n","            slice(-self.shift_size, None),\n","        )\n","        w_slices = (\n","            slice(0, -self.window_size),\n","            slice(-self.window_size, -self.shift_size),\n","            slice(-self.shift_size, None),\n","        )\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","        for blk in self.blocks:\n","            blk.H, blk.W = H, W\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x, attn_mask)\n","            else:\n","                x = blk(x, attn_mask)\n","        if self.downsample is not None:\n","            x_down = self.downsample(x, H, W)\n","            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n","            return x, H, W, x_down, Wh, Ww\n","        else:\n","            return x, H, W, x, H, W\n","\n","# SwinTransformer\n","class SwinTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        pretrain_img_size=224,\n","        patch_size=4,\n","        in_chans=3,\n","        embed_dim=96,\n","        depths=[2, 2, 6, 2],\n","        num_heads=[3, 6, 12, 24],\n","        window_size=7,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        qk_scale=None,\n","        drop_rate=0.0,\n","        attn_drop_rate=0.0,\n","        drop_path_rate=0.2,\n","        norm_layer=nn.LayerNorm,\n","        ape=False,\n","        patch_norm=True,\n","        out_indices=(0, 1, 2, 3),\n","        frozen_stages=-1,\n","        use_checkpoint=False,\n","    ):\n","        super().__init__()\n","\n","        self.pretrain_img_size = pretrain_img_size\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.ape = ape\n","        self.patch_norm = patch_norm\n","        self.out_indices = out_indices\n","        self.frozen_stages = frozen_stages\n","\n","        self.patch_embed = PatchEmbed(\n","            patch_size=patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None,\n","        )\n","\n","        if self.ape:\n","            pretrain_img_size = to_2tuple(pretrain_img_size)\n","            patch_size = to_2tuple(patch_size)\n","            patches_resolution = [\n","                pretrain_img_size[0] // patch_size[0],\n","                pretrain_img_size[1] // patch_size[1],\n","            ]\n","            self.absolute_pos_embed = nn.Parameter(\n","                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1])\n","            )\n","            trunc_normal_(self.absolute_pos_embed, std=0.02)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2 ** i_layer),\n","                depth=depths[i_layer],\n","                num_heads=num_heads[i_layer],\n","                window_size=window_size,\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                qk_scale=qk_scale,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n","                norm_layer=norm_layer,\n","                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n","                use_checkpoint=use_checkpoint,\n","            )\n","            self.layers.append(layer)\n","\n","        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n","        self.num_features = num_features\n","\n","        for i_layer in out_indices:\n","            layer = norm_layer(num_features[i_layer])\n","            layer_name = f\"norm{i_layer}\"\n","            self.add_module(layer_name, layer)\n","\n","        self._freeze_stages()\n","\n","    def _freeze_stages(self):\n","        if self.frozen_stages >= 0:\n","            self.patch_embed.eval()\n","            for param in self.patch_embed.parameters():\n","                param.requires_grad = False\n","        if self.frozen_stages >= 1 and self.ape:\n","            self.absolute_pos_embed.requires_grad = False\n","        if self.frozen_stages >= 2:\n","            self.pos_drop.eval()\n","            for i in range(0, self.frozen_stages - 1):\n","                m = self.layers[i]\n","                m.eval()\n","                for param in m.parameters():\n","                    param.requires_grad = False\n","\n","    def init_weights(self, pretrained=None):\n","        def _init_weights(m):\n","            if isinstance(m, nn.Linear):\n","                trunc_normal_(m.weight, std=0.02)\n","                if isinstance(m, nn.Linear) and m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.LayerNorm):\n","                nn.init.constant_(m.bias, 0)\n","                nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","        Wh, Ww = x.size(2), x.size(3)\n","        if self.ape:\n","            absolute_pos_embed = F.interpolate(\n","                self.absolute_pos_embed, size=(Wh, Ww), mode=\"bicubic\"\n","            )\n","            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)\n","        else:\n","            x = x.flatten(2).transpose(1, 2)\n","        x = self.pos_drop(x)\n","\n","        outs = {}\n","        for i in range(self.num_layers):\n","            layer = self.layers[i]\n","            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n","\n","            if i in self.out_indices:\n","                norm_layer = getattr(self, f\"norm{i}\")\n","                x_out = norm_layer(x_out)\n","                out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n","                outs[\"res{}\".format(i + 2)] = out\n","\n","        return outs\n","\n","    def train(self, mode=True):\n","        super(SwinTransformer, self).train(mode)\n","        self._freeze_stages()\n","\n","# Î∏åÎ¶øÏßÄ ÌÅ¥ÎûòÏä§ (BackboneÏö©)\n","class D2SwinTransformer(SwinTransformer):\n","    @configurable\n","    def __init__(self, *, input_shape, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        # cfg ÎåÄÏã† input_shapeÏùÑ ÏßÅÏ†ë Î∞õÏïÑ ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏàòÏ†ï\n","        # self._out_features = cfg.MODEL.SWIN.OUT_FEATURES # ÏõêÎ≥∏\n","        self._out_features = [\"res2\", \"res3\", \"res4\", \"res5\"] # ÏûÑÏãú ÌïòÎìúÏΩîÎî© (cfgÏóêÏÑú Î∞õÏïÑÏò§ÎèÑÎ°ù ÏàòÏ†ï ÌïÑÏöî)\n","\n","        self._out_feature_strides = {\n","            \"res2\": 4,\n","            \"res3\": 8,\n","            \"res4\": 16,\n","            \"res5\": 32,\n","        }\n","        self._out_feature_channels = {\n","            \"res2\": self.num_features[0],\n","            \"res3\": self.num_features[1],\n","            \"res4\": self.num_features[2],\n","            \"res5\": self.num_features[3],\n","        }\n","\n","    @classmethod\n","    def from_config(cls, cfg, input_shape):\n","        # Ïù¥ Î∂ÄÎ∂ÑÏùÄ cfg Í∞ùÏ≤¥Î•º Ïã§Ï†úÎ°ú ÏÇ¨Ïö©Ìï† Îïå Ï±ÑÏõåÏïº Ìï©ÎãàÎã§.\n","        # backboneÏΩîÎìú.ipynbÏùò Í∏∞Î≥∏Í∞íÏùÑ ÏÇ¨Ïö©ÌïòÎèÑÎ°ù kwargsÎ•º Íµ¨ÏÑ±Ìï©ÎãàÎã§.\n","        kwargs = {\n","            \"pretrain_img_size\": cfg.MODEL.SWIN.PRETRAIN_IMG_SIZE,\n","            \"patch_size\": cfg.MODEL.SWIN.PATCH_SIZE,\n","            \"in_chans\": input_shape.channels, # cfg.MODEL.SWIN.IN_CHANS ÎåÄÏã† input_shape ÏÇ¨Ïö©\n","            \"embed_dim\": cfg.MODEL.SWIN.EMBED_DIM,\n","            \"depths\": cfg.MODEL.SWIN.DEPTHS,\n","            \"num_heads\": cfg.MODEL.SWIN.NUM_HEADS,\n","            \"window_size\": cfg.MODEL.SWIN.WINDOW_SIZE,\n","            \"mlp_ratio\": cfg.MODEL.SWIN.MLP_RATIO,\n","            \"qkv_bias\": cfg.MODEL.SWIN.QKV_BIAS,\n","            \"qk_scale\": cfg.MODEL.SWIN.QK_SCALE,\n","            \"drop_rate\": cfg.MODEL.SWIN.DROP_RATE,\n","            \"attn_drop_rate\": cfg.MODEL.SWIN.ATTN_DROP_RATE,\n","            \"drop_path_rate\": cfg.MODEL.SWIN.DROP_PATH_RATE,\n","            \"norm_layer\": nn.LayerNorm, # cfg.MODEL.SWIN.NORM_LAYER Í∞íÏóê Îî∞Îùº ÏàòÏ†ï\n","            \"ape\": cfg.MODEL.SWIN.APE,\n","            \"patch_norm\": cfg.MODEL.SWIN.PATCH_NORM,\n","            \"use_checkpoint\": cfg.MODEL.SWIN.USE_CHECKPOINT,\n","            \"out_indices\": cfg.MODEL.SWIN.OUT_INDICES\n","        }\n","        return {\"input_shape\": input_shape, **kwargs}\n","\n","\n","    def forward(self, x):\n","        assert (\n","            x.dim() == 4\n","        ), f\"ÏûÖÎ†•ÌÖêÏÑúÍ∞Ä (N, C, H, W).Í∞Ä ÏïÑÎãå {x.shape}\"\n","        outputs = {}\n","        y = super().forward(x)\n","        for k in y.keys():\n","            if k in self._out_features:\n","                outputs[k] = y[k]\n","        return outputs\n","\n","    def output_shape(self):\n","        return {\n","            name: ShapeSpec(\n","                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n","            )\n","            for name in self._out_features\n","        }\n","\n","    @property\n","    def size_divisibility(self):\n","        return 32"],"metadata":{"id":"Qaa1edXZm8lq","executionInfo":{"status":"ok","timestamp":1763207785116,"user_tz":-540,"elapsed":66,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# --- 5. üß† Î™®Îç∏: 2Î∂Ä (Pixel Decoder) ---\n","# (pixeldecoder_tem_fpn...ipynb ÌååÏùºÏóêÏÑú Î≥µÏÇ¨)\n","\n","# @title BasePixelDecoder (pixeldecoder_tem_fpn.ipynb)\n","# BasePixelDecoderÎäî FPN Ïä§ÌÉÄÏùºÏù¥Î©∞, TransformerEncoderPixelDecoderÏùò Î∂ÄÎ™® ÌÅ¥ÎûòÏä§ÏûÖÎãàÎã§.\n","class BasePixelDecoder(nn.Module):\n","    @configurable\n","    def __init__(\n","        self,\n","        input_shape: Dict[str, ShapeSpec],\n","        *,\n","        conv_dim: int,\n","        mask_dim: int,\n","        norm: Optional[Union[str, Callable]] = None,\n","    ):\n","        \"\"\"\n","        NOTE: this interface is experimental.\n","        Args:\n","            input_shape: shapes (channels and stride) of the input features\n","            conv_dims: number of output channels for the intermediate conv layers.\n","            mask_dim: number of output channels for the final conv layer.\n","            norm (str or callable): normalization for all conv layers\n","        \"\"\"\n","        super().__init__()\n","\n","        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n","        self.in_features = [k for k, v in input_shape]  # res2 ~ res5\n","        feature_channels = [v.channels for k, v in input_shape]\n","\n","        lateral_convs = []\n","        output_convs = []\n","\n","        use_bias = norm == \"\"\n","        for idx, in_channels in enumerate(feature_channels):\n","            if idx == len(self.in_features) - 1: # ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥ (res5)\n","                output_norm = get_norm(norm, conv_dim)\n","                output_conv = Conv2d(\n","                    in_channels,\n","                    conv_dim,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=1,\n","                    bias=use_bias,\n","                    norm=output_norm,\n","                    activation=F.relu,\n","                )\n","                weight_init.c2_xavier_fill(output_conv)\n","                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n","\n","                lateral_convs.append(None)\n","                output_convs.append(output_conv)\n","            else: # res2, res3, res4\n","                lateral_norm = get_norm(norm, conv_dim)\n","                output_norm = get_norm(norm, conv_dim)\n","\n","                lateral_conv = Conv2d(\n","                    in_channels, conv_dim, kernel_size=1, bias=use_bias, norm=lateral_norm\n","                )\n","                output_conv = Conv2d(\n","                    conv_dim,\n","                    conv_dim,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=1,\n","                    bias=use_bias,\n","                    norm=output_norm,\n","                    activation=F.relu,\n","                )\n","                weight_init.c2_xavier_fill(lateral_conv)\n","                weight_init.c2_xavier_fill(output_conv)\n","                self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n","                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n","\n","                lateral_convs.append(lateral_conv)\n","                output_convs.append(output_conv)\n","\n","        self.lateral_convs = lateral_convs[::-1]\n","        self.output_convs = output_convs[::-1]\n","\n","        self.mask_dim = mask_dim\n","        self.mask_features = nn.Conv2d(\n","            conv_dim,\n","            mask_dim,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","        )\n","        weight_init.c2_xavier_fill(self.mask_features)\n","\n","        self.maskformer_num_feature_levels = 3  # 3Í∞úÏùò Ïä§ÏºÄÏùº ÏÇ¨Ïö©\n","\n","    @classmethod\n","    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n","        ret = {}\n","        # cfgÍ∞Ä ÏóÜÏúºÎ©¥ AttributeErrorÍ∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÏúºÎØÄÎ°ú try-except Ï∂îÍ∞Ä (ÎºàÎåÄÏö©)\n","        try:\n","            ret[\"input_shape\"] = {\n","                k: v for k, v in input_shape.items() if k in cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES\n","            }\n","            ret[\"conv_dim\"] = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","            ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","            ret[\"norm\"] = cfg.MODEL.SEM_SEG_HEAD.NORM\n","        except AttributeError:\n","             print(\"Í≤ΩÍ≥†: cfg Í∞ùÏ≤¥Í∞Ä ÏóÜÍ±∞ÎÇò Íµ¨Ï°∞Í∞Ä Îã§Î¶ÖÎãàÎã§. BasePixelDecoder Í∏∞Î≥∏Í∞íÏúºÎ°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.\")\n","             ret[\"input_shape\"] = {k: v for k, v in input_shape.items() if k in [\"res2\", \"res3\", \"res4\", \"res5\"]}\n","             ret[\"conv_dim\"] = 256\n","             ret[\"mask_dim\"] = 256\n","             ret[\"norm\"] = \"GN\"\n","        return ret\n","\n","    def forward_features(self, features):\n","        multi_scale_features = []\n","        num_cur_levels = 0\n","        y = None # Top-down ÏãúÏûëÏùÑ ÏúÑÌï¥ Ï¥àÍ∏∞Ìôî\n","\n","        # Reverse feature maps into top-down order (from low to high resolution)\n","        for idx, f in enumerate(self.in_features[::-1]):\n","            x = features[f]\n","            lateral_conv = self.lateral_convs[idx]\n","            output_conv = self.output_convs[idx]\n","            if lateral_conv is None:\n","                y = output_conv(x)\n","            else:\n","                if y is None:\n","                    raise ValueError(\"y (top-down feature) is not initialized in BasePixelDecoder.\")\n","\n","                cur_fpn = lateral_conv(x)\n","                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n","                y = output_conv(y)\n","\n","            if num_cur_levels < self.maskformer_num_feature_levels:\n","                multi_scale_features.append(y)\n","                num_cur_levels += 1\n","\n","        return self.mask_features(y), None, multi_scale_features\n","\n","    def forward(self, features, targets=None):\n","        # logger = logging.getLogger(__name__)\n","        # logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n","        return self.forward_features(features)\n","\n","\n","# @title TransformerEncoderLayer (pixeldecoder_tem_fpn.ipynb)\n","# Transformer Encoder Layer\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(\n","        self,\n","        d_model,\n","        nhead,\n","        dim_feedforward=2048,\n","        dropout=0.1,\n","        activation=\"relu\",\n","        normalize_before=False,\n","    ):\n","        super().__init__()\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.activation = _get_activation_fn(activation)\n","        self.normalize_before = normalize_before\n","\n","    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n","        return tensor if pos is None else tensor + pos\n","\n","    def forward_post(\n","        self,\n","        src,\n","        src_mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        q = k = self.with_pos_embed(src, pos)\n","        src2 = self.self_attn(\n","            q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n","        )[0]\n","        src = src + self.dropout1(src2)\n","        src = self.norm1(src)\n","        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n","        src = src + self.dropout2(src2)\n","        src = self.norm2(src)\n","        return src\n","\n","    def forward_pre(\n","        self,\n","        src,\n","        src_mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        src2 = self.norm1(src)\n","        q = k = self.with_pos_embed(src2, pos)\n","        src2 = self.self_attn(\n","            q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n","        )[0]\n","        src = src + self.dropout1(src2)\n","        src2 = self.norm2(src)\n","        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n","        src = src + self.dropout2(src2)\n","        return src\n","\n","    def forward(\n","        self,\n","        src,\n","        src_mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        if self.normalize_before:\n","            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n","        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n","\n","# @title TransformerEncoder (pixeldecoder_tem_fpn.ipynb)\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers, norm=None):\n","        super().__init__()\n","        self.layers = _get_clones(encoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","\n","    def forward(\n","        self,\n","        src,\n","        mask: Optional[Tensor] = None,\n","        src_key_padding_mask: Optional[Tensor] = None,\n","        pos: Optional[Tensor] = None,\n","    ):\n","        output = src\n","\n","        for layer in self.layers:\n","            output = layer(\n","                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos\n","            )\n","\n","        if self.norm is not None:\n","            output = self.norm(output)\n","\n","        return output\n","\n","# @title TransformerEncoderOnly (pixeldecoder_tem_fpn.ipynb)\n","class TransformerEncoderOnly(nn.Module):\n","    def __init__(\n","        self,\n","        d_model=512,\n","        nhead=8,\n","        num_encoder_layers=6,\n","        dim_feedforward=2048,\n","        dropout=0.1,\n","        activation=\"relu\",\n","        normalize_before=False,\n","    ):\n","        super().__init__()\n","\n","        encoder_layer = TransformerEncoderLayer(\n","            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n","        )\n","        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n","        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n","\n","        self._reset_parameters()\n","\n","        self.d_model = d_model\n","        self.nhead = nhead\n","\n","    def _reset_parameters(self):\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","    def forward(self, src, mask, pos_embed):\n","        # flatten NxCxHxW to HWxNxC\n","        bs, c, h, w = src.shape\n","        src = src.flatten(2).permute(2, 0, 1)\n","        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n","        if mask is not None:\n","            mask = mask.flatten(1)\n","\n","        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n","        return memory.permute(1, 2, 0).view(bs, c, h, w)\n","\n","# @title TransformerEncoderPixelDecoder (pixeldecoder_tem_fpn.ipynb)\n","class TransformerEncoderPixelDecoder(BasePixelDecoder):\n","    @configurable\n","    def __init__(\n","        self,\n","        input_shape: Dict[str, ShapeSpec],\n","        *,\n","        transformer_dropout: float,\n","        transformer_nheads: int,\n","        transformer_dim_feedforward: int,\n","        transformer_enc_layers: int,\n","        transformer_pre_norm: bool,\n","        conv_dim: int,\n","        mask_dim: int,\n","        norm: Optional[Union[str, Callable]] = None,\n","    ):\n","        \"\"\"\n","        NOTE: this interface is experimental.\n","        Args:\n","            input_shape: shapes (channels and stride) of the input features\n","            transformer_dropout: dropout probability in transformer\n","            transformer_nheads: number of heads in transformer\n","            transformer_dim_feedforward: dimension of feedforward network\n","            transformer_enc_layers: number of transformer encoder layers\n","            transformer_pre_norm: whether to use pre-layernorm or not\n","            conv_dims: number of output channels for the intermediate conv layers.\n","            mask_dim: number of output channels for the final conv layer.\n","            norm (str or callable): normalization for all conv layers\n","        \"\"\"\n","        super().__init__(input_shape, conv_dim=conv_dim, mask_dim=mask_dim, norm=norm)\n","\n","        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n","        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n","        feature_strides = [v.stride for k, v in input_shape]\n","        feature_channels = [v.channels for k, v in input_shape]\n","\n","        in_channels = feature_channels[len(self.in_features) - 1]\n","        self.input_proj = Conv2d(in_channels, conv_dim, kernel_size=1)\n","        weight_init.c2_xavier_fill(self.input_proj)\n","        self.transformer = TransformerEncoderOnly(\n","            d_model=conv_dim,\n","            dropout=transformer_dropout,\n","            nhead=transformer_nheads,\n","            dim_feedforward=transformer_dim_feedforward,\n","            num_encoder_layers=transformer_enc_layers,\n","            normalize_before=transformer_pre_norm,\n","        )\n","        # N_steps = conv_dim // 2 # Original incorrect line\n","        # --- Fix: Initialize pixel_position_embedding with conv_dim ---\n","        self.pe_layer = pixel_position_embedding(conv_dim)\n","        # --- End Fix ---\n","\n","        # update layer\n","        use_bias = norm == \"\"\n","        output_norm = get_norm(norm, conv_dim)\n","        output_conv = Conv2d(\n","            conv_dim,\n","            conv_dim,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=use_bias,\n","            norm=output_norm,\n","            activation=F.relu,\n","        )\n","        weight_init.c2_xavier_fill(output_conv)\n","        delattr(self, \"layer_{}\".format(len(self.in_features)))\n","        self.add_module(\"layer_{}\".format(len(self.in_features)), output_conv)\n","        self.output_convs[0] = output_conv\n","\n","    @classmethod\n","    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n","        ret = super().from_config(cfg, input_shape)\n","\n","        # (cfgÍ∞Ä ÏóÜÏúºÎ©¥ AttributeError Î∞úÏÉù)\n","        try:\n","            ret[\"transformer_dropout\"] = cfg.MODEL.MASK_FORMER.DROPOUT\n","            ret[\"transformer_nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n","            ret[\"transformer_dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n","            ret[\n","                \"transformer_enc_layers\"\n","            ] = cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS  # a separate config\n","            ret[\"transformer_pre_norm\"] = cfg.MODEL.MASK_FORMER.PRE_NORM\n","        except AttributeError:\n","             print(\"Í≤ΩÍ≥†: cfg Í∞ùÏ≤¥Í∞Ä ÏóÜÍ±∞ÎÇò Íµ¨Ï°∞Í∞Ä Îã§Î¶ÖÎãàÎã§. TransformerEncoderPixelDecoder Í∏∞Î≥∏Í∞íÏúºÎ°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.\")\n","             ret[\"transformer_dropout\"] = 0.1\n","             ret[\"transformer_nheads\"] = 8\n","             ret[\"transformer_dim_feedforward\"] = 1024 # Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ Í∞í\n","             ret[\"transformer_enc_layers\"] = 3 # Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ Í∞í\n","             ret[\"transformer_pre_norm\"] = False\n","\n","        return ret\n","\n","    def forward_features(self, features):\n","        multi_scale_features = []\n","        num_cur_levels = 0\n","        y = None # yÎ•º NoneÏúºÎ°ú Ï¥àÍ∏∞Ìôî (ÏõêÎ≥∏ ÏΩîÎìú)\n","\n","        # Reverse feature maps into top-down order (from low to high resolution)\n","        for idx, f in enumerate(self.in_features[::-1]):\n","            x = features[f]\n","            lateral_conv = self.lateral_convs[idx]\n","            output_conv = self.output_convs[idx]\n","            if lateral_conv is None:\n","                transformer = self.input_proj(x)\n","                pos = self.pe_layer(x)\n","                transformer = self.transformer(transformer, None, pos)\n","                y = output_conv(transformer)\n","                # save intermediate feature as input to Transformer decoder\n","                transformer_encoder_features = transformer\n","            else:\n","                cur_fpn = lateral_conv(x)\n","                # Following FPN implementation, we use nearest upsampling here\n","                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n","                y = output_conv(y)\n","            if num_cur_levels < self.maskformer_num_feature_levels:\n","                multi_scale_features.append(y)\n","                num_cur_levels += 1\n","        return self.mask_features(y), transformer_encoder_features, multi_scale_features\n","\n","    def forward(self, features, targets=None):\n","        # logger = logging.getLogger(__name__)\n","        # logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n","        return self.forward_features(features)"],"metadata":{"id":"Fv7psmhZ6Dpd","executionInfo":{"status":"ok","timestamp":1763207785187,"user_tz":-540,"elapsed":62,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# --- 6. üß† Î™®Îç∏: 3Î∂Ä (Transformer Decoder) ---\n","# (TransformDecoder.py ÌååÏùºÏóêÏÑú Î≥µÏÇ¨)\n","\n","# 1Îã®Í≥Ñ : masked attention + add & norm\n","class masked_attention(nn.Module):\n","\n","    #c_dim: feature dimension = query dimension\n","    #n_head: Î©ÄÌã∞Ìï¥Îçî Ïàò\n","    #ÎÖºÎ¨∏ÏóêÏÑú dropoutÏù¥ 0Ïùº ÎïåÍ∞Ä ÌçºÌè¨Î®ºÏä§Í∞Ä Í∞ÄÏû• Ï¢ãÏïòÎã§Í≥† Ìï¥ÏÑú defaultÎ•º 0ÏúºÎ°ú ÏÑ§Ï†ï\n","    def __init__(self, C_dim, n_head, dropout=0.0):\n","\n","        super(masked_attention, self).__init__()\n","        self.multihead_attn = nn.MultiheadAttention(C_dim, n_head, dropout=dropout, batch_first=True)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.norm = nn.LayerNorm(C_dim)\n","\n","        #parameter Ï¥àÍ∏∞Ìôî, xavier_uniform_ ÏÇ¨Ïö©\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for par in self.parameters():\n","            if par.dim() > 1:\n","                nn.init.xavier_uniform_(par)\n","\n","    # queryÏôÄ keyÏóê ÏúÑÏπò ÏûÑÎ≤†Îî© ÎçîÌïòÍ∏∞ ÏúÑÌï¥ Íµ¨ÌòÑ\n","    def add_pos_embedding(self, x: Tensor, pos: Tensor) -> Tensor:\n","        return x if pos is None else x + pos\n","\n","    # x_last: Í∑∏Ï†Ñ Îã®Í≥ÑÏùò Ï∂úÎ†•Í∞í\n","    # img_feat: Ïù¥ÎØ∏ÏßÄ feature map(C_dimÏúºÎ°ú Ï∞®ÏõêÎ≥ÄÌôò Îêú)\n","    # mask: ÎßàÏä§ÌÅ¨ ÌÖêÏÑú / ÌÇ§ ÎßàÏä§ÌÅ¨Îäî ÏóÜÎäî Í≤ÉÏúºÎ°ú ÏÑ§Ï†ï -> inputÏúºÎ°ú ÎÑ£Í∏∞ Ï†ÑÏóê Ï†ÑÏ≤òÎ¶¨ Îî∞Î°ú Ìï† ÏòàÏ†ï\n","    # query_pos: ÏøºÎ¶¨ Ìè¨ÏßÄÏÖò ÏûÑÎ≤†Îî© (ÏøºÎ¶¨Í∞Ä Ïñ¥Îñ§ ÏúÑÏπòÎ•º ÎÇòÌÉÄÎÇ¥ÎäîÏßÄ ÏïåÎ†§Ï£ºÎäî ÏûÑÎ≤†Îî©)\n","    # pixel_pos: ÌîΩÏÖÄ Ìè¨ÏßÄÏÖò ÏûÑÎ≤†Îî© (Ïù¥ÎØ∏ÏßÄ ÌîΩÏÖÄÏùò ÏúÑÏπò Ï†ïÎ≥¥Î•º Îã¥ÏùÄ ÏûÑÎ≤†Îî©)\n","    def forward(self, x_last, img_feat: Tensor,\n","                mask: Tensor,\n","                query_pos: Tensor, pixel_pos: Tensor\n","                ):\n","        # masked attention\n","        mask_attened = self.multihead_attn(query = self.add_pos_embedding(x_last, query_pos),\n","                                        key = self.add_pos_embedding(img_feat, pixel_pos),\n","                                        value = img_feat,\n","                                        attn_mask = mask,\n","                                        key_padding_mask = None\n","                                        )[0]\n","        # add & norm\n","        # ÎÖºÎ¨∏Ïóê ÎÇòÏò§Îäî add&norm Î∂ÄÎ∂ÑÏùÑ masked attention Î™®Îìà ÏïàÏóê Íµ¨ÌòÑ\n","        # add: x + f(x) / norm : Ï†ïÍ∑úÌôî(LayerNorm)\n","        x_next = x_last + self.dropout(mask_attened)\n","        x_next = self.norm(x_next)\n","\n","        return x_next\n","\n","# 2Îã®Í≥Ñ : self attention + add & norm\n","class self_attention(nn.Module):\n","    #c_dim: feature dimension = query dimension\n","    #n_head: Î©ÄÌã∞Ìï¥Îçî Ïàò\n","    #ÎÖºÎ¨∏ÏóêÏÑú dropoutÏù¥ 0Ïùº ÎïåÍ∞Ä ÌçºÌè¨Î®ºÏä§Í∞Ä Í∞ÄÏû• Ï¢ãÏïòÎã§Í≥† Ìï¥ÏÑú defaultÎ•º 0ÏúºÎ°ú ÏÑ§Ï†ï\n","    def __init__(self, C_dim, n_head, dropout=0.0):\n","        super(self_attention, self).__init__()\n","        self.self_attn = nn.MultiheadAttention(C_dim, n_head, dropout=dropout, batch_first=True)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.norm = nn.LayerNorm(C_dim)\n","\n","        self.reset_parameters()\n","\n","    #parameter Ï¥àÍ∏∞Ìôî, xavier_uniform_ ÏÇ¨Ïö©\n","    def reset_parameters(self):\n","        for par in self.parameters():\n","            if par.dim() > 1:\n","                nn.init.xavier_uniform_(par)\n","\n","    # queryÏôÄ keyÏóê ÏúÑÏπò ÏûÑÎ≤†Îî© ÎçîÌïòÍ∏∞ ÏúÑÌï¥ Íµ¨ÌòÑ\n","    def add_pos_embedding(self, x: Tensor, pos: Tensor) -> Tensor:\n","        return x if pos is None else x + pos\n","\n","    # x_last: Í∑∏Ï†Ñ Îã®Í≥ÑÏùò Ï∂úÎ†•Í∞í\n","    # mask: ÎßàÏä§ÌÅ¨ ÌÖêÏÑú / ÌÇ§ ÎßàÏä§ÌÅ¨Îäî ÏóÜÎäî Í≤ÉÏúºÎ°ú ÏÑ§Ï†ï(inputÏúºÎ°ú ÎÑ£Í∏∞ Ï†ÑÏóê Ï†ÑÏ≤òÎ¶¨ Îî∞Î°ú Ìï† ÏòàÏ†ï)\n","    # query_pos: ÏøºÎ¶¨ Ìè¨ÏßÄÏÖò ÏûÑÎ≤†Îî© (ÏøºÎ¶¨Í∞Ä Ïñ¥Îñ§ ÏúÑÏπòÎ•º ÎÇòÌÉÄÎÇ¥ÎäîÏßÄ ÏïåÎ†§Ï£ºÎäî ÏûÑÎ≤†Îî©)\n","    # self attentionÏù¥Í∏∞ ÎïåÎ¨∏Ïóê key = value\n","    def forward(self, x_last, mask = None, query_pos =None):\n","        # self attention\n","        Query_and_Key = self.add_pos_embedding(x_last, query_pos)\n","\n","        self_attened = self.self_attn(\n","            query = Query_and_Key,\n","            key = Query_and_Key,\n","            value = x_last,\n","            attn_mask = mask,\n","            key_padding_mask = None\n","            )[0]\n","\n","        # add & norm\n","        # ÎÖºÎ¨∏Ïóê ÎÇòÏò§Îäî add&norm Î∂ÄÎ∂ÑÏùÑ self attention Î™®Îìà ÏïàÏóê Íµ¨ÌòÑ\n","        # add: x + f(x) / norm : Ï†ïÍ∑úÌôî(LayerNorm)\n","        x_next = x_last + self.dropout(self_attened)\n","        x_next = self.norm(x_next)\n","\n","        return x_next\n","\n","# 3Îã®Í≥Ñ : FFN + add & norm\n","class FFN(nn.Module):\n","    #C_dim: feature dimension\n","    #dim_feedforward: FFN ÎÇ¥Î∂ÄÏùò hidden layer Ï∞®Ïõê\n","    #ÎÖºÎ¨∏ÏóêÏÑú dropoutÏù¥ 0Ïùº ÎïåÍ∞Ä ÌçºÌè¨Î®ºÏä§Í∞Ä Í∞ÄÏû• Ï¢ãÏïòÎã§Í≥† Ìï¥ÏÑú defaultÎ•º 0ÏúºÎ°ú ÏÑ§Ï†ï\n","    def __init__(self, C_dim, dim_feedforward=2048, dropout=0.0):\n","        super(FFN, self).__init__()\n","\n","        self.first_linear = nn.Linear(C_dim, dim_feedforward)\n","        self.second_linear = nn.Linear(dim_feedforward, C_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.activation = F.relu\n","\n","        self.norm = nn.LayerNorm(C_dim)\n","        #parameter Ï¥àÍ∏∞Ìôî, xavier_uniform_ ÏÇ¨Ïö©\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for par in self.parameters():\n","            if par.dim() > 1:\n","                nn.init.xavier_uniform_(par)\n","\n","    def forward(self, x_last):\n","        # FFN\n","        # FFN = relu(w1x + b1)W2 + b2\n","        ffn = self.second_linear(self.dropout(self.activation(self.first_linear(x_last))))\n","\n","        # add & norm\n","        # ÎÖºÎ¨∏Ïóê ÎÇòÏò§Îäî add&norm Î∂ÄÎ∂ÑÏùÑ FFN Î™®Îìà ÏïàÏóê Íµ¨ÌòÑ\n","        # add: x + f(x) / norm : Ï†ïÍ∑úÌôî(LayerNorm)\n","        x_next = x_last + self.dropout(ffn)\n","        x_next = self.norm(x_next)\n","\n","        return x_next\n","\n","# ÎßàÏä§ÌÇπ ÏúÑÌï¥ Multi-Layer Perceptron(MLP) Íµ¨ÌòÑ\n","class mlp(nn.Module):\n","    def __init__(self, n_layers, input_dim, hidden_dim, output_dim):\n","        super(mlp, self).__init__()\n","\n","        self.n_layers = n_layers\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.layers = self.make_layers()\n","        self.last_layer = nn.Linear(hidden_dim, output_dim)\n","\n","    def make_layers(self):\n","        mlp_layers = [nn.Linear(self.input_dim, self.hidden_dim)]\n","\n","        for _ in range(self.n_layers-2):\n","            mlp_layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n","\n","        return nn.ModuleList(mlp_layers)\n","\n","    def forward(self, input):\n","        for layer in self.layers:\n","            input = F.relu(layer(input))\n","\n","        output = self.last_layer(input)\n","        return output\n","\n","# pixel position embeddingÏùÑ ÌïòÍ∏∞ ÏúÑÌïú Î™®Îç∏ Íµ¨ÌòÑ\n","# sinusoidal position embedding ÏÇ¨Ïö©\n","class pixel_position_embedding(nn.Module):\n","    def __init__(self, d_model):\n","        super(pixel_position_embedding, self).__init__()\n","        self.d_model = d_model\n","\n","    #feature_map : (B, C, H, W)\n","    def forward(self, feature_map: Tensor):\n","        #sinusoidalÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† [H | W] Í∞ôÏù¥ HÏôÄ WÎ•º ÎÇòÎàÑÏñ¥ÏÑú ÏûÖÎ†•Ìï¥Ïïº Ìï¥ÏÑú d_modelÏù¥ 4Ïùò Î∞∞ÏàòÏó¨Ïïº Ìï®\n","        assert self.d_model % 4 == 0, \"Ï∞®ÏõêÏù¥ 4Ïùò Î∞∞ÏàòÍ∞Ä ÏïÑÎãôÎãàÎã§.\"\n","\n","        #Ï¢åÌëú Í≤©Ïûê ÎßåÎì§Í∏∞\n","        B, H, W = feature_map.size(0), feature_map.size(2), feature_map.size(3)\n","        H_coord, W_coord = torch.meshgrid(torch.arange(1, H+1, dtype=torch.float32, device=feature_map.device),\n","                                          torch.arange(1, W+1, dtype=torch.float32, device=feature_map.device),\n","                                          indexing='ij')\n","\n","        #Ï¢åÌëú Ïä§ÏºÄÏùºÎßÅ 0~2„Ö†\n","        H_coord = H_coord.unsqueeze(0).expand(B, -1, -1) / (H + 1e-6) * (2 * math.pi)\n","        W_coord = W_coord.unsqueeze(0).expand(B, -1, -1) / (W + 1e-6) * (2 * math.pi)\n","\n","        #pos ÎÇòÎàå Î∂ÑÎ™® ÎßåÎì§Í∏∞\n","        half_d = self.d_model // 2\n","        denom = torch.arange(0, half_d, dtype=torch.float32, device=feature_map.device) // 2\n","        denom = 10000 ** (2*denom / half_d)\n","\n","        #pos/10000^(2i/d_model)\n","        pos_H = H_coord.unsqueeze(-1) / denom\n","        pos_W = W_coord.unsqueeze(-1) / denom\n","\n","        #sin, cos Ï†ÅÏö©(ÏßùÏàòÎäî sin, ÌôÄÏàòÎäî cos)\n","        pos_H[:, :, :, 0::2].sin_()\n","        pos_H[:, :, :, 1::2].cos_()\n","        pos_W[:, :, :, 0::2].sin_()\n","        pos_W[:, :, :, 1::2].cos_()\n","\n","        #HÏôÄ W Ìï©ÏπòÍ∏∞ Î∞è (B, d_model, H, W)Î°ú Ï∞®Ïõê Î≥ÄÍ≤Ω\n","        pos_embedding = torch.cat([pos_H, pos_W], dim=-1).permute(0, 3, 1, 2)\n","\n","        # output shape : (B, d_model, H, W)\n","        return pos_embedding\n","\n","class Transformer_Decoder(nn.Module):\n","    \"\"\"\n","    ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Î™ÖÎûÄ\n","    C_dim: feature dimension = query dimension\n","    n_head: Î©ÄÌã∞Ìó§Îçî Ïñ¥ÌÖêÏÖòÏùò Ìó§Îçî Ïàò\n","    dim_feedforward: FFN ÎÇ¥Î∂ÄÏùò hidden layer Ï∞®Ïõê\n","    channel_dim_list: Í∞Å feature mapÏúÑ Ï±ÑÎÑê Ï∞®ÏõêÏàò Î¶¨Ïä§Ìä∏ [1/32, 1/16, 1/8, 1/4]\n","    mask_dim : 1/4 Ìï¥ÏÉÅÎèÑÏùò C Ï∞®Ïõê\n","    L: ÎîîÏΩîÎçî Î∞òÎ≥µ ÌöüÏàò Îî∞ÎùºÏÑú Ï¥ù 3LÍ∞úÏùò Î†àÏù¥Ïñ¥Í∞Ä ÏûàÎã§.(ÎÖºÎ¨∏Ïóê 3ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ defaultÎ•º 3ÏúºÎ°ú ÏÑ§Ï†ï)\n","    dropout: ÎìúÎ°≠ÏïÑÏõÉ (ÎÖºÎ¨∏ÏóêÏÑú 0ÏùºÎïåÍ∞Ä ÌçºÌè¨Î®ºÏä§Í∞Ä Í∞ÄÏû• Ï¢ãÏïòÎã§Í≥† Ìï¥ÏÑú defaultÎ•º 0ÏúºÎ°ú ÏÑ§Ï†ï)\n","    n_query: ÏøºÎ¶¨Ïùò Ïàò (ÎÖºÎ¨∏ÏóêÏÑú 100ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ defaultÎ•º 100ÏúºÎ°ú ÏÑ§Ï†ï)\n","    n_class: Íµ¨Î∂ÑÌï¥Ïïº ÌïòÎäî ÌÅ¥ÎûòÏàò Ïàò(Í±¥Î¨º ÌïòÎÇòÏùò ÌÅ¥ÎûòÏä§Îßå Ìï† Í≤ÉÏù¥ÎØÄÎ°ú default 1Î°ú ÏÑ§Ï†ï)\n","    \"\"\"\n","    def __init__(self,\n","                C_dim: int,\n","                n_head: int,\n","                dim_feedforward: int,\n","                channel_dim_list: list,\n","                mask_dim: int,\n","                L: int = 3,\n","                dropout: float = 0.0,\n","                n_query: int = 100,\n","                n_class: int = 1):\n","        super(Transformer_Decoder, self).__init__()\n","\n","        #Í∏∞Î≥∏ ÌååÎùºÎØ∏ÌÑ∞ Ï†ÄÏû•\n","        self.channel_dim_list = channel_dim_list\n","        self.L = L\n","        self.C_dim = C_dim\n","        self.n_head = n_head\n","        self.dim_feedforward = dim_feedforward\n","        self.dropout = dropout\n","        self.mask_dim = mask_dim\n","        self.n_class = n_class\n","\n","        # ÎîîÏΩîÎçî Î†àÏù¥Ïñ¥ ÎßåÎì§Í∏∞, 3Í∞ú Ï∏µÏùÑ LÎ≤à Î∞òÎ≥µ\n","        self.masked_attn_layers = nn.ModuleList()\n","        self.self_attn_layers = nn.ModuleList()\n","        self.ffn_layers = nn.ModuleList()\n","        self.make_layers()\n","\n","        #ÏøºÎ¶¨ ÏûÑÎ≤†Îî© ÎßåÎì§Í∏∞(ÌïôÏäµÍ∞ÄÎä• ÏûÑÎ≤†Îî©ÏúºÎ°ú ÎßåÎì§Í∏∞)\n","        self.n_query = n_query\n","        self.query_feat_embed = nn.Embedding(n_query, C_dim)\n","        self.query_pos_embed = nn.Embedding(n_query, C_dim)\n","\n","        #Ìï¥ÏÉÅÎèÑ ÏûÑÎ≤†Îî© ÎßåÎì§Í∏∞(3Í∞ÄÏßÄ Ìï¥ÏÉÅÎèÑÏóê ÎåÄÌï¥ ÌïôÏäµÍ∞ÄÎä• ÏûÑÎ≤†Îî©ÏúºÎ°ú ÎßåÎì§Í∏∞)\n","        self.resolution_embed = nn.Embedding(3, C_dim)\n","\n","        #Í∞Å feature mapÏùò Ï±ÑÎÑêÍ≥º C_dimÏù¥ Îã§Î•º Îïå Ï∞®ÏõêÏùÑ Î≥ÄÌôòÌïòÎäî Î™®Îìà ÎßåÎì§Í∏∞\n","        self.projection_modules = self.make_projection_modules()\n","\n","        #ÌîΩÏÖÄ ÏúÑÏπò ÏûÑÎ≤†Îî© Î™®Îìà ÎßåÎì§Í∏∞\n","        self.pixel_pos_embedding = pixel_position_embedding(d_model=C_dim)\n","\n","\n","        self.Cdim_norm = nn.LayerNorm(C_dim)\n","\n","        #class + Î∞∞Í≤Ω Î∞±ÌÑ∞\n","        self.class_predic = nn.Linear(C_dim, self.n_class+1)\n","        #mask embed: maske_dim(1/4 feature mapÏùò C)Î°ú Î∞îÍøà\n","        self.mask_embed = mlp(n_layers=3, input_dim=self.C_dim,\n","                              hidden_dim=self.C_dim, output_dim=self.mask_dim)\n","\n","\n","\n","    def make_layers(self):\n","        for _ in range(self.L * 3):\n","            self.masked_attn_layers.append(masked_attention(\n","                C_dim = self.C_dim,\n","                n_head = self.n_head,\n","                dropout = self.dropout\n","                ))\n","            self.self_attn_layers.append(self_attention(\n","                C_dim = self.C_dim,\n","                n_head = self.n_head,\n","                dropout = self.dropout\n","                ))\n","            self.ffn_layers.append(FFN(\n","                C_dim = self.C_dim,\n","                dim_feedforward = self.dim_feedforward,\n","                dropout = self.dropout\n","                ))\n","\n","    def make_projection_modules(self):\n","        output = nn.ModuleList()\n","        for channel_dim in self.channel_dim_list:\n","            if channel_dim != self.C_dim:\n","                output.append(nn.Conv2d(channel_dim, self.C_dim, kernel_size=1))\n","                weight_init.c2_xavier_fill(output[-1])\n","            else:\n","                output.append(nn.Identity())\n","        return output\n","\n","    def predic_class_mask_masking(self, x, feat_map4: Tensor, masking_size):\n","        x_normed = self.Cdim_norm(x)\n","        class_predic = self.class_predic(x_normed)\n","        mask_embed = self.mask_embed(x_normed)\n","\n","        # 1/4 Ìï¥ÏÉÅÎèÑ feature mapÍ≥º mask embed ÎÇ¥Ï†Å\n","        # (B, Q, C) @ (B, C, H/4, W/4) -> (B, Q, H/4, W/4)\n","        mask_predic = torch.einsum(\"bqc, bchw -> bqhw\", mask_embed, feat_map4)\n","\n","        #(B, Q, H_i, W_i) -> (B, Q, H_i*W_i)\n","        masking = F.interpolate(mask_predic, size=masking_size, mode='bilinear', align_corners= False).sigmoid().flatten(2)\n","        masking = masking.unsqueeze(0).repeat(self.n_head, 1, 1, 1).flatten(0,1)\n","        #0.5Ïù¥ÏÉÅÏù¥Î©¥ F ÎØ∏ÎßåÏù¥Î©¥ T\n","        #TÏù¥Î©¥ ÎßàÏä§ÌÇπÏùÑ Ìï¥Îùº, FÎ©¥ attentionÏùÑ ÌïòÎùº\n","        masking = (masking < 0.5).bool()\n","        #maskingÏù¥ mask_predicÏúºÎ°ú Î∂ÄÌÑ∞ ÌååÏÉùÎêòÎäî Í∞íÏù¥ÎØÄÎ°ú detach Ïã§Ìñâ\n","        masking = masking.detach()\n","\n","        return class_predic, mask_predic, masking\n","\n","    #feature_pyramid: ÌîΩÏÖÄ ÎîîÏΩîÎçîÏóêÏÑú ÎßåÎì† [1/32, 1/16, 1/8, 1/4] Ìï¥ÏÉÅÎèÑÏùò feature map Î¶¨Ïä§Ìä∏\n","    #Í∞Å feature mapÏùÄ (B, C, H, W) ÌòïÌÉú\n","    def forward(self, feature_pyramid: list[Tensor]):\n","        #feature mapÏùò Ï±ÑÎÑê Ï∞®Ïõê ÌôïÏù∏\n","        assert [feature_pyramid[i].size(1) for i in range(len(feature_pyramid))] == self.channel_dim_list, \"feature mapÏùò Ï±ÑÎÑê Ï∞®ÏõêÏù¥ ÎßûÏßÄ ÏïäÏäµÎãàÎã§.\"\n","\n","        #ÏøºÎ¶¨Ïóê ÎßûÍ≤å Ï∞®Ïõê Î≥ÄÌôòÎêú feature map + Ìï¥ÏÉÅÎèÑ embed\n","        proj_feat_list = []\n","        #pixelÏóê Îî∞Î•∏ embed Î¶¨Ïä§Ìä∏(ÏÉÅÏàò)\n","        pos_emb_list = []\n","        #Ìï¥ÏÉÅÎèÑ ÌÅ¨Í∏∞ Î¶¨Ïä§Ìä∏ (H, W)\n","        HW_list = []\n","        #Î∞∞ÏπòÏÇ¨Ïù¥Ï¶à\n","        B = feature_pyramid[0].size(0)\n","\n","        for i in range(3):\n","            #(B, C, H, W) -> (B, C_dim, HxW) + (1, C_dim, 1) -> (B, C_dim, HxW)\n","            proj_feat_list.append(\n","                self.projection_modules[i](feature_pyramid[i]).flatten(2)\n","                + self.resolution_embed.weight[i][None, :, None])\n","            #(B, C_dim, HxW)->(B, HxW, C_dim)\n","            proj_feat_list[-1] = proj_feat_list[-1].permute(0, 2, 1)\n","\n","            #(B, C_dim, H, W) -> (B, C_dim, HxW)->(B, HxW, C_dim)\n","            pos_emb_list.append(self.pixel_pos_embedding(feature_pyramid[i]).flatten(2).permute(0, 2, 1))\n","            #HWÏùò intÍ∞íÏùÑ Í∞ÄÏßÄÎäî Î¶¨Ïä§Ìä∏\n","            HW_list.append(feature_pyramid[i].shape[-2:])\n","\n","        # (Q, C_dim)-> (B, Q, C_dim)\n","        # layerÏóê ÏûÖÎ†•Ìï† ÏøºÎ¶¨Ïóê layerÏóêÏÑú ÎçîÌï† ÏøºÎ¶¨ ÏúÑÏπò embed ÏÑ§Ï†ï\n","        x = self.query_feat_embed.weight.unsqueeze(0).repeat(B, 1, 1)\n","        query_pos_embed = self.query_pos_embed.weight.unsqueeze(0).repeat(B, 1, 1)\n","\n","        class_predic_list = []\n","        mask_predic_list = []\n","\n","        class_predic, mask_predic, masking = self.predic_class_mask_masking(x, feat_map4=feature_pyramid[-1], masking_size=HW_list[0])\n","\n","        class_predic_list.append(class_predic)\n","        mask_predic_list.append(mask_predic)\n","\n","        #Ï¥ù 3L Î†àÏù¥Ïñ¥ : (1/32->1/16->1/8)*3\n","        for i in range(self.L):\n","            for j in range(3):\n","                masking[torch.where(torch.all(masking, dim=-1))] = False\n","\n","                index = 3*i + j\n","                #mask attention layer\n","                x = self.masked_attn_layers[index](\n","                    x_last=x,\n","                    img_feat=proj_feat_list[j],\n","                    mask = masking,\n","                    query_pos = query_pos_embed,\n","                    pixel_pos = pos_emb_list[j]\n","                    )\n","                #self attention layer\n","                x = self.self_attn_layers[index](\n","                    x_last=x,\n","                    query_pos=query_pos_embed,\n","                    mask = None\n","                )\n","                #FFN layer\n","                x = self.ffn_layers[index](\n","                    x_last=x\n","                )\n","\n","                # Îã§Ïùå Îã®Í≥Ñ ÎßàÏä§ÌÇπ ÏòàÏÉÅ + ÎßàÏä§ÌÅ¨, ÌÅ¥ÎûòÏä§ ÏòàÏÉÅ\n","                class_predic, mask_predic, masking = self.predic_class_mask_masking(x, feat_map4=feature_pyramid[-1], masking_size=HW_list[(j+1)%3])\n","\n","                #Í≤∞Í≥º Î¶¨Ïä§Ìä∏Ïóê Ï∂îÍ∞Ä\n","                class_predic_list.append(class_predic)\n","                mask_predic_list.append(mask_predic)\n","\n","        assert len(class_predic_list) == 3*self.L + 1, \"ÏòàÏ∏° ÌÅ¥ÎûòÏä§ Í∞úÏàò Ïïà ÎßûÏùå\"\n","        assert len(mask_predic_list) == 3*self.L + 1, \"ÏòàÏ∏° ÎßàÏä§ÌÅ¨ Í∞úÏàò Ïïà ÎßûÏùå\"\n","\n","        final_out = {\n","                'predicted_class' : class_predic_list[-1],\n","                'predicted_mask' : mask_predic_list[-1],\n","                'auxiliary_out' : [\n","                                   {\"predicted_class\":c, \"predicted_mask\":m}\n","                                   for c, m in zip(class_predic_list[:-1], mask_predic_list[:-1])\n","                                  ]\n","\n","                    }\n","        return final_out"],"metadata":{"id":"cXElTn8L6qfu","executionInfo":{"status":"ok","timestamp":1763207785286,"user_tz":-540,"elapsed":98,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# --- 7. ‚öñÔ∏è Î™®Îç∏: 4Î∂Ä (Matcher Î∞è Criterion) ---\n","\n","# --- from matcher.py ---\n","import torch\n","# (nn, Tensor, FÎäî Ïù¥ÎØ∏ ÏÉÅÎã®ÏóêÏÑú import ÌñàÏäµÎãàÎã§)\n","# from torch import nn, Tensor\n","# import torch.nn.functional as F\n","\n","# !pip install scipy (ÌïÑÏöîÏãú ÏÑ§Ïπò)\n","from scipy.optimize import linear_sum_assignment\n","\n","# @title (Helpers from matcher.py)\n","# matcher.pyÏóê Ï†ïÏùòÎêú JIT Ïä§ÌÅ¨Î¶ΩÌä∏ Ìï®ÏàòÎì§ÏùÑ ÌÅ¥ÎûòÏä§ Ïô∏Î∂ÄÏóê Ï†ïÏùòÌï©ÎãàÎã§.\n","# (Ï∞∏Í≥†: @torch.jit.script Îç∞ÏΩîÎ†àÏù¥ÌÑ∞Îäî ÌÅ¥ÎûòÏä§ Î©îÏÑúÎìú ÎÇ¥ÏóêÏÑú ÏßÅÏ†ë ÏÇ¨Ïö©ÌïòÍ∏∞Î≥¥Îã§\n","#  Î≥ÑÎèÑ Ìï®ÏàòÎ°ú Ï†ïÏùòÎêú Í≤ÉÏùÑ Ìò∏Ï∂úÌï† Îïå Í∞ÄÏû• Ïûò ÏûëÎèôÌï©ÎãàÎã§.)\n","\n","@torch.jit.script\n","def query_mask_sigmoid_ce_loss(mask_prob: Tensor, target_mask: Tensor, K: int):\n","            assert mask_prob.shape[-1]==K and target_mask.shape[-1]==K, \"KÍ∞Ä Ïïà ÎßûÏäµÎãàÎã§.\"\n","\n","            #Ï†ïÎãµÏù¥ 1Ïùº ÎïåÏùò loss Í≥ÑÏÇ∞ -> (Q, K)\n","            calc_one_loss = F.binary_cross_entropy_with_logits(\n","                mask_prob, torch.ones_like(mask_prob), reduction='none'\n","            )\n","            #Ï†ïÎãµÏù¥ 0Ïùº ÎïåÏùò loss Í≥ÑÏÇ∞ -> (Q, K)\n","            calc_zero_loss = F.binary_cross_entropy_with_logits(\n","                mask_prob, torch.zeros_like(mask_prob), reduction='none'\n","            )\n","\n","            loss_sum = (\n","                        #Î™®Îëê 1ÏùºÎïåÏùò loss Í∞íÍ≥º Ïã§Ï†ú ÎßàÏä§ÌÅ¨Î•º Í≥±Ìï¥ Ïã§Ï†ú 1Ïùº ÎïåÏùò loss Íµ¨Ìï®\n","                        #(Q, K) @ (K, N) -> (Q,  N)\n","                        torch.einsum('qk,nk->qn', calc_one_loss, target_mask)\n","                        #Î™®Îëê 0ÏùºÎïåÏùò loss Í∞íÍ≥º Ïã§Ï†ú ÎßàÏä§ÌÅ¨Î•º Í≥±Ìï¥(0Í≥º 1ÏùÑ Î∞îÍæº) Ïã§Ï†ú 0ÏùºÎïåÏùò loss Íµ¨Ìï®\n","                        #(Q, K) @ (K, N) -> (Q,  N)\n","                        + torch.einsum('qk,nk->qn', calc_zero_loss, (1 - target_mask))\n","                        )\n","            return loss_sum / K\n","\n","@torch.jit.script\n","def query_dice_loss(mask_prob: Tensor, target_mask: Tensor, K: int):\n","            assert mask_prob.shape[-1]==K and target_mask.shape[-1]==K, \"KÍ∞Ä Ïïà ÎßûÏäµÎãàÎã§.\"\n","            assert mask_prob.ndim == target_mask.ndim == 2 , \"Ï∞®ÏõêÏù¥ Ïïà ÎßûÏäµÎãàÎã§.\"\n","\n","            mask_prob = mask_prob.sigmoid()\n","            #2(A^B)\n","            numerator = 2*(torch.einsum('qk,nk->qn', mask_prob, target_mask))\n","            #|A|+|B|\n","            denominator = mask_prob.sum(-1).unsqueeze(-1) + target_mask.sum(-1).unsqueeze(0)\n","            # 1- 2(A^B)/(|A|+|B|) + zero division error Î∞©ÏßÄ\n","            loss = 1 -(numerator+1)/(denominator+1)\n","            return loss\n","\n","# @title hungarian_matcher (matcher.py)\n","class hungarian_matcher(nn.Module):\n","    #Í∞ÅÍ∞ÅÏùò lossÏùò Í∞ÄÏ§ëÏπòÎì§ÏùÑ ÌååÎùºÎØ∏ÌÑ∞Î°ú Í∞ÄÏßê\n","    def __init__(self, class_cost_weight:float = 1, mask_cost_weight:float = 1, dice_cost_weight:float = 1):\n","        super().__init__()\n","\n","        self.weight_c = class_cost_weight\n","        self.weight_m = mask_cost_weight\n","        self.weight_d = dice_cost_weight\n","        assert not (self.weight_c == 0 and self.weight_d == 0 and self.weight_m == 0), \"Í∞ÄÏ§ëÏπòÍ∞Ä Î™®Îëê 0ÏûÖÎãàÎã§.\"\n","\n","        # JIT Ìï®ÏàòÎ•º ÌÅ¥ÎûòÏä§ ÏÜçÏÑ±ÏúºÎ°ú Ìï†Îãπ\n","        self.mask_sigmoid_ce_loss_jit = query_mask_sigmoid_ce_loss\n","        self.dice_loss_jit = query_dice_loss\n","\n","    \"\"\"\n","    Trans_dec_oupts: Transform DecoderÏóêÏÑú ÎßåÎì† ÏµúÏ¢Ö output\n","    {\n","    'predicted_class' : (B, Q, 2)\n","    'predicted_mask' : (B, Q, H/4, W/4),\n","    'auxiliary_out' : [...]\n","    }\n","\n","    targets: Ïù¥ÎØ∏ÏßÄÏùò ÎùºÎ≤® [{instance_class: tensor, mask: tensor[N,H,W]}, {...} , {...}, ...]\n","    K: Îû®Îç§ÏúºÎ°ú ÌôïÏù∏Ìï† Ï¢åÌëú(ÌîΩÏÖÄ) Ïàò -> ÎÖºÎ¨∏ÏóêÏÑú 112*112Î•º ÏÇ¨Ïö©Ìï¥ÏÑú default Í∞íÏúºÎ°ú ÏÇ¨Ïö©\n","    \"\"\"\n","    def forward(self, trans_dec_outputs: dict, targets: list[dict] , K: int = 112*112):\n","\n","        B, Q = trans_dec_outputs['predicted_class'].shape[0:2]\n","        assert B == trans_dec_outputs['predicted_mask'].shape[0] and Q == trans_dec_outputs['predicted_mask'].shape[1], \"BÎûë QÍ∞Ä Ïù¥ÏÉÅÌï©ÎãàÎã§.\"\n","\n","        B_matching_cost = []\n","        for i_b in range(B):\n","            #(Q, 2)\n","            class_prob = trans_dec_outputs['predicted_class'][i_b].softmax(-1)\n","            #(N)\n","            target_class = targets[i_b]['instance_class'].to(class_prob.device)\n","            #(Q, N=1)\n","            class_cost = -class_prob[:, target_class]\n","\n","            #(Q, H/4, H/4) -> (Q, 1, H/4, W/4)\n","            mask_prob = trans_dec_outputs['predicted_mask'][i_b].unsqueeze(1)\n","            #(N, H, W) -> (N, 1, H, W)\n","            target_mask = targets[i_b]['mask'].to(mask_prob).unsqueeze(1)\n","\n","            # [-1, 1) ÏÇ¨Ïù¥ Îû®Îç§ Ï¢åÌëú KÍ∞ú ÏÉùÏÑ± (1, 1, K, 2)\n","            rand_coords = torch.rand(1, 1, K, 2, device=class_prob.device)*2 -1\n","\n","            mask_prob = F.grid_sample(\n","                mask_prob,\n","                rand_coords.repeat(mask_prob.shape[0], 1, 1, 1),\n","                align_corners = False\n","            )\n","            #(Q, 1, 1, K) -> (Q,K)\n","            mask_prob = mask_prob.squeeze(1).squeeze(1)\n","\n","            target_mask = F.grid_sample(\n","                target_mask,\n","                rand_coords.repeat(target_mask.shape[0], 1, 1, 1),\n","                align_corners = False\n","            )\n","            #(N, 1, 1, K) -> (N, K)\n","            target_mask = target_mask.squeeze(1).squeeze(1)\n","\n","            #float 32Î°ú Ïó∞ÏÇ∞\n","            with torch.cuda.amp.autocast(enabled=False):\n","                mask_prob = mask_prob.float()\n","                target_mask = target_mask.float()\n","                # mask match ÎπÑÏö© Í≥ÑÏÇ∞\n","                mask_cost = self.mask_sigmoid_ce_loss_jit(mask_prob, target_mask, K)\n","                # dice match ÎπÑÏö© Í≥ÑÏÇ∞\n","                dice_cost = self.dice_loss_jit(mask_prob, target_mask, K)\n","\n","            weighted_cost = self.weight_m*mask_cost + self.weight_d*dice_cost + self.weight_c*class_cost\n","            weighted_cost = weighted_cost.cpu()\n","            B_matching_cost.append(linear_sum_assignment(weighted_cost.detach().cpu().numpy()))\n","\n","        #[(ÏøºÎ¶¨ Ïù∏Îç±Ïä§ ÌÉ†ÏÑú, ÌÉÄÍ≤ü Ïù∏Îç±Ïä§ ÌÉ†ÏÑú), ...] -> batch Í∞úÏàòÎßåÌÅº Ìà¨Ìîå ÏûàÎäî Î¶¨Ïä§Ìä∏\n","        return [(torch.as_tensor(p, dtype=torch.int64), torch.as_tensor(t, dtype=torch.int64))\n","                    for p, t in B_matching_cost\n","        ]\n","\n","# --- from criterion.py ---\n","\n","# @title (Helpers from criterion.py)\n","# criterion.pyÏóê Ï†ïÏùòÎêú JIT Ïä§ÌÅ¨Î¶ΩÌä∏ Ìï®ÏàòÎì§ÏùÑ ÌÅ¥ÎûòÏä§ Ïô∏Î∂ÄÏóê Ï†ïÏùòÌï©ÎãàÎã§.\n","@torch.jit.script\n","def mask_sigmoid_ce_loss(mask_prob: Tensor, mask_target: Tensor):\n","    assert mask_prob.size(-1) == mask_target.size(-1), \"ÏòàÏ∏°Í≥º ÌÉÄÍ≤üÏùò ÏÇ¨Ïù¥Ï¶àÍ∞Ä Ïïà ÎßûÏäµÎãàÎã§.\"\n","\n","    loss = F.binary_cross_entropy_with_logits(mask_prob, mask_target, reduction='none')\n","\n","    return loss.mean(-1).sum() / mask_prob.size(0)\n","\n","@torch.jit.script\n","def dice_loss(mask_prob: Tensor, mask_target: Tensor):\n","    mask_prob = mask_prob.sigmoid()\n","\n","    numerator = 2 * (mask_prob * mask_target).sum(-1)\n","    denominator = mask_prob.sum(-1) + mask_target.sum(-1)\n","\n","    loss = 1-(numerator+1) / (denominator+1)\n","\n","    return loss.sum() / mask_prob.size(0)\n","\n","# @title criterion (criterion.py)\n","class criterion(nn.Module):\n","    def __init__(self, oversample_ratio: float ,important_sample_ratio: float, class_cost_weight: float=1, mask_cost_weight: float=1, dice_cost_weight: float=1, eos_weigt: float=1,\n","                K: int=112*112 ):\n","        super().__init__()\n","\n","        self.matcher = hungarian_matcher()\n","\n","        self.important_sample_ratio = important_sample_ratio\n","        self.oversample_ratio = oversample_ratio\n","\n","        self.w_m = mask_cost_weight\n","        self.w_c = class_cost_weight\n","        self.w_d = dice_cost_weight\n","\n","        self.w_e = eos_weigt\n","\n","        background_ratio = torch.ones(2)\n","        background_ratio[-1] = self.w_e\n","        self.background_ratio = background_ratio\n","\n","        self.K = K\n","\n","        self.mask_loss_jit = mask_sigmoid_ce_loss\n","        self.dice_loss_jit = dice_loss\n","\n","    #Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅÏùÑ ÌïòÍ∏∞ ÏúÑÌïú Ï†êÏàòÎ•º Í≥ÑÏÇ∞ÌïòÎäî Ìï®Ïàò\n","    #(N, K)\n","   # (7Î≤à ÏÖÄÏùò criterion ÌÅ¥ÎûòÏä§ ÎÇ¥Î∂ÄÎ•º Ïù¥Í±∏Î°ú ÏàòÏ†ïÌïòÏÑ∏Ïöî)\n","# (class criterion(nn.Module): ... self.dice_loss_jit = dice_loss ... Ïù¥ÌõÑ)\n","\n","    #Ï§ëÏöîÎèÑ ÏÉòÌîåÎßÅÏùÑ ÌïòÍ∏∞ ÏúÑÌïú Ï†êÏàòÎ•º Í≥ÑÏÇ∞ÌïòÎäî Ìï®Ïàò\n","    #(N, K)\n","    def calc_uncertainty_score(self, coords):\n","        #Ïï†Îß§Ìïú ÌîΩÏÖÄ(ÌôïÎ•† 0.5Ïóê Í∞ÄÍπåÏö¥)ÏùÑ ÎÜíÏùÄ Ï†êÏàòÎ•º Ï£ºÍ∏∞ ÏúÑÌï¥ -Ï†àÎåìÍ∞í\n","        score = -(torch.abs(coords))\n","        return score\n","\n","    # [ÏàòÏ†ïÎê®] get_query_idx1ÏùÑ Ìò∏Ï∂úÌï† Îïå deviceÎ•º ÎÑòÍ≤®Ï§çÎãàÎã§.\n","    def calc_class_loss(self, matched: list[tuple], predicted_class: Tensor):\n","        B, Q = predicted_class.size(0), predicted_class.size(1)\n","\n","        # [ÏàòÏ†ï] predicted_class.deviceÎ•º Ïù∏ÏûêÎ°ú Ï†ÑÎã¨\n","        idx = self.get_query_idx1(matched, Q, device=predicted_class.device)\n","\n","        target_class = torch.ones(B*Q, dtype=torch.int64, device=predicted_class.device)\n","        target_class[idx] = 0\n","\n","        # (ÏàòÏ†ï) background_ratioÎ•º predicted_classÏôÄ ÎèôÏùºÌïú deviceÎ°ú Ïù¥Îèô\n","        class_loss = F.cross_entropy(predicted_class.reshape(B*Q, 2), target_class, self.background_ratio.to(predicted_class.device))\n","        output = {'class_loss': class_loss}\n","        return output\n","\n","    # [ÏàòÏ†ïÎê®] device Ïù∏ÏûêÎ•º Î∞õÍ≥†, CPU ÌÖêÏÑúÎ•º .to(device)Î°ú ÏòÆÍπÅÎãàÎã§.\n","    def get_query_idx1(self, matched: list[tuple], Q:int, device): # 'device' Ïù∏Ïûê Ï∂îÍ∞Ä\n","        # (ÏàòÏ†ï) matched Î¶¨Ïä§Ìä∏Í∞Ä ÎπÑÏñ¥ÏûàÏùÑ Í≤ΩÏö∞(Îß§Ïπ≠Îêú Í≤ÉÏù¥ ÏóÜÏùÑ Îïå) Îπà ÌÖêÏÑú Î∞òÌôò\n","        if not matched or all(len(q) == 0 for q, _ in matched):\n","            # [ÏàòÏ†ï] 'cpu' ÎåÄÏã† Ï†ÑÎã¨Î∞õÏùÄ 'device' ÏÇ¨Ïö©\n","            return torch.tensor([], dtype=torch.int64, device=device)\n","\n","        # [ÏàòÏ†ï] matcherÍ∞Ä CPUÎ°ú Î∞òÌôòÌïú 'q'Î•º .to(device)Î°ú GPUÏóê Ïò¨Î¶º\n","        idx = torch.cat([i*Q + q.to(device) for i, (q, _) in enumerate(matched)])\n","        return idx\n","\n","    # [ÏàòÏ†ïÎê®] device Ïù∏ÏûêÎ•º Î∞õÍ≥†, CPU ÌÖêÏÑúÎ•º .to(device)Î°ú ÏòÆÍπÅÎãàÎã§.\n","    def get_query_idx2(self, matched: list[tuple], device): # 'device' Ïù∏Ïûê Ï∂îÍ∞Ä\n","        # (ÏàòÏ†ï) matched Î¶¨Ïä§Ìä∏Í∞Ä ÎπÑÏñ¥ÏûàÏùÑ Í≤ΩÏö∞ Îπà ÌÖêÏÑú Î∞òÌôò\n","        if not matched or all(len(q) == 0 for q, _ in matched):\n","            # [ÏàòÏ†ï] 'cpu' ÎåÄÏã† Ï†ÑÎã¨Î∞õÏùÄ 'device' ÏÇ¨Ïö©\n","            empty_tensor = torch.tensor([], dtype=torch.int64, device=device)\n","            return empty_tensor, empty_tensor\n","\n","        # [ÏàòÏ†ï] matcherÍ∞Ä CPUÎ°ú Î∞òÌôòÌïú 'q'Î•º .to(device)Î°ú GPUÏóê Ïò¨Î¶º\n","        batch_idx = torch.cat([torch.full_like(q.to(device), i) for i, (q, _) in enumerate(matched)])\n","        query_idx = torch.cat([q.to(device) for (q, _) in matched])\n","        return batch_idx, query_idx\n","\n","    # [ÏàòÏ†ïÎê®] get_query_idx2ÏôÄ get_target_idxÎ•º Ìò∏Ï∂úÌï† Îïå deviceÎ•º ÎÑòÍ≤®Ï§çÎãàÎã§.\n","    def calc_mask_dice_loss(self, matched: list[tuple], predicted_mask:Tensor, label: list[dict]):\n","\n","        # [ÏàòÏ†ï] predicted_mask.deviceÎ•º Ïù∏ÏûêÎ°ú Ï†ÑÎã¨\n","        q_idx = self.get_query_idx2(matched, device=predicted_mask.device)\n","\n","        # (ÏàòÏ†ï) Îß§Ïπ≠Îêú Í≤ÉÏù¥ ÏóÜÏúºÎ©¥ 0 Î∞òÌôò\n","        if q_idx[0].numel() == 0:\n","            return {\n","                'mask_loss': torch.tensor(0.0, device=predicted_mask.device),\n","                'dice_loss': torch.tensor(0.0, device=predicted_mask.device)\n","            }\n","\n","        #(B, Q, H/4, W/4) -> (T, 1, H/4, W/4) -> T: ÌôïÏù∏Ìï† Ï¥ù ÎßàÏä§ÌÅ¨ Ïàò\n","        predicted_mask = predicted_mask[q_idx].unsqueeze(1)\n","\n","        # [ÏàòÏ†ï] predicted_mask.deviceÎ•º Ïù∏ÏûêÎ°ú Ï†ÑÎã¨\n","        t_idx = self.get_target_idx(matched, device=predicted_mask.device)\n","\n","        #(T, H, W)- -> (T, 1, H, W)\n","        target_mask = torch.cat([t['mask'] for t in label], dim=0)\n","        #ÏàúÏÑúÎåÄÎ°ú Î∞îÍæ∏Í∏∞\n","        target_mask = torch.index_select(target_mask, dim=0, index=t_idx).unsqueeze(1) # <- Ïó¨Í∏∞Í∞Ä Î¨∏Ï†úÏùò ÏßÄÏ†ê\n","\n","        with torch.no_grad():\n","            coords = self.get_coords(predicted_mask).unsqueeze(1)\n","            target_point = F.grid_sample(\n","                target_mask.float(), # (ÏàòÏ†ï) grid_sampleÏùÄ float ÏûÖÎ†• ÌïÑÏöî\n","                coords,\n","                align_corners=False\n","            ).squeeze(1).squeeze(1)\n","\n","        predict_point = F.grid_sample(\n","            predicted_mask,\n","            coords,\n","            align_corners=False\n","        ).squeeze(1).squeeze(1)\n","\n","        loss = {\n","                'mask_loss': self.mask_loss_jit(predict_point, target_point),\n","                'dice_loss': self.dice_loss_jit(predict_point, target_point)\n","            }\n","\n","        return loss\n","\n","    # [ÏàòÏ†ïÎê®] device Ïù∏ÏûêÎ•º Î∞õÍ≥†, CPU ÌÖêÏÑúÎ•º .to(device)Î°ú ÏòÆÍπÅÎãàÎã§.\n","    def get_target_idx(self, matched: list[tuple], device): # 'device' Ïù∏Ïûê Ï∂îÍ∞Ä\n","        target = [t for (_, t) in matched]\n","\n","        # (ÏàòÏ†ï) Îß§Ïπ≠Îêú Í≤ÉÏù¥ ÏóÜÏúºÎ©¥ Îπà ÌÖêÏÑú Î∞òÌôò\n","        if not target or all(t.numel() == 0 for t in target):\n","             # [ÏàòÏ†ï] 'cpu' ÎåÄÏã† Ï†ÑÎã¨Î∞õÏùÄ 'device' ÏÇ¨Ïö©\n","             return torch.tensor([], dtype=torch.int64, device=device)\n","\n","        size = [t.numel() for t in target]\n","\n","        # (ÏàòÏ†ï) device ÏùºÏπò\n","        # device = target[0].device  <- Ïù¥ Î∂ÄÎ∂ÑÏù¥ CPUÎ•º Ï∞∏Ï°∞ÌïòÍ≤å ÎêòÏñ¥ Î≤ÑÍ∑∏ Î∞úÏÉù. ÏÇ≠Ï†ú.\n","\n","        # [ÏàòÏ†ï] matcherÍ∞Ä CPUÎ°ú Î∞òÌôòÌïú 't'Î•º .to(device)Î°ú GPUÏóê Ïò¨Î¶º\n","        idx = torch.cat([t.to(device) for t in target])\n","\n","        # [ÏàòÏ†ï] offset Í≥ÑÏÇ∞ÎèÑ Ï†ÑÎã¨Î∞õÏùÄ 'device' ÏÇ¨Ïö©\n","        offset = torch.tensor([0] + list(torch.cumsum(torch.tensor(size[:-1]), 0)), device=device)\n","        offset = torch.repeat_interleave(offset, torch.tensor(size, device=device))\n","\n","        return idx + offset\n","\n","    # (get_coords Ìï®ÏàòÏôÄ forward Ìï®ÏàòÎäî ÏàòÏ†ïÌï† ÌïÑÏöî ÏóÜÏù¥ Í∑∏ÎåÄÎ°ú ÎëêÎ©¥ Îê©ÎãàÎã§)\n","    # ... def get_coords(self, ...): ...\n","    # ... def forward(self, ...): ...\n","\n","    def get_coords(self, predicted_mask):\n","        assert self.oversample_ratio >= 1, \"oversample_ratioÎäî 1Î≥¥Îã§ Ïª§Ïïº Ìï©ÎãàÎã§.\"\n","        assert 0 <= self.important_sample_ratio <= 1, \"important_sample_ratioÎäî 0Í≥º 1 ÏÇ¨Ïù¥Ïù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.\"\n","\n","        n_sample = int(self.oversample_ratio * self.K)\n","        n_important = int(self.important_sample_ratio * self.K)\n","        n_random = self.K - n_important\n","\n","        T = predicted_mask.shape[0]\n","        #(T, n_sample, 2)\n","        sample_coord = torch.rand(T, n_sample, 2, device=predicted_mask.device)*2 - 1\n","         #(T, 1, n_sample, 2)\n","        sample_coord_grid = sample_coord.unsqueeze(1)\n","        #(T, 1, 1, n_sample)\n","        sample_point = F.grid_sample(\n","            predicted_mask,\n","            sample_coord_grid,\n","            align_corners=False\n","        )\n","        #(T, n_sample)\n","        sample_point = sample_point.squeeze(1).squeeze(1)\n","\n","        score = self.calc_uncertainty_score(sample_point)\n","        idx = torch.topk(score, k=n_important, dim=1)[1]\n","\n","        offset = torch.arange(T, dtype=torch.long, device=predicted_mask.device)*n_sample\n","        idx += offset.unsqueeze(-1)\n","\n","        important_coord = sample_coord.view(-1, 2)[idx.view(-1), :].view(T, n_important, 2)\n","\n","        if n_random > 0:\n","            random_coord = torch.rand(T, n_random, 2, device=predicted_mask.device)*2 -1\n","            final_coord = torch.cat([important_coord, random_coord], dim=1)\n","        else:\n","            final_coord = important_coord\n","\n","        return final_coord\n","\n","    def forward(self, trans_dec_outputs: dict, targets: list[dict] ):\n","\n","        auxiliary_out = trans_dec_outputs['auxiliary_out']\n","\n","        #matcher: [(ÏøºÎ¶¨ Ïù∏Îç±Ïä§ ÌÉ†ÏÑú, ÌÉÄÍ≤ü Ïù∏Îç±Ïä§ ÌÉ†ÏÑú), ...] -> batch Í∞úÏàòÎßåÌÅº Ìà¨Ìîå ÏûàÎäî Î¶¨Ïä§Ìä∏\n","        matched = self.matcher(trans_dec_outputs, targets, K=self.K)\n","\n","        final_loss = {}\n","        final_loss.update(self.calc_class_loss(matched, trans_dec_outputs['predicted_class']))\n","        final_loss.update(self.calc_mask_dice_loss(matched, trans_dec_outputs['predicted_mask'], targets))\n","\n","        for i, aux in enumerate(auxiliary_out):\n","            # (ÏàòÏ†ï) aux loss Í≥ÑÏÇ∞ ÏãúÏóêÎèÑ KÍ∞í Ï†ÑÎã¨\n","            aux_matched = self.matcher(aux, targets, K=self.K)\n","            temp = self.calc_class_loss(aux_matched, aux['predicted_class'])\n","            temp.update(self.calc_mask_dice_loss(aux_matched, aux['predicted_mask'], targets))\n","            temp = {k + f\"_{i}\": v for k, v in temp.items()}\n","            final_loss.update(temp)\n","\n","        return final_loss"],"metadata":{"id":"jUScFprSP6sE","executionInfo":{"status":"ok","timestamp":1763207785362,"user_tz":-540,"elapsed":75,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# --- 7. üöÄ Î™®Îç∏: 4Î∂Ä (Mask2Former Ï°∞Î¶Ω) ---\n","# (ÎàÑÎùΩÎêú Î∂ÄÎ∂ÑÏù¥ÎØÄÎ°ú, ÌëúÏ§Ä ÏïÑÌÇ§ÌÖçÏ≤òÏóê ÎßûÏ∂∞ ÏÉàÎ°ú ÏûëÏÑ±)\n","\n","class Mask2Former(nn.Module):\n","    \"\"\"\n","    Backbone (Swin) + Pixel Decoder (TEncoderPixelDecoder) + Transformer Decoder (Transformer_Decoder)\n","    Î•º Ï°∞Î¶ΩÌïòÎäî ÏµúÏ¢Ö Mask2Former Î™®Îç∏ÏûÖÎãàÎã§.\n","    \"\"\"\n","\n","    # Ïù¥ ÌÅ¥ÎûòÏä§Îäî from_configÎ•º ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ≥†,\n","    # ÌïÑÏöîÌïú ÏÑ§Ï†ïÍ∞í(cfg)ÏùÑ ÏßÅÏ†ë Î∞õÏïÑ ÌïòÏúÑ Î™®ÎìàÏùÑ Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.\n","    def __init__(self, backbone_cfg, pixel_decoder_cfg, transformer_decoder_cfg, input_shape):\n","        super().__init__()\n","\n","        # --- 1. Backbone (Swin) Ï¥àÍ∏∞Ìôî ---\n","        # input_shapeÏùÄ (C, H, W)Î•º Í∞ÄÏ†ï (e.g., ShapeSpec(channels=3))\n","        # D2SwinTransformer.from_configÎäî cfgÏôÄ input_shapeÏùÑ Î∞õÏùå\n","        self.backbone = D2SwinTransformer(\n","            **D2SwinTransformer.from_config(backbone_cfg, input_shape)\n","        )\n","\n","        # Î∞±Î≥∏Ïùò Ï∂úÎ†• shapeÏùÑ Í∞ÄÏ†∏Ïò¥ (e.g., {\"res2\": ShapeSpec, ...})\n","        backbone_output_shape = self.backbone.output_shape()\n","\n","        # --- 2. Pixel Decoder (FPN + Encoder) Ï¥àÍ∏∞Ìôî ---\n","        # TransformerEncoderPixelDecoder.from_configÎäî cfgÏôÄ backbone_output_shapeÏùÑ Î∞õÏùå\n","        pixel_decoder_params = TransformerEncoderPixelDecoder.from_config(\n","            pixel_decoder_cfg, backbone_output_shape\n","        )\n","        self.pixel_decoder = TransformerEncoderPixelDecoder(**pixel_decoder_params)\n","\n","        # --- 3. Transformer Decoder Ï¥àÍ∏∞Ìôî ---\n","        # Transformer_DecoderÎäî cfgÍ∞Ä ÏïÑÎãå Í∞úÎ≥Ñ Ïù∏ÏûêÎ•º Î∞õÏäµÎãàÎã§.\n","        # (Ï£ºÏùò: channel_dim_listÏôÄ C_dim, mask_dimÏù¥ Pixel DecoderÏôÄ ÏùºÏπòÌï¥Ïïº Ìï®)\n","\n","        # Pixel DecoderÏùò FPN Ï∂úÎ†• Ï±ÑÎÑê (conv_dim)\n","        # (res5 -> 1/32, res4 -> 1/16, res3 -> 1/8)\n","        # (Ï£ºÏùò: TransformerEncoderPixelDecoderÏùò multi_scale_featuresÎäî 3Í∞úÎßå Î∞òÌôòÌï®)\n","        conv_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","\n","        # Pixel DecoderÏùò ÏµúÏ¢Ö ÎßàÏä§ÌÅ¨ ÌäπÏßï Ï±ÑÎÑê (mask_dim)\n","        # (res2 -> 1/4)\n","        mask_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","\n","        # Transformer_DecoderÏóê Ï†ÑÎã¨Ìï† Ï±ÑÎÑê Î¶¨Ïä§Ìä∏\n","        # [1/32, 1/16, 1/8, 1/4] Ïä§ÏºÄÏùºÏóê Ìï¥Îãπ\n","        # (Ï£ºÏùò: Transformer_DecoderÏùò projection_modules[i]Îäî feature_pyramid[i]Ïóê Ï†ÅÏö©Îê®)\n","        # (Ï£ºÏùò: Transformer_DecoderÏùò forwardÎäî 3Í∞úÏùò feature map(0,1,2)Í≥º 1Í∞úÏùò mask map(3)ÏùÑ ÏÇ¨Ïö©Ìï®)\n","        # (Ï£ºÏùò: TransformerEncoderPixelDecoderÏùò multi_scale_featuresÎäî 3Í∞ú(conv_dim)Î•º,\n","        #  mask_featuresÎäî 1Í∞ú(mask_dim)Î•º Î∞òÌôòÌï®. channel_dim_listÏôÄ ÏàúÏÑú/Í∞úÏàòÍ∞Ä ÎßûÏßÄ ÏïäÏùå)\n","\n","        # --- [Ï∂©Îèå Ìï¥Í≤∞] ---\n","        # Ï†úÍ≥µÎêú Transformer_DecoderÏùò forwardÎäî 4Í∞úÏùò feature_pyramid ÏûÖÎ†•ÏùÑ Î∞õÏïÑ\n","        # [0], [1], [2] (3Í∞ú)Îäî cross-attentionÏóê, [3] (1Í∞ú)Îäî ÎßàÏä§ÌÅ¨ ÏòàÏ∏°Ïóê ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n","\n","        # TransformerEncoderPixelDecoderÏùò multi_scale_features (3Í∞ú)Î•º [0, 1, 2]Ïóê,\n","        # mask_features (1Í∞ú)Î•º [3]Ïóê Îß§ÌïëÌï©ÎãàÎã§.\n","\n","        # (Pixel DecoderÏùò multi_scale_featuresÎäî [res5(1/32), res4(1/16), res3(1/8)] ÏàúÏÑú)\n","        # (Pixel DecoderÏùò mask_featuresÎäî [res2(1/4)] Ïä§ÏºÄÏùº)\n","\n","        # channel_dim_list (Transformer_DecoderÍ∞Ä Í∏∞ÎåÄÌïòÎäî ÏûÖÎ†• Ï±ÑÎÑê)\n","        # [1/32, 1/16, 1/8, 1/4]\n","        # (res5, res4, res3Ïùò Ï±ÑÎÑêÏùÄ pixel_decoderÏùò conv_dim)\n","        # (res2Ïùò Ï±ÑÎÑêÏùÄ pixel_decoderÏùò mask_dim)\n","        decoder_channel_dim_list = [conv_dim, conv_dim, conv_dim, mask_dim]\n","\n","        # Transformer_DecoderÏùò C_dim (ÎÇ¥Î∂Ä Hidden Dim)\n","        # pixel_decoder_cfgÏùò conv_dimÍ≥º ÎèôÏùºÌïòÍ≤å ÎßûÏ∂îÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏûÖÎãàÎã§.\n","        decoder_C_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","\n","        # Transformer_DecoderÏùò mask_dim (MLP Ï∂úÎ†• Dim)\n","        # pixel_decoder_cfgÏùò mask_dimÍ≥º ÎèôÏùºÌï¥Ïïº Ìï©ÎãàÎã§.\n","        decoder_mask_dim = pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","\n","        self.transformer_decoder = Transformer_Decoder(\n","            C_dim=decoder_C_dim,\n","            n_head=transformer_decoder_cfg.MODEL.MASK_FORMER.NHEADS,\n","            dim_feedforward=transformer_decoder_cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD,\n","            channel_dim_list=decoder_channel_dim_list,\n","            mask_dim=decoder_mask_dim,\n","            L=transformer_decoder_cfg.MODEL.MASK_FORMER.DEC_LAYERS, # (cfgÏóê Ïù¥ Ìï≠Î™©Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§)\n","            dropout=transformer_decoder_cfg.MODEL.MASK_FORMER.DROPOUT,\n","            n_query=transformer_decoder_cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES, # (cfgÏóê Ïù¥ Ìï≠Î™©Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§)\n","            n_class=transformer_decoder_cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES # (cfgÏóê Ïù¥ Ìï≠Î™©Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§)\n","        )\n","\n","    def forward(self, x):\n","        # 1. Î∞±Î≥∏ Ïã§Ìñâ (Swin)\n","        # (Input: [B, 3, H, W])\n","        # (Output: {\"res2\": [B,C2,H/4,W/4], \"res3\": [B,C3,H/8,W/8], ...})\n","        features = self.backbone(x)\n","\n","        # 2. ÌîΩÏÖÄ ÎîîÏΩîÎçî Ïã§Ìñâ (TransformerEncoderPixelDecoder)\n","        # (Input: features dict)\n","        # (Output: mask_features [B, mask_dim, H/4, W/4],\n","        #          transformer_encoder_features [B, conv_dim, H/32, W/32],\n","        #          multi_scale_features [List of 3 Tensors: 1/32, 1/16, 1/8 scale])\n","        mask_features, _, multi_scale_features = self.pixel_decoder.forward_features(features)\n","\n","        # 3. Ìä∏ÎûúÏä§Ìè¨Î®∏ ÎîîÏΩîÎçî Ïã§Ìñâ (Transformer_Decoder)\n","        # Transformer_DecoderÎäî [1/32, 1/16, 1/8, 1/4] Ïä§ÏºÄÏùºÏùò Î¶¨Ïä§Ìä∏Î•º Í∏∞ÎåÄÌï©ÎãàÎã§.\n","\n","        # (Pixel DecoderÏùò multi_scale_featuresÎäî [res5(1/32), res4(1/16), res3(1/8)])\n","        # (Pixel DecoderÏùò mask_featuresÎäî res2(1/4) Ïä§ÏºÄÏùº)\n","\n","        # (Ï£ºÏùò) TransformerEncoderPixelDecoderÏùò multi_scale_featuresÎäî\n","        # [res5, res4, res3] (High-res to Low-resÍ∞Ä ÏïÑÎãò)\n","        # ÏõêÎ≥∏ BasePixelDecoderÏùò ÏàúÏÑúÎäî low-to-high ([res5, res4, res3]) ÏûÖÎãàÎã§.\n","        # Transformer_DecoderÏùò forwardÎäî [1/32, 1/16, 1/8] ÏàúÏÑúÎ•º Í∏∞ÎåÄÌï©ÎãàÎã§.\n","\n","        # feature_pyramid Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±: [feat_1/32, feat_1/16, feat_1/8, feat_1/4]\n","        feature_pyramid = [\n","            multi_scale_features[0], # 1/32 (res5)\n","            multi_scale_features[1], # 1/16 (res4)\n","            multi_scale_features[2], # 1/8 (res3)\n","            mask_features            # 1/4 (res2, mask_dim Ï±ÑÎÑê)\n","        ]\n","\n","        # (Input: [List of 4 Tensors])\n","        # (Output: {'predicted_class': ..., 'predicted_mask': ..., 'auxiliary_out': ...})\n","        outputs = self.transformer_decoder(feature_pyramid)\n","\n","        return outputs"],"metadata":{"id":"g4aJhOu99Ssk","executionInfo":{"status":"ok","timestamp":1763207785365,"user_tz":-540,"elapsed":1,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# --- 8. üöÄ Ïã§Ìñâ Î∞è Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ ---\n","#\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import glob\n","import os\n","import types\n","\n","# (ÏÖÄ 1, 3, 4, 5, 6, 7ÏóêÏÑú Ï†ïÏùòÎêú Î™®Îì† ÌÅ¥ÎûòÏä§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§)\n","# (e.g., ShapeSpec, D2SwinTransformer, TransformerEncoderPixelDecoder, Transformer_Decoder, criterion)\n","\n","# --- 1. ÌïôÏäµÏö© Dataset Î∞è DataLoader Ï§ÄÎπÑ ---\n","\n","class BuildingTrainingDataset(Dataset):\n","    \"\"\"\n","    Ï†ÑÏ≤òÎ¶¨Î°ú ÏÉùÏÑ±Îêú Í∞úÎ≥Ñ .pt ÌååÏùºÎì§ÏùÑ ÏùΩÏñ¥Ïò§Îäî ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖã\n","    \"\"\"\n","    def __init__(self, data_dir):\n","        self.image_dir = os.path.join(data_dir, \"images\")\n","        self.label_dir = os.path.join(data_dir, \"labels_dict\")\n","        self.file_list = sorted(glob.glob(os.path.join(self.label_dir, \"*.pt\")))\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        label_path = self.file_list[idx]\n","        file_name = os.path.basename(label_path)\n","        image_path = os.path.join(self.image_dir, file_name)\n","\n","        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú (float Î≥ÄÌôò Î∞è Ï†ïÍ∑úÌôî)\n","        # (Ï£ºÏùò) ÏõêÎ≥∏ .ptÍ∞Ä uint8Ïù¥ÏóàÎã§Î©¥ float() Î≥ÄÌôò Î∞è 255.0 ÎÇòÎàÑÍ∏∞ ÌïÑÏöî\n","        # (2Î≤à ÏÖÄÏùò create_final_dataset_per_fileÏóêÏÑú .float()Î°ú Ï†ÄÏû•ÌñàÏúºÎØÄÎ°ú /255.0Îßå ÌïÑÏöî)\n","        image = torch.load(image_path) / 255.0\n","\n","        # ÎùºÎ≤® ÎîïÏÖîÎÑàÎ¶¨ Î°úÎìú\n","        target = torch.load(label_path)\n","\n","        # (Ï§ëÏöî) ÎùºÎ≤® ÌÇ§ Î≥ÄÍ≤Ω (criterion ÏΩîÎìúÏôÄ ÎßûÏ∂§)\n","        # (2Î≤à ÏÖÄÏóêÏÑú 'masks'Î°ú Ï†ÄÏû•ÌñàÏúºÎØÄÎ°ú Ïù¥Î¶Ñ Î≥ÄÍ≤Ω Î∂àÌïÑÏöî)\n","        # target['masks'] = target.pop('mask')\n","        # (2Î≤à ÏÖÄÏóêÏÑú 'instance_class'Î°ú Ï†ÄÏû•ÌñàÏúºÎØÄÎ°ú Ïù¥Î¶Ñ Î≥ÄÍ≤Ω ÌïÑÏöî)\n","        target['instance_class'] = target.pop('instance_class')\n","        # (ÏàòÏ†ï) criterion.pyÏùò calc_mask_dice_lossÍ∞Ä 'mask' ÌÇ§Î•º Ï∞∏Ï°∞Ìï©ÎãàÎã§.\n","        # (ÏàòÏ†ï) 2Î≤à ÏÖÄÏùò convert_to_target_formatÏù¥ 'masks'Î°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\n","        # (ÏàòÏ†ï) 7Î≤à ÏÖÄÏùò matcher.pyÍ∞Ä 'mask' ÌÇ§Î•º Ï∞∏Ï°∞Ìï©ÎãàÎã§. -> 7Î≤à ÏÖÄ ÏàòÏ†ï ÌïÑÏöî\n","        # (ÏûÑÏãú) 2Î≤à ÏÖÄÏùò 'masks'Î•º 7Î≤à ÏÖÄÏóê ÎßûÍ≤å 'mask'Î°ú Î≥ÄÍ≤Ω\n","        target['mask'] = target.pop('masks')\n","\n","\n","        return image, target\n","\n","def collate_fn(batch):\n","    \"\"\"\n","    Î∞∞Ïπò ÎÇ¥Ïùò Ïù¥ÎØ∏ÏßÄÏôÄ ÌÉÄÍ≤ü(ÎîïÏÖîÎÑàÎ¶¨ Î¶¨Ïä§Ìä∏)ÏùÑ Î¨∂Ïñ¥Ï§çÎãàÎã§.\n","    \"\"\"\n","    images = [item[0] for item in batch]\n","    targets = [item[1] for item in batch]\n","    images = torch.stack(images, dim=0)\n","    return images, targets\n","\n","# --- 2. ÏÑ§Ï†ïÍ∞í (Configs) Ï§ÄÎπÑ (9Î≤à ÏÖÄÏóêÏÑú Î≥µÏÇ¨) ---\n","\n","# 2.1: Î∞±Î≥∏(Swin) ÏÑ§Ï†ï\n","backbone_cfg = _CfgNode()\n","backbone_cfg.MODEL = _CfgNode()\n","backbone_cfg.MODEL.SWIN = _CfgNode()\n","backbone_cfg.MODEL.SWIN.PRETRAIN_IMG_SIZE = 224\n","backbone_cfg.MODEL.SWIN.PATCH_SIZE = 4\n","backbone_cfg.MODEL.SWIN.EMBED_DIM = 96\n","backbone_cfg.MODEL.SWIN.DEPTHS = [2, 2, 6, 2]\n","backbone_cfg.MODEL.SWIN.NUM_HEADS = [3, 6, 12, 24]\n","backbone_cfg.MODEL.SWIN.WINDOW_SIZE = 7\n","backbone_cfg.MODEL.SWIN.MLP_RATIO = 4.0\n","backbone_cfg.MODEL.SWIN.QKV_BIAS = True\n","backbone_cfg.MODEL.SWIN.QK_SCALE = None\n","backbone_cfg.MODEL.SWIN.DROP_RATE = 0.0\n","backbone_cfg.MODEL.SWIN.ATTN_DROP_RATE = 0.0\n","backbone_cfg.MODEL.SWIN.DROP_PATH_RATE = 0.3\n","backbone_cfg.MODEL.SWIN.APE = False\n","backbone_cfg.MODEL.SWIN.PATCH_NORM = True\n","backbone_cfg.MODEL.SWIN.USE_CHECKPOINT = False\n","backbone_cfg.MODEL.SWIN.OUT_INDICES = (0, 1, 2, 3)\n","\n","# 2.2: ÌîΩÏÖÄ ÎîîÏΩîÎçî (TEncoderPixelDecoder) ÏÑ§Ï†ï\n","pixel_decoder_cfg = _CfgNode()\n","pixel_decoder_cfg.MODEL = _CfgNode()\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD = _CfgNode()\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM = 256\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.MASK_DIM = 256\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.NORM = \"GN\"\n","pixel_decoder_cfg.MODEL.MASK_FORMER = _CfgNode()\n","pixel_decoder_cfg.MODEL.MASK_FORMER.DROPOUT = 0.1\n","pixel_decoder_cfg.MODEL.MASK_FORMER.NHEADS = 8\n","pixel_decoder_cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD = 1024\n","pixel_decoder_cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS = 3\n","pixel_decoder_cfg.MODEL.MASK_FORMER.PRE_NORM = False\n","\n","# 2.3: Ìä∏ÎûúÏä§Ìè¨Î®∏ ÎîîÏΩîÎçî (Transformer_Decoder) ÏÑ§Ï†ï\n","transformer_decoder_cfg = _CfgNode()\n","transformer_decoder_cfg.MODEL = _CfgNode()\n","transformer_decoder_cfg.MODEL.MASK_FORMER = _CfgNode()\n","transformer_decoder_cfg.MODEL.MASK_FORMER.NHEADS = 8\n","transformer_decoder_cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD = 2048\n","transformer_decoder_cfg.MODEL.MASK_FORMER.DEC_LAYERS = 3\n","transformer_decoder_cfg.MODEL.MASK_FORMER.DROPOUT = 0.0\n","transformer_decoder_cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES = 100\n","transformer_decoder_cfg.MODEL.SEM_SEG_HEAD = _CfgNode()\n","transformer_decoder_cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 1\n","\n","# --- 3. ÌïµÏã¨ Íµ¨ÏÑ±ÏöîÏÜå Ï¥àÍ∏∞Ìôî ---\n","\n","print(\"--- üß† ÌïôÏäµ Ï§ÄÎπÑ ÏãúÏûë ---\")\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"DEVICE: {DEVICE}\")\n","\n","# [Ï§ëÏöî]\n","# Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏(256)ÏôÄ Îã¨Î¶¨, Ï†ÑÏ≤òÎ¶¨Îêú ÌÉÄÏùº(1024) ÌÅ¨Í∏∞Î°ú Î™®Îç∏ ÏûÖÎ†• shape ÏÑ§Ï†ï\n","INPUT_H, INPUT_W = 1024, 1024\n","input_shape = ShapeSpec(channels=3, height=INPUT_H, width=INPUT_W)\n","\n","# 3.1: Î™®Îç∏ (Mask2Former)\n","model = Mask2Former(\n","    backbone_cfg,\n","    pixel_decoder_cfg,\n","    transformer_decoder_cfg,\n","    input_shape\n",").to(DEVICE)\n","\n","# 3.2: ÏÜêÏã§ Ìï®Ïàò (Criterion) - 7Î≤à ÏÖÄÏóêÏÑú Ï†ïÏùòÎê®\n","# (7Î≤à ÏÖÄÏùò Í∏∞Î≥∏Í∞íÏùÑ ÏÇ¨Ïö©)\n","model_criterion = criterion(\n","    oversample_ratio=3.0,\n","    important_sample_ratio=0.75,\n","    class_cost_weight=1.0,\n","    mask_cost_weight=1.0,\n","    dice_cost_weight=1.0,\n","    eos_weigt=0.1 # 'Î∞∞Í≤Ω' ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò (ÏõêÎ≥∏ ÎÖºÎ¨∏ Í∞í)\n",").to(DEVICE) # criterionÎèÑ deviceÎ°ú Î≥¥ÎÇ¥Îäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§.\n","\n","# 3.3: ÏòµÌã∞ÎßàÏù¥Ï†Ä (Optimizer)\n","# Transformer Î™®Îç∏ÏùÄ AdamWÍ∞Ä ÌëúÏ§ÄÏûÖÎãàÎã§.\n","optimizer = optim.AdamW(\n","    model.parameters(),\n","    lr=1e-4,          # ÌïôÏäµÎ•† (Learning Rate)\n","    weight_decay=1e-4 # Í∞ÄÏ§ëÏπò Í∞êÏá† (Weight Decay)\n",")\n","\n","# 3.4: Îç∞Ïù¥ÌÑ∞ Î°úÎçî (DataLoader)\n","TRAIN_DATA_DIR = \"dummy_dataset_output\" # 2Î≤à ÏÖÄÏóêÏÑú ÎßåÎì† ÎçîÎØ∏Îç∞Ïù¥ÌÑ∞ Ìè¥Îçî\n","BATCH_SIZE = 1 # [Ï§ëÏöî] 1024x1024 Ïù¥ÎØ∏ÏßÄÎäî ÌÅ¨ÎØÄÎ°ú, Colab Î©îÎ™®Î¶¨Î•º ÏúÑÌï¥ Î∞∞Ïπò 1Î°ú ÏÑ§Ï†ï\n","\n","train_dataset = BuildingTrainingDataset(TRAIN_DATA_DIR)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True, # ÌïôÏäµ ÏãúÏóêÎäî Îç∞Ïù¥ÌÑ∞Î•º ÏÑûÏñ¥Ï§çÎãàÎã§.\n","    collate_fn=collate_fn\n",")\n","\n","print(\"‚úÖ Î™®Îç∏, ÏÜêÏã§Ìï®Ïàò, ÏòµÌã∞ÎßàÏù¥Ï†Ä, Îç∞Ïù¥ÌÑ∞Î°úÎçî Ï¥àÍ∏∞Ìôî ÏôÑÎ£å!\")\n","\n","# --- 4. Ïã§Ï†ú ÌïôÏäµ(Training) Î£®ÌîÑ ---\n","\n","# (ÌÖåÏä§Ìä∏Ïö©) ÎçîÎØ∏Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûëÏúºÎØÄÎ°ú 10 ÏóêÌè¨ÌÅ¨(Epoch)Îßå ÎèåÎ†§Î¥ÖÎãàÎã§.\n","NUM_EPOCHS = 10\n","\n","print(f\"--- üöÄ ÌïôÏäµ ÏãúÏûë (Ï¥ù {NUM_EPOCHS} ÏóêÌè¨ÌÅ¨) ---\")\n","\n","model.train() # Î™®Îç∏ÏùÑ \"ÌïôÏäµ Î™®Îìú\"Î°ú ÏÑ§Ï†ï (Dropout Îì± ÌôúÏÑ±Ìôî)\n","\n","for epoch in range(NUM_EPOCHS):\n","\n","    # train_loaderÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º 1Í∞úÏî© (batch_size=1) Í∫ºÎÇ¥ÏòµÎãàÎã§.\n","    # images: [1, 3, 1024, 1024]\n","    # targets: [{'mask': [N, 1024, 1024], 'instance_class': [N]}] (Í∏∏Ïù¥ 1ÏßúÎ¶¨ Î¶¨Ïä§Ìä∏)\n","    for i, (images, targets) in enumerate(train_loader):\n","\n","        # 1. [ÌïÑÏàò] Îç∞Ïù¥ÌÑ∞ÏôÄ ÌÉÄÍ≤üÏùÑ GPU(DEVICE)Î°ú Ïù¥Îèô\n","        images = images.to(DEVICE)\n","\n","        targets_gpu = []\n","        for t in targets:\n","            # (Ï£ºÏùò) 7Î≤à ÏÖÄÏùò criterionÏù¥ 'mask'ÏôÄ 'instance_class'Î•º Í∏∞ÎåÄÌï©ÎãàÎã§.\n","            targets_gpu.append({\n","                'mask': t['mask'].to(DEVICE),\n","                'instance_class': t['instance_class'].to(DEVICE)\n","            })\n","\n","        # 2. [ÌïÑÏàò] ÏòµÌã∞ÎßàÏù¥Ï†Ä Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï¥àÍ∏∞Ìôî\n","        optimizer.zero_grad()\n","\n","        # 3. [ÌïµÏã¨] Î™®Îç∏ ÏàúÏ†ÑÌåå (Forward Pass)\n","        # Ïù¥ÎØ∏ÏßÄÎ•º Î™®Îç∏Ïóê ÎÑ£Ïñ¥ ÏòàÏ∏°Í∞í(outputs)ÏùÑ Î∞õÏäµÎãàÎã§.\n","        outputs = model(images)\n","\n","        # 4. [ÌïµÏã¨] ÏÜêÏã§ Í≥ÑÏÇ∞ (Loss Calculation)\n","        # ÏòàÏ∏°(outputs)Í≥º Ï†ïÎãµ(targets_gpu)ÏùÑ ÎπÑÍµêÌïòÏó¨ ÏÜêÏã§ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n","        # (7Î≤à ÏÖÄÏùò criterion.forwardÍ∞Ä Ìò∏Ï∂úÎê®)\n","        loss_dict = model_criterion(outputs, targets_gpu)\n","\n","        # 5. [ÌïµÏã¨] Ï¥ù ÏÜêÏã§(Total Loss) Í≥ÑÏÇ∞\n","        # criterionÏùÄ Ïó¨Îü¨ Ï¢ÖÎ•òÏùò ÏÜêÏã§(class, mask, dice)ÏùÑ ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôò\n","        # Ïù¥Îì§ÏùÑ Î™®Îëê ÎçîÌï¥ÏÑú ÏµúÏ¢Ö ÏÜêÏã§Í∞í(Ïä§ÏπºÎùº)ÏúºÎ°ú ÎßåÎì≠ÎãàÎã§.\n","        total_loss = sum(loss_dict.values())\n","\n","        # 6. [ÌïµÏã¨] Ïó≠Ï†ÑÌåå (Backward Pass)\n","        # \"Î™®Îç∏Ïù¥ Ïñ¥ÎîîÏÑú ÌãÄÎ†∏ÎäîÏßÄ\" Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n","        total_loss.backward()\n","\n","        # 7. [ÌïµÏã¨] Î™®Îç∏ ÏóÖÎç∞Ïù¥Ìä∏ (Optimizer Step)\n","        # Í≥ÑÏÇ∞Îêú Í∑∏ÎûòÎîîÏñ∏Ìä∏Î•º Î∞îÌÉïÏúºÎ°ú Î™®Îç∏Ïùò Í∞ÄÏ§ëÏπòÎ•º Í∞±Ïã†Ìï©ÎãàÎã§.\n","        optimizer.step()\n","\n","        # 8. (Î°úÍπÖ) ÌòÑÏû¨ ÏÉÅÌÉú Ï∂úÎ†•\n","        if i % 1 == 0: # (ÎçîÎØ∏Îç∞Ïù¥ÌÑ∞Îäî Î∞∞ÏπòÍ∞Ä 1Í∞úÎùº Îß§Î≤à Ï∂úÎ†•Îê®)\n","            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{i+1}/{len(train_loader)}], Loss: {total_loss.item():.4f}\")\n","\n","    print(f\"--- Epoch {epoch+1} ÏôÑÎ£å ---\")\n","\n","print(\"üéâ ÌïôÏäµ Î£®ÌîÑÍ∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï¢ÖÎ£åÎêòÏóàÏäµÎãàÎã§!\")"],"metadata":{"id":"-ZRXHoxvAbK4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763207795712,"user_tz":-540,"elapsed":10347,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}},"outputId":"8dba90e5-a3fa-47aa-f948-4c6b5f360d5b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["--- üß† ÌïôÏäµ Ï§ÄÎπÑ ÏãúÏûë ---\n","DEVICE: cuda\n","‚úÖ Î™®Îç∏, ÏÜêÏã§Ìï®Ïàò, ÏòµÌã∞ÎßàÏù¥Ï†Ä, Îç∞Ïù¥ÌÑ∞Î°úÎçî Ï¥àÍ∏∞Ìôî ÏôÑÎ£å!\n","--- üöÄ ÌïôÏäµ ÏãúÏûë (Ï¥ù 10 ÏóêÌè¨ÌÅ¨) ---\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1559942682.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=False):\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Batch [1/1], Loss: 24.3005\n","--- Epoch 1 ÏôÑÎ£å ---\n","Epoch [2/10], Batch [1/1], Loss: 16.1963\n","--- Epoch 2 ÏôÑÎ£å ---\n","Epoch [3/10], Batch [1/1], Loss: 15.7089\n","--- Epoch 3 ÏôÑÎ£å ---\n","Epoch [4/10], Batch [1/1], Loss: 15.7469\n","--- Epoch 4 ÏôÑÎ£å ---\n","Epoch [5/10], Batch [1/1], Loss: 15.7604\n","--- Epoch 5 ÏôÑÎ£å ---\n","Epoch [6/10], Batch [1/1], Loss: 15.4849\n","--- Epoch 6 ÏôÑÎ£å ---\n","Epoch [7/10], Batch [1/1], Loss: 15.2452\n","--- Epoch 7 ÏôÑÎ£å ---\n","Epoch [8/10], Batch [1/1], Loss: 14.9601\n","--- Epoch 8 ÏôÑÎ£å ---\n","Epoch [9/10], Batch [1/1], Loss: 14.7057\n","--- Epoch 9 ÏôÑÎ£å ---\n","Epoch [10/10], Batch [1/1], Loss: 14.4920\n","--- Epoch 10 ÏôÑÎ£å ---\n","üéâ ÌïôÏäµ Î£®ÌîÑÍ∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï¢ÖÎ£åÎêòÏóàÏäµÎãàÎã§!\n"]}]},{"cell_type":"code","source":["# --- 9. ÌõÑÏ≤òÎ¶¨ ---\n","#\n","\n","@torch.no_grad() # Ï∂îÎ°† ÏãúÏóêÎäî Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞Ïù¥ ÌïÑÏöî ÏóÜÏäµÎãàÎã§.\n","def post_process_predictions(outputs, target_size_hw, class_threshold=0.7, mask_threshold=0.5):\n","    \"\"\"\n","    Î™®Îç∏Ïùò ÏõêÏãú Ï∂úÎ†•(logits)ÏùÑ Î∞õÏïÑ, ÌïÑÌÑ∞ÎßÅ, ÏóÖÏÉòÌîåÎßÅ, Ïù¥ÏßÑÌôîÎ•º Í±∞Ï≥ê\n","    ÏµúÏ¢Ö Í≥†Ìï¥ÏÉÅÎèÑ Ïù¥ÏßÑ ÎßàÏä§ÌÅ¨ Î¶¨Ïä§Ìä∏Î•º Î∞òÌôòÌï©ÎãàÎã§.\n","\n","    Args:\n","        outputs (dict): model(x)Ïùò Î∞òÌôòÍ∞í.\n","                        {'predicted_class': [B, Q, C+1], 'predicted_mask': [B, Q, H, W]}\n","        target_size_hw (tuple): (height, width) e.g., (1024, 1024)\n","        class_threshold (float): 'Í±¥Î¨º' ÌÅ¥ÎûòÏä§(logit 0Î≤à)Ïùò ÌôïÎ•† ÏûÑÍ≥ÑÍ∞í\n","        mask_threshold (float): ÎßàÏä§ÌÅ¨Î•º 0 ÎòêÎäî 1Î°ú Í≤∞Ï†ïÌïòÎäî ÏûÑÍ≥ÑÍ∞í\n","\n","    Returns:\n","        list[dict]: Î∞∞Ïπò ÎÇ¥ Í∞Å Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú Í≤∞Í≥º Î¶¨Ïä§Ìä∏.\n","                    Í∞Å ÎîïÏÖîÎÑàÎ¶¨Îäî {'scores': [N], 'labels': [N], 'masks': [N, H_target, W_target]}\n","                    (N: ÏûÑÍ≥ÑÍ∞íÏùÑ ÌÜµÍ≥ºÌïú Í∞ùÏ≤¥ Ïàò)\n","    \"\"\"\n","\n","    # 1. Î™®Îç∏ Ï∂úÎ†• Î∂ÑÎ¶¨\n","    # (B, Q, 2) -> (B, Q, 2)\n","    pred_logits = outputs['predicted_class']\n","    # (B, Q, 64, 64) -> (B, Q, 64, 64)\n","    pred_masks = outputs['predicted_mask']\n","\n","    B, Q, _ = pred_logits.shape\n","    H_small, W_small = pred_masks.shape[-2:]\n","\n","    # 2. ÌÅ¥ÎûòÏä§ ÌôïÎ•† Í≥ÑÏÇ∞ Î∞è ÌïÑÌÑ∞ÎßÅ\n","    # SoftmaxÎ•º Ï†ÅÏö©Ìï¥ ÌôïÎ•†Î°ú Î≥ÄÌôò (B, Q, 2)\n","    probs = F.softmax(pred_logits, dim=-1)\n","\n","    # 'Í±¥Î¨º' ÌÅ¥ÎûòÏä§(Ïù∏Îç±Ïä§ 0)Ïùò ÌôïÎ•† (B, Q)\n","    # (Ï∞∏Í≥†: criterion ÏΩîÎìúÏóêÏÑú Ï†ïÎãµ ÌÅ¥ÎûòÏä§Î•º 0ÏúºÎ°ú ÏÑ§Ï†ïÌñàÏùå)\n","    scores = probs[..., 0]\n","\n","    # Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n","    results = []\n","\n","    # 3. Î∞∞Ïπò ÎÇ¥ Í∞Å Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ ÌõÑÏ≤òÎ¶¨ ÏàòÌñâ\n","    for i in range(B):\n","        image_scores = scores[i]       # (Q,)\n","        image_masks_low_res = pred_masks[i] # (Q, 64, 64)\n","\n","        # 3.1. ÌïÑÌÑ∞ÎßÅ: 'Í±¥Î¨º' ÌôïÎ•†Ïù¥ ÏûÑÍ≥ÑÍ∞í(class_threshold)Î≥¥Îã§ ÎÜíÏùÄ ÏøºÎ¶¨Îßå ÏÑ†ÌÉù\n","        keep = image_scores > class_threshold\n","\n","        filtered_scores = image_scores[keep]          # (N,)\n","        filtered_masks_low_res = image_masks_low_res[keep] # (N, 64, 64)\n","\n","        # ÎßåÏïΩ ÏûÑÍ≥ÑÍ∞íÏùÑ ÎÑòÎäî Í±¥Î¨ºÏù¥ ÌïòÎÇòÎèÑ ÏóÜÎã§Î©¥\n","        if filtered_masks_low_res.shape[0] == 0:\n","            results.append({\n","                'scores': torch.tensor([]),\n","                'labels': torch.tensor([]),\n","                'masks': torch.empty(0, target_size_hw[0], target_size_hw[1])\n","            })\n","            continue\n","\n","        # 3.2. ÏóÖÏÉòÌîåÎßÅ (Ï†ÄÌï¥ÏÉÅÎèÑ -> Í≥†Ìï¥ÏÉÅÎèÑ)\n","        # F.interpolateÎäî [N, C, H, W] ÌòïÌÉúÎ•º Í∏∞ÎåÄÌïòÎØÄÎ°ú, Ï±ÑÎÑê(C=1) Ï∞®ÏõêÏùÑ Ï∂îÍ∞Ä\n","        # (N, 64, 64) -> (N, 1, 64, 64)\n","        upsampled_masks = F.interpolate(\n","            filtered_masks_low_res.unsqueeze(1),\n","            size=target_size_hw,      # (1024, 1024)\n","            mode=\"bilinear\",        # Î∂ÄÎìúÎüΩÍ≤å Î≥¥Í∞Ñ\n","            align_corners=False\n","        )\n","        # (N, 1, 1024, 1024) -> (N, 1024, 1024)\n","        upsampled_masks = upsampled_masks.squeeze(1)\n","\n","        # 3.3. Ïù¥ÏßÑÌôî (Binarization)\n","        # (Ï§ëÏöî) Î™®Îç∏Ïù¥ Ï∂úÎ†•Ìïú ÎßàÏä§ÌÅ¨(pred_masks)Îäî logitÏù¥ÎØÄÎ°ú,\n","        # sigmoidÎ•º Ï†ÅÏö©Ìï¥ 0~1 ÏÇ¨Ïù¥ ÌôïÎ•†Î°ú ÎßåÎì§Í≥†, mask_threshold(0.5)Î°ú Ïù¥ÏßÑÌôî\n","        final_binary_masks = (upsampled_masks.sigmoid() > mask_threshold).to(torch.uint8)\n","\n","        # 3.4. Î†àÏù¥Î∏î ÏÉùÏÑ± (Î™®Îëê 'Í±¥Î¨º' ÌÅ¥ÎûòÏä§ 0)\n","        filtered_labels = torch.zeros_like(filtered_scores, dtype=torch.long)\n","\n","        results.append({\n","            'scores': filtered_scores,\n","            'labels': filtered_labels,\n","            'masks': final_binary_masks\n","        })\n","\n","    return results\n","\n","    # (9Î≤à ÏÖÄÏùò Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏Í∞Ä Ïã§ÌñâÎêòÏóàÎã§Í≥† Í∞ÄÏ†ï)\n","# model, dummy_input, outputs Í∞Ä Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï®\n","\n","print(\"\\n--- üöÄ ÌõÑÏ≤òÎ¶¨ ÌÖåÏä§Ìä∏ ÏãúÏûë ---\")\n","\n","# 1. ÌõÑÏ≤òÎ¶¨ Ìï®ÏàòÎ°ú Í≥†Ìï¥ÏÉÅÎèÑ Ïù¥ÏßÑ ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n","TARGET_H, TARGET_W = 1024, 1024 # ÏõêÎ≥∏ ÌÉÄÏùº ÌÅ¨Í∏∞\n","processed_results = post_process_predictions(outputs, (TARGET_H, TARGET_W), class_threshold=0.7)\n","\n","# 'processed_results'Îäî Î¶¨Ïä§Ìä∏ÏûÖÎãàÎã§. (Î∞∞Ïπò ÌÅ¨Í∏∞=1Ïù¥ÎØÄÎ°ú 0Î≤à Ïù∏Îç±Ïä§ ÏÇ¨Ïö©)\n","result_item = processed_results[0]\n","binary_masks_tensor = result_item['masks'] # (N, 1024, 1024)\n","scores = result_item['scores']           # (N,)\n","\n","print(f\"‚úÖ ÎßàÏä§ÌÅ¨ Î≥µÏõê ÏÑ±Í≥µ!\")\n","print(f\"   -> 100Í∞ú ÏøºÎ¶¨ Ï§ë {binary_masks_tensor.shape[0]}Í∞úÏùò Í±¥Î¨ºÏùÑ Ï∞æÏùå (Threshold=0.7)\")\n","print(f\"   -> ÏµúÏ¢Ö ÎßàÏä§ÌÅ¨ ÌÖêÏÑú ÌòïÌÉú: {binary_masks_tensor.shape}\")\n","\n","\n","# 2. Îã§Í∞ÅÌòï Î≥ÄÌôò ÌÖåÏä§Ìä∏\n","if binary_masks_tensor.shape[0] > 0:\n","    # (Ï§ëÏöî) Ïã§Ï†úÎ°úÎäî DatasetÏóêÏÑú 'tile_transform'ÏùÑ Î∂àÎü¨ÏôÄÏïº Ìï©ÎãàÎã§.\n","    # Ïó¨Í∏∞ÏÑúÎäî ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÌîΩÏÖÄ Ï¢åÌëú(1,1)Î•º ÏÇ¨Ïö©ÌïòÎäî Í∞ÄÏßú(dummy) transformÏùÑ ÎßåÎì≠ÎãàÎã§.\n","    dummy_geo_transform = rasterio.transform.from_origin(\n","        west=0, north=0, xsize=1, ysize=1\n","    )\n","\n","    polygons = convert_masks_to_polygons(binary_masks_tensor, dummy_geo_transform)\n","\n","    print(f\"\\n‚úÖ Îã§Í∞ÅÌòï Î≥ÄÌôò ÏÑ±Í≥µ!\")\n","    print(f\"   -> {len(polygons)}Í∞úÏùò Îã§Í∞ÅÌòï(Shapely Í∞ùÏ≤¥) ÏÉùÏÑ±Îê®\")\n","    if polygons:\n","        print(\"   -> (ÏòàÏãú) 0Î≤à Îã§Í∞ÅÌòïÏùò WKT(Well-Known Text) ÌëúÌòÑ:\")\n","        print(f\"      {polygons[0].wkt[:100]}...\") # WKT: POLYGON ((...))\n","else:\n","    print(\"\\n‚ÑπÔ∏è Ï∞æÏùÄ Í±¥Î¨ºÏù¥ ÏóÜÏñ¥ Îã§Í∞ÅÌòï Î≥ÄÌôòÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")"],"metadata":{"id":"oab5qsDhMTqt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763207795747,"user_tz":-540,"elapsed":34,"user":{"displayName":"ÍπÄÎØºÍ∑†","userId":"09841089427860745418"}},"outputId":"93da8a50-5535-45ef-f507-d83a52aab474"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- üöÄ ÌõÑÏ≤òÎ¶¨ ÌÖåÏä§Ìä∏ ÏãúÏûë ---\n","‚úÖ ÎßàÏä§ÌÅ¨ Î≥µÏõê ÏÑ±Í≥µ!\n","   -> 100Í∞ú ÏøºÎ¶¨ Ï§ë 0Í∞úÏùò Í±¥Î¨ºÏùÑ Ï∞æÏùå (Threshold=0.7)\n","   -> ÏµúÏ¢Ö ÎßàÏä§ÌÅ¨ ÌÖêÏÑú ÌòïÌÉú: torch.Size([0, 1024, 1024])\n","\n","‚ÑπÔ∏è Ï∞æÏùÄ Í±¥Î¨ºÏù¥ ÏóÜÏñ¥ Îã§Í∞ÅÌòï Î≥ÄÌôòÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\n"]}]}]}