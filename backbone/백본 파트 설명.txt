<백본 파트 역할>
우리 모델 Mask2Former에서 백본의 역할은, 입력된 항공사진(.tif)으로부터 다양한 수준의 특징 정보(Feature Maps)를 추출하여, 후속 모듈인 픽셀 디코더(Pixel Decoder)와 트랜스포머 디코더(Transformer Decoder)로 전달하는 것입니다.​

<핵심 코드 (BuildingDataset 클래스 및 모델 실행 파트)>
BuildingDataset 클래스 구현:
__getitem__ 메소드 안에서 .tif 이미지와 .npy 라벨을 한 쌍으로 불러옵니다.
이미지 전처리:
리사이즈: GPU 메모리 부족 문제를 해결하고 학습 효율을 높이기 위해, 거대한 원본 이미지를 (512, 512)의 작은 크기로 통일시켰습니다.
정규화: 이미지의 픽셀 값 분포를 표준화하여 모델이 더 안정적으로 학습하도록 했습니다.
이 모든 과정을 거쳐, 최종적으로 모델에 입력될 PyTorch 텐서(Tensor)를 반환합니다.

2. 모델 로드 및 백본 추출:
Mask2FormerForUniversalSegmentation.from_pretrained(...) 코드로 Hugging Face Hub에서 사전 학습된 Mask2Former 모델 전체를 불러왔습니다.
model.model.pixel_level_module.encoder를 통해, 우리가 필요한 Swin Transformer 백본 부분만 정확히 추출했습니다.

3. 백본 실행 및 결과 확인:
DataLoader를 통해 준비된 이미지 데이터(image_batch)를 백본 모델에 입력했습니다.
swin_backbone(image_batch, ...) 코드가 성공적으로 실행되며, 백본의 최종 결과물인 hidden_states (다중 스케일 특징 맵)가 반환되었습니다.


[실행 결과]
데이터 로더에서 첫 번째 이미지 배치(형태: torch.Size([1, 3, 512, 512]))를 가져왔습니다.
Swin Transformer 백본이 추출한 다중 스케일 특징 맵:
  - Stage 0 특징 맵 형태: torch.Size([1, 16384, 128])
  - Stage 1 특징 맵 형태: torch.Size([1, 16384, 128])
  - Stage 2 특징 맵 형태: torch.Size([1, 4096, 256])
  - Stage 3 특징 맵 형태: torch.Size([1, 1024, 512])
  - Stage 4 특징 맵 형태: torch.Size([1, 256, 1024])

-> 결과물의 의미: 픽셀 디코더와 트랜스포머 디코더로 전달될 데이터
위 결과에서 Swin Transformer 백본이 추출한 다중 스케일 특징 맵이 backbone 파트의 최종 결과물이며, 다른 파트로 전달될 입력 데이터입니다.

다중 스케일(Multi-scale)의 의미: 이 특징 맵들은 하나의 이미지를 다양한 관점에서 분석한 결과물입니다.​
-초기 Stage (고해상도): 이미지의 세밀한 경계선, 모서리 등 '디테일' 정보를 담고 있습니다.
-후기 Stage (저해상도): 이미지의 전체적인 구조, 객체의 형태 등 '맥락' 정보를 담고 있습니다.
팀원 파트와의 연관성:
-픽셀 디코더 파트:  다중 스케일 특징 맵들(hidden_states)을 입력으로 받아, 이를 다시 융합하고 업샘플링하여 최종 마스크를 예측하는 데 필요한 고해상도의 픽셀별 임베딩(per-pixel embeddings)을 생성하게 됩니다.​
-트랜스포머 디코더 파트:  이 특징 맵들과 학습 가능한 쿼리(queries)를 함께 사용하여, 이미지에 어떤 객체들이 있는지, 그리고 각 객체가 어떤 픽셀들을 포함하는지에 대한 정보를 추출하게 됩니다.​

<결론 및 다음 단계>
여기서 구현한 코드의 hidden_states 변수가 다른 파트의 모듈로 전달될 입력 데이터이니, 이를 기반으로 파트 연동을 준비해 주시면 됩니다.